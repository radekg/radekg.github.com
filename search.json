[{"content":"I’ve been deep in YugabyteDB trenches for the last 6 months. At Klarrio, we are building a Database as a Service solution for the Data Services Hub. This is a self-managed, multi-tenant solution designed to run on top of Apache Mesos. We have a small team doing all kinds of integration work. I am focusing on the core database rollout.\nWorking with new technology, for me, often means going all in, deep into the darkness of the source code. I like to know how things work. It’s no different with YugbyteDB. I’m not the kind of guy who takes an upstream component and just runs it in production without understanding how it breaks. The best way to figure out how things work is to take them apart and put back together.\nIn the last 6 months, I’ve been doing all sorts of crazy things with YugabyteDB. To some, it might look like procrastinating but I truly believe that running a complex system in production without understanding how that system works is a very bad idea. And so, I’ve been doing things like:\n Building a number of Docker1, Mesos and Kubernetes based clusters, Running them at different sizes and in different configurations, Building my own Docker images with all sorts of bells and whistles included, Digging deep in the source code of YugabyteDB and PostgreSQL, First time ever writing PostgreSQL extensions and figuring out how to build YugabyteDB targeted extensions or soft PostgreSQL multi-tenancy, Contributed a couple of features:  Foreign Data Wrapper2 shared_preload_libraries3   Building my own bench for working with YugabyetDB source code4 and built it from sources so many times, in my dreams, I sometimes hear all 44 cores of my lab server blasting out at full speed at 3am, Writing a bunch of tools5 for YugabyteDB, looked into monitoring, observability, logging, and operation of a cluster, Speaking at the Distributed SQL Summit 20216.  I cannot thank the Yugabyte team enough for the help they have provided me along the way. They’ve been patient, supportive, professional. The depth of technical answers to my questions is outstanding.\nCredit also has to go to the client and my employer, who supported me on this journey.\nIt’s fair to say that I’ve learned a lot. But I am in no way an expert. You see, the nature of my work is to deliver up to spec projects to clients. I do R\u0026D, architectural work, implementation, full delivery: functional tests, operations and en user documentation, sometimes day two operations for a bit.\nBut once the project is finished, I move to another one. What I learn never goes to waste. To a certain extent what I learn will always be relevant in future projects. I often find myself building all sorts of crazy stuff with technology from previous projects. A false premise, building stuff I dream about becoming products but it always lacks focus and I am not really in a position to build a product today. I very much enjoy doing all that crazy lab work but let’s face it: that’s actually procrastinating.\nI learn a lot, I love it, it keeps me honest. But it is procrastinating. All that lab stuff ends up behind closed doors and rots away.\nWhile working on this huge YugabyteDB rollout, I could observe those same emotions again. What can I do differently this time? Around August an idea started growing in my head that maybe instead of building imaginary products, I should write a book.\nHave you ever seen the book from Jacek Laskowski on Spark internals7? This book describes each individual Spark internal component so one can really get to know how Spark works.\nSidetrack: back in 2015 I’ve spent good six months bending Spark in all sorts of ways. I made it work in Mesos with Docker bridge networking with the patched Akka build8 before it was all hip.\nSuddenly I was toying with the idea of writing something similar about YugabyteDB. And then, about a month ago, Jonathan from Apress reached out on LinkedIn asking if I would be interested in writing a book on YugabyteDB.\nThat gave it focus. The answer is: yes. Someone I used to work with, who also wrote a book, said:\n Writing a book takes you out of your comfort zone and forces you to look at every corner of the product, things you’d never touch in your daily work.\n Sounds like a fantastic way to learn YugabyteDB inside out. And yes, I know there’s no money in writing technical books but money isn’t the driver for me here. For once to have continuity and contribute in a meaningful way—those are the drivers.\nAs I learned later, Jonathan also reached out to Franck Pachot—Developer Advocate at Yugabyte.\nSo I’ve spoken to Franck and we agreed to co-author a book. It’s all very early stages, the proposal is being written, priorities have to be taken care of, everybody has be aligned, everyone has to feel comfortable with choices being made. It can still go in every direction.\nStay tuned.\n  YugabyteDB multi-tenant-paas ↩︎\n [YSQL] Foreign Data Wrapper support ↩︎\n [YSQL] Merge user provided shared_preload_libraries to enable custom PSQL extensions ↩︎\n YugabyteDB build infrastructure ↩︎\n YugabyteDB client for go ↩︎\n Workload isolation in a multi-tenant container based architecture - DSS21 ↩︎\n The Internals of Apache Spark 3.2.0 ↩︎\n Apache Spark on Mesos with Docker bridge networking ↩︎\n   ","description":"I know there’s no money in writing technical books but money isn’t the driver for me here","tags":["yugabytedb","yugabyte","writing"],"title":"YugabyteDB: the book","uri":"/posts/2021-11-11-yugabytedb-the-book/"},{"content":"Mmmm, nearly missed it.\nYugabyteDB 2.9.1.0 was released on the 29th of October.\nSo here’s the thing. Back in August 2021, I contributed foreign data wrapper support to YugabyteDB, and 2.9.1.0 is the first beta release with this feature included. What I’m trying to say: postgres_fdw extension can be used in YugabyteDB starting with version 2.9.1.0.\nThe [YSQL] Foreign Data Wrapper support pull request contains all the interesting details but the bottom line is:\n create / alter / drop: foreign data wrapper create / alter / drop: server create / alter / drop: user mapping create / alter / drop: foreign table (with the caveat that not supported alter table features of YugabyteDB will also not work here) import foreign schema: this statement uses collate under the hood and requires YugabyteDB with ICU support to work out of the box  Let’s give it a go.\nthe environment We will need a proper YugabyteDB cluster. This setup will have 3 masters and 9 TServers. The design of this cluster uses 3 always-on TServers and 2 groups of TServers, each with 3 TServers per tenant. In effect, we have:\n a shared set of masters region: base1a: 3 TServers region: tenant1a: 3 TServers region: tenant2a: 3 TServers each TServer region has its own Envoy proxy in front  I am going to use my reference Docker compose setup which is available here.\nDocker image Build the referenced Docker image:\n1 2 3  cd .docker/yugabyte-db/ docker build -t local/yugabyte:2.9.1.0-b140 . cd -   And start the cluster:\n1 2 3 4 5  docker-compose --env-file \"$(pwd)/.env\" \\  -f compose-masters.yml \\  -f compose-tservers-shared.yml \\  -f compose-tservers-tenant1.yml \\  -f compose-tservers-tenant2.yml up   This will most likely take some time to settle. On my lab server, this takes about 15 seconds.\nMind you, this cluster needs about 25GB RAM to operate rather reasonably. There are 12 containers, each reserving 2GB RAM and some Envoy proxies.\nwhat’s the plan The plan of action goes like this:\n as yugabyte user: configure two tenant databases,  each database is owned by a respective tenant user, each tenant user has a tablespace assigned,   as tenant2: create a table, as yugabyte: configure the foreign data wrapper for tenant1, as tenant1, create a foreign table and run some queries on it.  setting things up All tooling for this setup is already within the repository. All commands should be executed from the directory where compose files live.\nsetup tenants The password for the yugabyte user is yugabyte (default). Passwords for those new accounts are the same as usernames: tenant1 and tenant2 respectively.\nExtension related errors can be ignored.\n1 2 3 4 5  docker run --rm \\  --net=yb-dbnet \\  -v \"$(pwd)/sql-init-tenant1.sql:/init.sql\" \\  -ti postgres:11.2 \\  bash -c 'psql \"host=envoy-yb-shared port=35432 user=yugabyte dbname=yugabyte\" -f /init.sql'   Password for user yugabyte: CREATE ROLE CREATE DATABASE CREATE TABLESPACE REVOKE REVOKE GRANT ALTER ROLE ALTER ROLE You are now connected to database \"tenant1db\" as user \"yugabyte\". psql:/init.sql:12: ERROR: could not open extension control file ... 1 2 3 4 5  docker run --rm \\  --net=yb-dbnet \\  -v \"$(pwd)/sql-init-tenant2.sql:/init.sql\" \\  -ti postgres:11.2 \\  bash -c 'psql \"host=envoy-yb-shared port=35432 user=yugabyte dbname=yugabyte\" -f /init.sql'   Password for user yugabyte: CREATE ROLE CREATE DATABASE CREATE TABLESPACE REVOKE REVOKE GRANT ALTER ROLE ALTER ROLE You are now connected to database \"tenant1db\" as user \"yugabyte\". psql:/init.sql:12: ERROR: could not open extension control file ... create tenant2 table Connect as tenant2:\n1 2 3 4  docker run --rm \\  --net=yb-dbnet \\  -ti postgres:11.2 \\  bash -c 'psql \"host=envoy-yb-tenant2 port=35432 user=tenant2 dbname=tenant2db\"'   Password for user tenant2: psql (11.2 (Debian 11.2-1.pgdg90+1)) Type \"help\" for help. tenant2db=\u003e And create a table:\n1  create table sharedtableexample (rowid int, rowval text) split into 3 tablets;   CREATE TABLE Close the connection:\n1  \\q   I’ve noticed that all-lower-case names are the easiest to work with.\nconfigure tenant1 foreign data wrapper Technically, it does not matter which Envoy proxy is used for this operation, as long as it is executed as the yugabyte user:\n1 2 3 4  docker run --rm \\  --net=yb-dbnet \\  -ti postgres:11.2 \\  bash -c 'psql \"host=envoy-yb-shared port=35432 user=yugabyte dbname=tenant1db\"'   Password for user yugabyte: psql (11.2 (Debian 11.2-1.pgdg90+1)) Type \"help\" for help. tenant1db=# Run these commands to create the extension, create a server and setup use mapping with permissions:\n1 2 3 4 5 6 7 8 9 10  create extension postgres_fdw; create server fdw_tenant2shares foreign data wrapper postgres_fdw options ( host 'envoy-yb-tenant2', port '35432', dbname 'tenant2db'); create user mapping for tenant1 server fdw_tenant2shares options ( user 'tenant2', password 'tenant2'); grant usage on foreign server fdw_tenant2shares to tenant1; \\q   postgres_fdw extension needs to be created within the tenant database so we have connected directly to the target database. Alternatively, I could have connected to the yugabyte database and used the \\connect tenant1db command.\ncreate foreign table as tenant1 As tenant1:\n1 2 3 4  docker run --rm \\  --net=yb-dbnet \\  -ti postgres:11.2 \\  bash -c 'psql \"host=envoy-yb-tenant1 port=35432 user=tenant1 dbname=tenant1db\"'   Password for user tenant1: psql (11.2 (Debian 11.2-1.pgdg90+1)) Type \"help\" for help. tenant1db=\u003e List existing foreign servers and create the foreign table:\n1 2 3 4 5 6  \\des+ create foreign table sharedtableexample_foreign ( rowid integer options (column_name 'rowid'), rowval text options (column_name 'rowval') ) server fdw_tenant2shares options ( schema_name 'public', table_name 'sharedtableexample');   run some queries tenant1 can now insert data to the foreign table:\n1 2 3 4  insert into sharedtableexample_foreign (rowid, rowval) values (1, 'hello, world!'); insert into sharedtableexample_foreign (rowid, rowval) values (2, 'hello, world!'); insert into sharedtableexample_foreign (rowid, rowval) values (3, 'hello, world!'); \\q   verify Connect back to tenant2db as tenant2:\n1 2 3 4  docker run --rm \\  --net=yb-dbnet \\  -ti postgres:11.2 \\  bash -c 'psql \"host=envoy-yb-tenant2 port=35432 user=tenant2 dbname=tenant2db\"'   Password for user tenant2: psql (11.2 (Debian 11.2-1.pgdg90+1)) Type \"help\" for help. tenant2db=\u003e And select the data from the original table:\n1  select * from sharedtableexample order by rowid;   All records are in:\n rowid | rowval -------+--------------- 1 | hello, world! 2 | hello, world! 3 | hello, world! (3 rows) voila! The beauty of YugabyteDB. Because it is PostgreSQL under the hood, postgres_fdw is simply working.\n","description":"a brief intro to postgres_fdw with YugabyteDB","tags":["yugabytedb","postgres"],"title":"YugabyteDB: Postgres foreign data wrapper","uri":"/posts/2021-11-08-yugabytedb-postgres-foreign-data-wrapper/"},{"content":"Yes, it’s perfectly fine to run databases in containers. The only challenge is to make sure that the data stored by the database does not reside within the file system of the container. Otherwise, after removing the container, the data will be gone, too.\nbasic docker Let’s have a look at the most basic example from the Postgres Docker Hub:\n1 2 3 4  docker run \\  --name some-postgres \\  -e POSTGRES_PASSWORD=mysecretpassword \\  -d postgres   The problem here is, once we stop working with this container, if we do docker stop some-postgres \u0026\u0026 docker rm some-postgres, the data stored in the database will be gone because the default Postgres data directory resides within the container file system.\nTo prevent this from happening, the Postgres container can be configured with a volume mapped to the host directory. For example, to store the data in the /tmp/postgres-data directory of the host, the container can be started like this:\n1 2 3 4 5 6  docker run -d \\  --name some-postgres \\  -e POSTGRES_PASSWORD=mysecretpassword \\  -e PGDATA=/var/lib/postgresql/data/pgdata \\  -v /tmp/postgres-data:/var/lib/postgresql/data \\  postgres   The -v, or long –volume, option has the form of [host path]:[container path]. The PGDATA environment variable tells the docker-entrypoint.sh to configure the pgdata directory with its value. At the same time, the container starts with that directory mapped from the host.\nThe -v option can be specified multiple times to map different directories or files. The option is documented here.\ndocker compose When working with Docker Compose, the best option is to use a bind volume mount. The simplest example would be:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  version:'3.3'services:postgres:image:postgres:13.2restart:unless-stoppedenvironment:POSTGRES_DB:mydbPOSTGRES_USER:myuserPOSTGRES_PASSWORD:mysecretpasswordPGDATA:/var/lib/postgresql/data/pgdataports:- \"5432:5432\"volumes:-type:bindsource:/tmp/postgres-datatarget:/var/lib/postgresql/datanetworks:- referencenetworks:reference:  running in the cloud When running containers in a cloud environment, say on an EC2 instance in AWS, the preferred way is to use an EBS like volume (block storage) as the host volume for the container host path location. This will ensure data survivability across container instances and VM instances, as long as the EBS volume with the host directory is attached to the expected host running the Docker container.\n","description":"keep the postgres data across container restarts","tags":["docker","postgres"],"title":"Postgres in Docker with persistent storage","uri":"/posts/2021-07-12-postgres-in-docker-with-persistent-storage/"},{"content":"The default YugabyteDB Docker image from Docker Hub runs the database as a root user.\nI need to run it as a non-root user and there is no release Docker image Dockerfile available in YugabyteDB repositories.\nSo I’ve created my own and here it is.\n To build the image, run this command:\n1 2 3 4  curl --silent https://gist.githubusercontent.com/radekg/3f749cba86e91a8c88eb0e88c8b8754c/raw \u003e Dockerfile docker build -t yb-test:latest . ... =\u003e =\u003e naming to docker.io/library/yb-test:latest   Start the container:\n1  docker run --rm -ti yb-test:latest bash   Run this in the the container to start the cluster:\n1  [myybuser@0fb8cfa7c3d0 /]$ yugabyted start   The output:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  Starting yugabyted... ✅ System checks +--------------------------------------------------------------------------------------------------+ | yugabyted | +--------------------------------------------------------------------------------------------------+ | Status : Running. Leader Master is present | | Web console : http://127.0.0.1:7000 | | JDBC : jdbc:postgresql://127.0.0.1:5433/yugabyte?user=yugabyte\u0026password=yugabyte | | YSQL : bin/ysqlsh -U yugabyte -d yugabyte | | YCQL : bin/ycqlsh -u cassandra | | Data Dir : /home/myybuser/var/data | | Log Dir : /home/myybuser/var/logs | | Universe UUID : c9e704e3-ff8b-46df-8921-6c5bbd6de2f8 | +--------------------------------------------------------------------------------------------------+ 🚀 yugabyted started successfully! To load a sample dataset, try 'yugabyted demo'. 🎉 Join us on Slack at https://www.yugabyte.com/slack 👕 Claim your free t-shirt at https://www.yugabyte.com/community-rewards/   That’s about it.\n","description":"create a YugabyteDB Docker image with custom uid / gid","tags":["yugabytedb"],"title":"YugabyteDB Docker image","uri":"/posts/2021-06-15-yugabytedb-docker-image/"},{"content":"After some insightful weeks of diving into the Ory platform, I am reverting back to Keycloak to investigate some other of its interesting features. The last few weeks spent in the Ory-land were enlightening.\nOne of my previous post, Introduction to Keycloak Authorization Services1, gets about 20 daily reads but authorization services isn’t the only awesome thing about Keycloak.\nservice provider interfaces Keycloak’s extensibility is what absolutely blows my mind. Keycloak defines a number of service provider interfaces (SPI)2 which allow the developer to tap into and add completely new functionality. Authorization is a bit like CRM or ERP, every organization has their own quirks.\nSPI implementations are written in Java—any language targeting the JVM, really—and deployed to the Keycloak instance by placing them in a predefined directory.\nThe SPI deployment supports hot reloading, SPIs can be updated while the server is running.\ntesting the waters I’m going to start this little series by looking at a required action provider. A required action provider hooks into the registration and login process and executes an action which allows amending the flow and conditionally allow or reject the login attempt.\nThe question: how difficult would it be to implement the following scenario:\n The user registers via self service. The user is not allowed to log in via self service until a specific attribute is set; this attribute would be set by an administrative operator; effectively requires a registration approval. If the user account does not have the required attribute, the user is forwarded to an information page outside of Keycloak.  I’m not implementing the actual approval process because this is way out of scope of this article.\nthe Java code Any Keycloak provider requires, at minimum, two implementation classes:\n A provider factory: a class implementing a specific SPI factory interface. An implementation of the SPI initialized by the previously written factory.  The required action provider is one of the simplest SPIs available, but it does allow for some pretty wild implementations. For example, the out of the box WebAuthn support is one of those.\nHere, the User must be approved factory, is pretty straightforward:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36  package com.gruchalski.idp.spi.actions; import org.keycloak.Config; import org.keycloak.authentication.RequiredActionFactory; import org.keycloak.authentication.RequiredActionProvider; import org.keycloak.models.KeycloakSession; import org.keycloak.models.KeycloakSessionFactory; public class UserMustBeApprovedActionFactory implements RequiredActionFactory { private static final UserMustBeApprovedAction SINGLETON = new UserMustBeApprovedAction(); @Override public RequiredActionProvider create(KeycloakSession session) { return SINGLETON; } @Override public void init(Config.Scope scope) {} @Override public void postInit(KeycloakSessionFactory keycloakSessionFactory) {} @Override public void close() {} @Override public String getId() { return UserMustBeApprovedAction.PROVIDER_ID; } @Override public String getDisplayText() { return \"User must be approved\"; } }   This class implements the org.keycloak.authentication.RequiredActionFactory and creates a static action instance. The create, init and postInit methods are pretty neat because they allow us to configure the action based on whatever the Keycloak status is. They enable full integration with Keycloak runtime. This pattern exists across all of the Keycloak SPIs.\nThe actual action looks like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  package com.gruchalski.idp.spi.actions; import org.keycloak.authentication.RequiredActionContext; import org.keycloak.authentication.RequiredActionProvider; public class UserMustBeApprovedAction implements RequiredActionProvider { public static String PROVIDER_ID = \"USER_MUST_BE_APPROVED\"; @Override public void evaluateTriggers(RequiredActionContext requiredActionContext) {} @Override public void requiredActionChallenge(RequiredActionContext requiredActionContext) { if (requiredActionContext .getUser() .getAttributes() .containsKey(\"x-approved\")) { requiredActionContext.success(); } else { requiredActionContext .getAuthenticationSession() .setRedirectUri(\"https://account.gruchalski.com/errors/approval-required/\"); requiredActionContext.failure(); } } @Override public void processAction(RequiredActionContext requiredActionContext) {} @Override public void close() {} }   The requiredActionChallenge(RequiredActionContext) method is where the action happens. This method is called when the user enters the User must be approved login step; usually after submitting the login form. The requiredActionContext argument provides a number of interesting methods. For example, we can create forms with arbitrary fields asking the user for additional input. If we did that, we could process that input in the processAction(RequiredActionContext) method.\nThis action, however, is a binary decision - the user either has or does not have the attribute assigned. We can inspect the logging in user by looking up various details using the requiredActionContext.getUser() method. Here, the program checks whether the user account has the x-approved. If yes, the context is approved using the success() method. Otherwise, the request is redirected to an arbitrary URI which could provide detailed explanation of the reason.\nTo have this action available in Keycloak, a third file is required. The file is called org.keycloak.authentication.RequiredActionFactory and must be placed in resources/META-INF/services directory of the Java project. The content is simply:\ncom.gruchalski.idp.spi.actions.UserMustBeApprovedActionFactory deployment Regardless on your Java packaging tool of choice, the outcome is a jar file containing the compiled classes and the resources directory. The jar file must be copied to the Keycloak /opt/jboss/keycloak/standalone/deployments directory. Give Keycloak a couple of seconds to load the classes, the status will be printed in the logs.\nenabling and testing Sign in to Keycloak and navigate to the realm Authentication (left menu) / Required Actions tab. Click the Register button in top left table corner and select the User must be approved action from the list. Click Ok. The action will be added to the list.\nIf you want to enforce this action for every user in the realm, tick the Default action checkbox. Be careful, if you do this, make sure your current user you are logged in as, has the x-approved attribute assigned - otherwise you will lock yourself out! If in doubt, test on a temporary realm or on a Keycloak instance you can easily wipe.\nThe action can be enforced for individual users only. To do so, go to realm Users, find the account for which the action should be enforced and select it in the Required User Actions. Save the changes.\nThat’s it. Now, when the user tries to sign in and there is no required attribute, they will be redirected to the information page. This action also takes effect when using the direct password grant. Without the attribute, the response is:\n1 2 3 4  { \"error\": \"invalid_grant\", \"error_description\": \"Account is not fully set up\" }   In one of the future posts, I am going to look at adding custom forms to the action.\n  Introduction to Keycloak Authorization Services ↩︎\n Keycloak Service Provider Interfaces ↩︎\n   ","description":"","tags":["keycloak"],"title":"Extending Keycloak—required actions: user must be approved","uri":"/posts/2021-06-06-extending-keycloak-required-action-providers/"},{"content":"The 24 hours of Nürburgring race was just red flagged for the remainder of the night due to the fog. That’s a perfect opportunity to add TLS to my Keycloak Docker Compose setup described previously here1.\nThere are multiple ways of setting up TLS for Keycloak, one of them being the native Java JKS key store / trust store gymnastics.\nWell, that’s certainly a way to go. If you prefer that path, feel free to do so, details are here2 but that’s a lot of work. I like my life simple so I choose to use a proxy to terminate TLS instead.\nLet’s have a look at the original compose.yml file, it’s short:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  version:'3.9'services:postgres:image:postgres:13.2restart:unless-stoppedenvironment:POSTGRES_DB:${POSTGRESQL_DB}POSTGRES_USER:${POSTGRESQL_USER}POSTGRES_PASSWORD:${POSTGRESQL_PASS}networks:- local-keycloakkeycloak:depends_on:- postgrescontainer_name:local_keycloakenvironment:DB_VENDOR:postgresDB_ADDR:postgresDB_DATABASE:${POSTGRESQL_DB}DB_USER:${POSTGRESQL_USER}DB_PASSWORD:${POSTGRESQL_PASS}image:jboss/keycloak:${KEYCLOAK_VERSION}ports:- \"28080:8080\"restart:unless-stoppednetworks:- local-keycloaknetworks:local-keycloak:  To enable the proxy with TLS support, let’s modify the yaml file to this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47  version:'3.9'services:envoy:image:envoyproxy/envoy:v1.18.2restart:unless-stoppedcommand:/usr/local/bin/envoy -c /etc/envoy/envoy-keycloak.yaml -l debugports:- 443:443- 8001:8001volumes:- type:bindsource:./etc/envoytarget:/etc/envoynetworks:- local-keycloakpostgres:image:postgres:13.2restart:unless-stoppedenvironment:POSTGRES_DB:${POSTGRESQL_DB}POSTGRES_USER:${POSTGRESQL_USER}POSTGRES_PASSWORD:${POSTGRESQL_PASS}networks:- local-keycloakkeycloak:depends_on:- envoy- postgrescontainer_name:local_keycloakenvironment:DB_VENDOR:postgresDB_ADDR:postgresDB_DATABASE:${POSTGRESQL_DB}DB_USER:${POSTGRESQL_USER}DB_PASSWORD:${POSTGRESQL_PASS}PROXY_ADDRESS_FORWARDING:\"true\"image:jboss/keycloak:${KEYCLOAK_VERSION}restart:unless-stoppednetworks:- local-keycloaknetworks:local-keycloak:  There are four differences in the new file:\n there is a new envoy service the keycloak service additionally depends on the envoy service the keycloak service no longer exposes the 28080 port on the host there is a new environment variable defined for the keycloak service: PROXY_ADDRESS_FORWARDING: \"true\"  envoy configuration Looking closely at the envoy service, we can spot the host ./etc/envoy to container /etc/envoy volume bind. The proxy command references the /etc/envoy/envoy-keycloak.yaml configuration file. The file must have the yaml extension, yml is not going to work. The content is:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55  static_resources:listeners:- address:socket_address:address:0.0.0.0port_value:443listener_filters:- name:\"envoy.filters.listener.tls_inspector\"filter_chains:- filter_chain_match:server_names:- idp.gruchalski.comfilters:- name:envoy.filters.network.http_connection_managertyped_config:\"@type\": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManagercodec_type:AUTOstat_prefix:ingress_httproute_config:name:local_routevirtual_hosts:- name:keycloakdomains:- \"*\"routes:- match:prefix:\"/\"route:cluster:proxy-domain1http_filters:- name:envoy.filters.http.routertransport_socket:name:envoy.transport_sockets.tlstyped_config:\"@type\": type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.DownstreamTlsContextcommon_tls_context:tls_certificates:- certificate_chain:filename:/etc/envoy/certificates/idp.gruchalski.com.crtprivate_key:filename:/etc/envoy/certificates/idp.gruchalski.com.keyclusters:- name:proxy-domain1type:STRICT_DNSlb_policy:ROUND_ROBINconnect_timeout:10sload_assignment:cluster_name:proxy-domain1endpoints:- lb_endpoints:- endpoint:address:socket_address:address:keycloakport_value:8080  The directory structure looks like this:\n. ├── compose.yml └── etc └── envoy └── envoy-keycloak.yaml You might ask what is this config file doing so let’s look at it from top to bottom.\n First, we define a listener bound to 0.0.0.0 on port 443 - standard HTTPS stuff. Next, we create a filter chain matching the idp.gruchalski.com domain name - this is the TLS SNI matching. The TLS SNI implies that our service, here Keycloak, will be accessed over HTTPS only and hostname advertised during the TLS handshake is used to find the upstream (cluster) target to forward the traffic to. As you can probably already imagine, I will be accessing Keycloak via https://idp.gruchalski.com. The connections matching the filtered domain will be forwarded to the proxy-domain1 cluster via the http_filter. The cluster forwards the requests to the load balancer endpoints, in this, we have one at keycloak:8080. This is the name of the container on Docker network used for this setup.  The part I’ve glossed over is the transport_socket.common_tls_context.tls_certificates. It points at the TLS certificate and key used for the filter for the domain.\nOkay, a couple of caveats:\n we don’t have the certificate yet how do we access Keycloak using the domain name when it is running in local compose  the domain name Easy, modify the /etc/hosts file by adding:\n127.0.0.1 idp.gruchalski.com certificates This isn’t a rocket science either. In your case, you probably already have a domain name you want to use instead of idp.gruchalski.com so replace all occurences with your own domain in configs above and commands below.\nBecause Keycloak is used in the browser, we want real TLS certificates from one of the public trusted certificate authorities. Let’s Encrypt is for sure an awesome choice. We can get the LE certficites in multiple ways but at the core, we either need the control over the DNS for the dns-01 challenge or we need a http/https server reachable via the domain names for which the certificates should be issued. More about LE challenge types3.\nLong story short, as I am requesting the certifciates for the local compose setup, the http-01 and tls-alpn-01 challenges are not an option because Let’s Encrypt will not be able to call back to a server running on my local machine.\nThe dns-01 challenge is the way to go but it requires having an administrative control over the DNS server so the required TXT records can be created to complete the LE challenge. I have that, I use AWS Route 53 as my DNS of choice.\nA couple of days ago, I have written about the LEGO client which I used for obtaining the certificates4. Here, I’d use the following command:\n1 2 3 4 5 6 7 8 9 10 11 12  cd etc/envoy docker run --rm \\  -v $(pwd):/lego \\  -v ${HOME}/.aws/credentials:/root/.aws/credentials \\  -e AWS_PROFILE=lego \\  -ti goacme/lego \\  --accept-tos \\  --domains=idp.gruchalski.com \\  --server=https://acme-v02.api.letsencrypt.org/directory \\  --email=radek@gruchalski.com \\  --path=/lego \\  --dns=route53 run   As a result, my file structure now looks like this:\n. ├── compose.yml └── etc └── envoy ├── accounts │ └── acme-v02.api.letsencrypt.org │ └── radek@gruchalski.com │ ├── account.json │ └── keys │ └── radek@gruchalski.com.key ├── certificates │ ├── idp.gruchalski.com.crt │ ├── idp.gruchalski.com.issuer.crt │ ├── idp.gruchalski.com.json │ └── idp.gruchalski.com.key └── envoy-keycloak.yaml The configuration is now complete.\nAfter starting the setup with docker compose -f compose.yml up, I can access my Keycloak by entering https://idp.gruchalski.com in the browser address bar. The TLS request is terminated at Envoy and Envoy finds the cluster based on the hostname advertised during the TLS handshake. The request is then forwarded to Keycloak on port 8080.\nInstalltion finalization is exactly the same as in the previous article.\n  Keycloak with Docker Compose ↩︎\n Keycloak documentation: setting up SSL ↩︎\n Let’s Encrypt challenge types ↩︎\n Let’s Encrypt certificates for local development ↩︎\n   ","description":"","tags":["keycloak","tls","envoy"],"title":"Keycloak with TLS in Docker compose behind Envoy proxy","uri":"/posts/2021-06-06-keycloak-with-tls-behind-envoy/"},{"content":"While building a couple of browser based prototypes, I’ve hit an interesting problem. Basically, I am trying to replicate a full remote setup with a reverse proxy and TLS SNI while running everything on localhost. Getting the DNS functioning is pretty easy—I just add the required hosts to the /etc/hosts file and I’m done with it.\nHowever, I still need actual certificates trusted by the browser. In 2021, Let’s Encrypt is the way to go. Turns out, this is also pretty easy to do. There’s a Let’s Encrypt client and ACME library written in Go1 which can be used via Docker on any operating system.\nHere’s how to use it with AWS Route53:\n1 2 3 4 5 6 7 8 9 10 11 12 13  mkdir -p /tmp/certs \u0026\u0026 cd /tmp/certs docker run --rm \\  -v $(pwd):/lego \\  -v ${HOME}/.aws/credentials:/root/.aws/credentials \\  -e AWS_PROFILE=lego \\  -ti goacme/lego \\  --accept-tos \\  --domains=subdomain1.example.com \\  --domains=subdomain2.example.com \\  --server=https://acme-staging-v02.api.letsencrypt.org/directory \\  --email=info@example.com \\  --path=/lego \\  --dns=route53 run   This command starts a Docker container using thr latest LEGO Docker image and requests a certificate for two subdomains. It mounts two volumes:\n current working directory as /lego in the container, this is where the retrieved data will land ${HOME}/.aws/credentials as /root/.aws/credentials  The command assumes that AWS profiles are used and there is a profile named lego. The profile must have the following IAM permissions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"\", \"Effect\": \"Allow\", \"Action\": [ \"route53:GetChange\", \"route53:ChangeResourceRecordSets\", \"route53:ListResourceRecordSets\" ], \"Resource\": [ \"arn:aws:route53:::hostedzone/*\", \"arn:aws:route53:::change/*\" ] }, { \"Sid\": \"\", \"Effect\": \"Allow\", \"Action\": \"route53:ListHostedZonesByName\", \"Resource\": \"*\" } ] }   As I have not specified the hosted zone ID in the command, LEGO client will figure it out from the --domains flags. The output directory is configured with the --path flag pointing to the first volume target.\nTo fetch the real production certificate, use https://acme-v02.api.letsencrypt.org/directory as the --server value.\nPretty neat. I now have LE real certificates without messing with remote HTTP servers.\nFull documentation is available here2.\nuse as a library The LEGO client can be used as a library in any go program. Here’s a repository3 wrapping the library in a more high level client. Very decent sample can be found in the examples folder.\n  Let’s Encrypt client and ACME library written in Go ↩︎\n LEGO documentation ↩︎\n eggsampler/acme ↩︎\n   ","description":"LEGO client with Docker and AWS Route53 DNS","tags":["letsencrypt","tls"],"title":"Let’s Encrypt certificates for local development","uri":"/posts/2021-06-04-letsencrypt-certificates-for-local-development/"},{"content":"Oh boy, 20 years of software engineering definitely makes you tune your BS radar when reading claims about how awesome and mature a relatively new technology is. Every new project claims to solve all the problems for the current itch on hand. Do you know that feeling? You spend a week reading the documentation, prepare the test deployment, find out that documentation doesn’t match reality, look at GitHub and find dozens open issues. Finally, you throw it out in anger and curse at yourself because you just wasted a week of time.\nRecently, I’ve been on a hunt for a PostgreSQL (optionally distributed) compatible database for one of the work tasks. It’s an awesome task because it forced me to look deep into the current database ecosystem. Other than occasional Postgres Docker image pull, I haven’t touched a traditional SQL database since 2012. This exercise is a good opportunity to figure out what’s new in the relational databases world since my last experiences with Microsoft SQL Server 2008, which I enjoyed very much!\nLooking at just PostgreSQL was much fun. PostgreSQL is simply amazing. Fast, just works, JSON operations included and the grant system is outstanding. Permissions can be managed on a database, tablespace, schema, column and row level. With tablespaces, I can tell Postgres where the particular bit of data resides on disk. It’s a solid, ubiquitous database available in every cloud and very easy to deploy on premise. The tooling is mature, the operations are pretty well understood. Some claim that setting up replication and automatic failover can be a pita. I don’t know, I haven’t had a chance to go that far yet. However, when I was contracting for Saywell International back in 2010, we were running running an active-active MSSQL setup with log shipping over Dropbox. Crazy idea but it worked and was a lot of fun, ha! 2021 Postgres cannot be more difficult than that.\nPostgreSQL is an ACID1 (atomic, consistent, isolated, durable) RDBMS (relational database management system). There’s a catch with RDBMS. Data must fit on a single server. If the data outgrows a server, it can be manually shareded (partitioned) or moved to a bigger server. Well, we can can get pretty big servers today, for sure. 256BG RAM? 44 cores? 12TB disks? Sure. But when that bad boy has a hiccup, things go bad fast. The cost of these servers, even in the cloud, is mind-boggling. Triple that when you’re in need of replicas.\nA database article without mentioning the CAP theorem2 is not a database article. The CAP theorem describes the three major properties of a database system: consistency, availability and partition tolerance. The CAP theorem also says that it is not possible to have all three properties fulfilled by one system, choose any of the two properties but all three is not possible. An RDMBS satisfies the C and A properties: data is consistent and available but not partition tolerant.\nThe last few years, and especially the last couple of years, has seen some really interesting new developments in the database space. New SQL databases are popping up left and right and many of them attempt to crack the hard problem of distributed querying, distributed transactions and data replication. This is an interesting development, most likely, rooted in NoSQL (not only SQL). Databases like Cassandra or Riak brought the concept of Amazon’s Dynamo to the masses. These new databases took advantage of and introduced many new concepts. Consistent hashing3 and distributed hash tables4 (DHT) for data storage being probably most crucial ones.\nDHTs made the automatic partitioning possible. Consistent hashing makes it possible to identify the location of the partition the data belongs to within the cluster with O(1) complexity. These new databases replicate the data within the cluster. Data is available, a loss of a node does not imply service interruption. These databases fulfill the A and P properties of CAP.\nOn a hand-wavy, high conceptual level, the storage mechanism wasn’t different than RDBMS sharding with manual replication configuration. However, with regular RDBMS, it isn’t possible to treat a sharded table as one. Querying across shards is not straightforward, transactions across shards are not possible.\nSolving one problem often implies introducing another. These new databases are in fact key/value stores with an SQL layer on top. There are no transactions, there are no joins, distributed consistent counters are difficult. Due to the nature of replication, the replicas are eventually consistent5; it takes time for the replica to catch up with the main partition (hence no C property). Using databases like Cassandra or Riak implied plenty of write overhead. For performance reasons, it’s often better to store the data in the format intended for reading. It’s a common pattern to write the data multiple multiple times to different tables, depending on the context it would be used when querying. Capacity planning is difficult, what sits where in the database is difficult to track by just looking at the schema. There are no foreign keys, constraints are not not always possible. Plenty of the database logic is shifted to the application layer.\nAs the world was figuring out NoSQL, on the other side of the spectrum, the smart folks at Google and Amazon had a crack at the number one RDBMS problem. How to make a global, scalable, distributed, highly available ACID database a reality. The result of that work is—respectively—Google Spanner and Amazon Aurora. Both are available, consistent, isolated and durable global scale, distributed SQL databases featuring distributed transactions, joins and majority of the niceties of an RDBMS. Both can be used with existing RDBMS tools and often are a drop-in replacement. Things don’t always work like in the real implementation because of the design trade-offs of the distributed implementation, but things are often close enough. The problem with those? They’re proprietary and prohibitively expensive when using from the outside of the respective cloud provider.\nAs it usually happens with those clever whitepapers from the big players, other smart people pick them up and go on to build interesting products. It’s no different with Spanner / Aurora thing. The two most interesting products currently available are CockroachDB and YugabyteDB. They are roughly similar in what problems they tackle. Both are CP databases but both achieve high availability. Both are ACID and both are PostgreSQL compatible. Both have a roughly similar architecture. CockroachDB is written in go and reimplements the Postgres protocol from scratch, YugabyteDB is C++ and lifts the complete original C++ PostgreSQL engine.\nIt’s like PostgreSQL with storage ripped out and new RAFT based storage layer added.\nI have briefly looked at CockroachDB but rejected it almost immediately due to the licensing constraints. It’s not really clear what becomes Apache 2 licensed when and it explicitly disallows as-a-service use.\nYugabyteDB6 is Apache 2 licensed with no strings attached. The database core has no use restrictions. The cloud control plane is not Apache 2 licensed but YugabyteDB can be deployed without it.\nYugabyteDB Here’s the description:\n YugabyteDB is a free and open-source, distributed, relational, NewSQL database management system designed to handle large amounts of data spanning across multiple availability zones and geographic regions while providing single-digit latency, high availability, and no single point of failure.\n Basically, the magic pixie dust. I was very reluctant giving it a try. My BS radar was fully tuned.\nBut… wow…\nThis was different than anxiously anticipated! It does deliver everything what it claims it does.\nDatabases, schemas, tables, views, triggers, foreign data wrapper, and tablespaces, it’s all in there. Permission management comes from PostgreSQL and works exactly as expected. Roles can be restricted to databases, schemas, tablesspaces. Access to tables, views, functions, triggers, even individual columns and rows, can be controlled exactly like in Postgres—it’s all in there and working.\nThere’s more. YugabyteDB is a distributed system for a reason. The underlying DocDB layer automatically replicates and shards the data for high availability while leaving full control over those aspects to the operator. This is great because the replication is the first backup layer, a loss of a node does not mean data loss. Of course, take your backups…\nYugabyteDB uses RAFT replication per table. Different tables can have different replication factor. Different tables can be tied to different regions. Wow, even specific rows of specific tables can be placed in exact geographical locations, for example for legal compliance. There are distributed queries, joins, triggers, transactions, everything.\nMy first Postgres compatibility tests used a single YB master and YB TServer setup. The next day, I had a 30 node cluster with latency on par with PostgreSQL.\nMaybe I’m still in the honeymoon period with YugabyteDB. Whatever, so far I love the product and the execution. It behaves like PostgreSQL. For the tooling, it’s basically PostgreSQL with a lot of muscle behind. Like the new tablespaces implementation taking the Postgres concept one step further to allow geo-aware placement.\nWhile the Postgres layer is amazing in itself, YugabyteDB is not only about Postgres. There’s also the YCQL layer which is basically Cassandra on steroids and there’s the Yedis layer—Redis on steroids. There’s change data capture, point in time restore, Kafka ingest integration and many more features adding up to a very solid all round database solution ready for the cloud-native XXI century.\nIt’s clear that the team behind the product understands the problem space and is focused on solving their clients problems in open source rather than chasing an ever longer feature list. There does not seem to be a secondary agenda. There are two commercial products next to the open source version: the enterprise fully managed solution and the self-managed licensed solution. The company behind clearly knowns what, how and for whom.\nIf you enjoy using PostgreSQL, you’ll love YugabyteDB.\n  ACID ↩︎\n CAP theorem ↩︎\n Consistent hashing ↩︎\n Distributed hash table ↩︎\n Eventual consistency ↩︎\n YugabyteDB ↩︎\n   ","description":"It’s like PostgreSQL with the storage part ripped out and RAFT plugged in","tags":["yugabytedb","postgresql"],"title":"On YugabyteDB","uri":"/posts/2021-05-30-on-yugabytedb/"},{"content":"S3, Azure Blob, Google Storage and Minio, they’re all a K/V storage at the core. Yes, of course, they provide much, much more functionality beyond that but—at the core—object storage systems, S3, GCS, Minio and the likes, are K/V stores.\nEach one provides a HTTP interface. Putting the data in object storage is done via HTTP PUT or POST requests, fetching is available via GET. To check if an object under the key exists, that’s a HEAD request. Deleting is a single HTTP DELETE away.\nIt is somewhat interesting that we usually don’t think about S3 or GCS as key/value systems. They’re hidden behind a HTTP layer and our perception tells us they’re some sort of directories and files. They’re certainly not file systems.\nIf we start treating object storage as K/V, we can quickly find resemblance to other K/V systems. For example Redis or Cassandra. Or a PostgreSQL table with a primary key.\nLike S3 or Azure Blob, these dedicated systems provide functionality beyond just K/V. If the item to be fetched is identifiable by a well known ID and that fetch is one hop away, there’s no need to filter or join over anything else, that’s definitely K/V like!\nOry Hydra as an example The lifecycle of an OAuth token is not very complex. Once a token is generated, it lives for some period of time. Maybe 10 minutes, maybe a month. They’re handed over to an external application which holds on to them until a new token is needed. A token serves a couple of major purposes:\n it assures the holder that the data in the token comes from the issuer; this fact can be proven by validating the token’s signature, it can be sent to another application which can also—in turn—itself validate that the middleman has not tampered with the original data.  There are two major types of tokens:\n JWT tokens: contains readable content, anybody can decode them, the information is readable, opaque tokens: these are not meant to be read, only the issuer understands what’s inside.  Either type is issued in response to some event requiring assurance of a successful action on the issuer side. Most often, that’s an authentication or authorization. Tokens are idempotent. Once issued, they do not change.\nMost often, the following actions are performed on the tokens:\n signature validation: token does not need to be sent to the issuer, the signature can be validated on the consumer side by loading public keys from issuer’s JWKS verification: is the token active?  usually validated by checking the expiry timestamp and verifying that the server has not invalidated the token yet   invalidation: the token should not be recognized anymore refreshing a token: in return for a valid refresh token, a new access token is issued  Even if the token is forwarded to a third party and used for some fancy application specific logic, the third-party will essentially do one of the four of actions listed above.\nthe big question So the big question this write up asks: is the database system even needed to store them?\nThe signature validation does not require a look up. Even the issuer does not need to look anything up. Verification, in case of a database system, usually implies checking if the row for the token exists. Eventually, if there is an invalidation row in another table. Invalidation usually means removing the token from the table or creating the invalidation row in another table.\nThese operations are the same for a refresh token but a new token is generated, if refresh token is still valid.\nThis looks awfully close to K/V.\nSo what’s the point of having a database for that at all? Why not using globally distributed object storage instead?\nmental exercise Testing for expired token can be done in two complementary ways:\n issue a HEAD request against the key with the token object, 404 Not Found means the token is not valid, eventually, a HEAD request can be issued to test if there is an invalidation object for the respective token.  Maybe these operations can be reversed depending on how probable the invalidation of a token is. Additionally, all of the object storage systems provide object expiration so the cleanup of old tokens comes for free, as in.\nInvalidation is an operation fulfilling the criteria for the conditions above.\nsome numbers Let’s go through some numbers based on a semi-real example. A client with roughly 1000 accounts, each account logging in once a week or so. That’s about 4000 tokens a month with further, say, 600K token verifications a month. If we wanted to run a cheapest version of HA database in the public cloud, options are (among other, of course):\n Cloud SQL: $0.0966 per vCPU / GB RAM, that’s $69.552 / month based on 30 days AWS RDS Multi-zone: $0.36 for 2 vCPU with 1GB RAM burstable db.t3.micro instances: $25.92 / month  It’s definitely not easy to run a HA database system with reasonable performance for less than $25 / month, even if we consider the likes of Hetzner.\nNeither of these could be recommended for a production public facing system. The prices above are compute only. There is no:\n data transfer cost, maintenance cost, backup / restore, storage and snapshots.  My point is, neither of these prices reflects reality and the actual cost will be definitely higher.\nLet’s compare this with some back-of-the-envelope calculations for object storage operations.\nAWS S3 storage cost, for a few thousand tokens, is going to be negligible. We are taking about data in megabytes, not gigabytes. Azure, GCS would be the same. Object storage operations are by far the most expensive. S3 charges $0.005 / 1000 PUT requests and $0.004 / 1000 GET and other (including HEAD) requests.\nGCS divides the operation in 2 classes. storage.object.put belongs to class A, these are $0.05 / 10000 operations, storage.object.get is a class B operation and these are priced at $0.004 per 10000 operations, effectively 10 times cheaper than S3.\nIn terms of number of operations:\n issuing a token is 1 PUT request, token verification is at most 2 HEAD requests token refresh is at most 2 HEAD requests and 1 PUT request.  That’s respectively:\n S3 Standard:  $0.000005 for issuing a token $0.000008 for a token verification worst case scenario and $0.000004 for best case $0.000008+$0.000005 per token refresh worst case scenario and $0.000004+$0.000005 for best case   GCS:  $0.000005 for issuing a token (price the same as S3) $0.0000008 for a token verification worst case scenario and $0.0000004 for best case (10 times cheaper than S3) $0.0000008+$0.000005 per token refresh worst case scenario and $0.0000004+$0.000005 for best case (PUT has the same price as S3 but GET is 10 times cheaper)    If there is no need to support invalidation and expiry can be checked at the edge, there is no need to touch storage at all. If invalidation check is opportunistic, not every token will be checked for invalidation. However, for the most pessimistic usage, this client could run their token storage for:\n S3 Standard: $0.000005 * 4000 tokens + $0.000008 * 600K verifications = $4.82 / month GCS: $0.000005 * 4000 tokens + $0.0000008 * 600K verifications = $0.50 / month  what about compute Aha, I’m glad this question came up. Turns out that running Ory Hydra on AWS Lambda using a Docker container results in an end to end request latency of ~100ms / request and it fits easily within the minimum 128MB execution runtime. That’s a mere $0.000000021 per request. The ~600K requests a month would cost that client a whopping $0.0126.\nCold start takes around 1.5 second but with 600K verifications, there is a request roughly every ~4 seconds so the cold start would not be frequent. They can be be minimized by trimming Hydra down, if there was no need to initialize ORM for example.\nIt’s fair to say that maybe $30 or $70 / month is not a lot of money. But consider that there are places where that might not be the case. Or maybe it’s a side project in an evaluation phase and that $70 becomes $840 / year. Finally, simply compare the number of requests per month and imagine that your $70 / month is used for a mostly idle resource. Why paying for a mostly idle resource at all?\nThere’s definitely a clear cut off point where a database becomes a reasonable choice worth paying for.\nMmm, how far away is it, though.\nno infrastructure The side effect here is: it is possible to run all this authentication and authorization stuff without any permanent compute resources. The same principle applies to Keto and Oathkepeer. All the decision related data can be easily put in a container, if it needs to be.\nThere is no need for a database in any of these systems. Kratos is the only difficult case.\n","description":"S3, Azure Blob, Google Storage and Minio, they’re all a K/V storage at the core","tags":["ory","hydra","kratos","keto","oathkeeper","serverless"],"title":"Do you really need a database for that Ory stack?","uri":"/posts/2021-05-23-do-you-really-need-a-database-for-that-ory-stack/"},{"content":"I must admin—I struggled understanding Oathkeeper. Looking back, I think the reason was, I compared it one for one to things like Traefik or Envoy. Turns out, Oathkeeper does not necessarily intend replacing a reverse proxy, although many people probably use it as such.\nI was glossing over it for a long time and recently decided to come back to it. While reading the Oathkeeper Docs1 Introduction section, the part about using Oathkeeper as a decision API for Envoy, Ambassador and Nginx started working.\nDigging through issues and pull requests2, I found some recent commits mentioning Traefik ForwardAuth support. Little bit more digging and I found this little gem:\nGET /decisions HTTP/1.1 Accept: application/json with the following description:\n This endpoint mirrors the proxy capability of ORY Oathkeeper’s proxy functionality but instead of forwarding the request to the upstream server, returns 200 (request should be allowed), 401 (unauthorized), or 403 (forbidden) status codes.\n It’s in the clear, in the REST API documentation! Yes, that’s it.\nwhat is Oathkeeper Oathkeeper authorizes HTTP requests by matching a request to a rule from a set of defined rules, applying some guarding logic and allowing or denying a request. Oathkeeper has two modes of work:\n a proxy: if a request is allowed, it is forwarded to the upstream an arbiter validating a request and returning HTTP success status or an error  In both cases a request originating from the HTTP client must, at least, touch Oathkeeper. In the proxy mode, it flows completely through it.\nOathkeeper applies a maximum of four internal steps to each request:\n first, it finds if there is any rule matching the request URL and eventually a HTTP method, the matches can be either regexp or glob if there are no matching rules, the request is denied otherwise, the request passes through a pipeline of maximum three types of, let’s call them filters, for a lack of better word:  authenticators: responsible for validating credentials authorizers: permissions the subject, basically: is the user behind the request allowed to execute this request mutators: transforms input credentials into upstream credentials    rules The rules are always defined as references to JSON or YAML files in the main Oathkeeper YAML configuration. For example /etc/config/ok/oathkeeper.yaml (furhter always referred to as global configuration):\n1 2 3 4  access_rules:repositories:- file:///etc/config/ok/rules.json- https://example.com/ok.yaml  More than one repository can be defined, rules can be loaded from Azure Blob Storage, Google Storage, S3, HTTPS, local files and more.\nSadly, there is no support for ETCD or Consul.\nAn example might look like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  [ { \"id\": \"some-id\", \"version\": \"v0.36.0-beta.4\", \"upstream\": { \"url\": \"http://my-backend-service\", \"preserve_host\": true, \"strip_path\": \"/api/v1\" }, \"match\": { \"url\": \"http://my-app/some-route/\u003c.*\u003e\", \"methods\": [\"GET\", \"POST\"] }, \"authenticators\": [{ \"handler\": \"noop\" }], \"authorizer\": { \"handler\": \"allow\" }, \"mutators\": [{ \"handler\": \"noop\" }], \"errors\": [{ \"handler\": \"json\" }] } ]   Rules have unique IDs, can be optionally tagged with a supported Oathekeeper version and are matched according to the match section of the rule.\nIn the case above, any GET or POST request to any URL starting with http://my-app/some-route/ would be matched and authenticators, the authorizer and mutators would be applied. In this case, the request would be automatically allowed because there is only one noop authenticator. This rule appears to be used as a proxy rule because the upstream is defined.\nThe decision to allow or deny the request is made based on the evaluation of authenticators and optionally the authorizer. Mutators allow amending the credentials for the authenticator. For example, it could read a bearer token out of a cookie and place it in an Authorization header instead.\nIn order to use an authenticator, authorizer or a mutator in a rule, the respective item must be enabled in the main Oathkeeper YAML configuration.\nIf the respective item type requires additional configuration, the minimum configuration must be specified in the global configuration but can be overridden in individual rules.\nauthenticators An authenticator inspects the HTTP request and returns a boolean decision based on the implementation logic. At the time of writing, there are following authenticators available:\n noop: bypass authentication, authorization and mutation, forward or allow the request further downstream outright unauthorized: outright reject the request anonymous: if there is no Authorization header, set the subject to anonymous (subject can be configured) cookie_session: forwards the request headers, path and method to a session store, basically: validate session based on headers (cookies, these are headers after all…) bearer_token: similar to cookie_session but allows configuring the source of the token: cookie, header or query parameter oauth2_client_credentials: uses the Authorization: Basic to perform OAuth 2.0 credentials grant to check if the credentials are valid, with a bit of reverse proxy trickery, if could potentially facilitate Hydra with credentials grant oauth2_introspection: uses the token introspection endpoint to validate the token and required scopes jwt: requires an Authorization: Bearer and assumes a JWT token, validates the signature of the token  Every authenticator type has its dedicated configuration parameters. The configuration can be specified in the rule. As mentioned earlier, to be able to use an authenticator in the rule, the authenticator must be enabled globally.\nFor example, to use noop:\n1 2 3  authenticators:noop:enabled:true  authorizers An authorizer ensures that the subject (the entity behind the request) has sufficient permissions to issue the request. There are currently five different authorizers with one of them being a Keto 0.5 specific legacy authorizer:\n allow: permits outright deny: denies outright remote: this one issues a POST request to the configured remote authorization endpoint and sends the original request body, if the remote returns 200 OK, the request is allowed, if the endpoint returns 403 Forbidden, the request is denied remote_json: this one issues a POST request to the configured remote authorization endpoint and sends configured JSON payload in the POST body, if the remote returns 200 OK, the request is allowed, if the endpoint returns 403 Forbidden, the request is denied keto_engine_acp_ory: is the Keto 0.5 specific authorizer, unless you are already using Keto 0.5, you’ll never use this one  As with authenticators, the authorizers have to be explicitly enabled in the global configuration.\nFor example:\n1 2 3  authorizers:noop:enabled:true  The remote_json authorizer can be used to authorize the request with Keto 0.6 Zanzibar fanciness:\n1 2 3 4 5 6 7 8 9 10 11  authorizer:handler:remote_jsonconfig:remote:http://keto:4466/checkpayload:|{ \"namespace\": \"default-namespace\", \"subject\": \"{{ print .Subject }}\", \"object\": \"reports\", \"relation\": \"edit\" }  The example shows that it is possible to use templating to populate the JSON payload from an AuthenticationSession object, where the respective golang code is:\n1 2 3 4 5 6 7 8 9 10 11 12 13  type AuthenticationSession struct { Subject string `json:\"subject\"` Extra map[string]interface{} `json:\"extra\"` Header http.Header `json:\"header\"` MatchContext MatchContext `json:\"match_context\"` } type MatchContext struct { RegexpCaptureGroups []string `json:\"regexp_capture_groups\"` URL *url.URL `json:\"url\"` Method string `json:\"method\"` Header http.Header `json:\"header\"` }   Frankly, this bit lacks proper documentation because it is totally not clear where is this constructed, based on what data, how exactly is the subject extracted and when the rest of the data is available.\nAt least in the context of a JWT bearer token, the subject appears to be the JWT token subject.\nmutators Mutators transform an incoming credential into an outgoing credential. Following mutators are available:\n noop: no mutation, forward headers as they came in id_token: converts the subject into a signed ID Token, the back end service can verify this token using the public key of Oathkeeper JWKS header: allows constructing additional headers from the HTTP request cookie: similar to header but constructs named cookies hydrator: allows fetching additional data from an external API and populates the mysterious AuthenticationSession object mentioned earlier, hmmm… maybe this can somehow be used to dynamically populate object and relation in the remote_json authorizer?  error handlers Optionally, as the last step of a rule, a method of handling the authorization error can be specified using the errors rule section. Following handlers are available:\n json: returns the error as a JSON payload with application/json content type redirect: redirect the request to the location using HTTP 301 or 302 status, configurable www_authenticate: responds with HTTP 401 status and the WWW-Authenticate header  The default handler is json. The order of the default handlers can be changed in the global configuration, for example:\n1 2 3 4  errors:fallback:- redirect- json  As with any other object type, the error handler type has to be explicitly enabled in the global config, for example:\n1 2 3 4  errors:handlers:json:enabled:true  Error handlers can be conditionally matched using when clauses:\n1 2 3 4 5 6 7 8 9 10  errors:redirect:enabled:trueconfig:to:https://bad.robotwhen:- request:header:accept:- application/json  two modes of operation I have briefly mentioned that Oathkeeper has two modes of operation: the proxy and the pure decision API.\nthe proxy The flow goes roughly like this:\nThe request is validated via Oathkeeper. When found okay, it is proxied upstream. It’s possible to instruct Oathkeeper to keep the original request host and strip a path prefix. These request go via the proxy server. The proxy server is configured in the global configuration, for example:\n1 2 3 4  serve:proxy:host:0.0.0.0port:4455  The proxy server does not define any custom routes, it serves as a catch all router, as you’d expect from, well, a proxy.\nthe decision API This is the interesting one. In the simplest case, it works like this:\nIn this mode, Oathkeeper never sends the requests to upstream. In fact, a rule used with the decision mode does not need to define the upstream section. This would work perfectly fine:\n1 2 3 4 5 6 7 8 9 10 11 12 13  [ { \"id\": \"some-decision-id\", \"match\": { \"url\": \"http://my-app/some-route/\u003c.*\u003e\", \"methods\": [\"GET\", \"POST\"] }, \"authenticators\": [{ \"handler\": \"noop\" }], \"authorizer\": { \"handler\": \"allow\" }, \"mutators\": [{ \"handler\": \"noop\" }], \"errors\": [{ \"handler\": \"json\" }] } ]   Assuming that the client wants to authorize a GET /some-route/abc request, it would send a GET /decisions/some-route/abc request to Oathkeeper in API mode. Oathkeeper would then look up the /some-route/abc rule for GET method and run its regular pipeline on it.\nIf the request was authorized, a HTTP 200 OK would be returned, otherwise the result would be HTTP 403 Forbidden.\nIt’s exactly the same as proxy mode but there is no proxying going on.\nThe API server is configured in the global configuration:\n1 2 3 4  serve:api:host:0.0.0.0port:4456  and would normally be placed behind a reverse proxy. For example, the following Nginx configuration could take advantage of this:\nhttp { #... server { #... location /private/ { auth_request /auth; auth_request_set $auth_status $upstream_status; } location = /auth { internal; proxy_pass http://oathkeeper:4456/decisions$request_uri; #... } } } summary This was a quick, 10 minute-like, introduction into Oathkeeper. A brain dump of sorts.\nI’m not sure if I’d ever use it as a pure proxy but the decision API looks very neat. Putting it behind Traefik would be an awesome feat but first investigations imply that a custom ForwardAuth plugin would be required.\nTraefik would be an awesome choice because of automatic ACME and the ability to use a single ForwardAuth to serve multiple sites via single Oathkeeper.\nThe reason why a custom ForwardAuth might be required, is that Traefik forwards the original request data in headers. The custom ForwardAuth would have to issue the Oathkeeper request constructed from those headers.\nWell, it’s not so complicated after all…\n  Oathkeeper Docs ↩︎\n Integrate with Traefik, Nginx, Ambassador, Envoy on GitHub ↩︎\n   ","description":"","tags":["ory","oathkeeper","traefik","iap"],"title":"Figuring out Ory Oathkeeper","uri":"/posts/2021-05-20-figuring-out-oathkeeper/"},{"content":"There was an interesting question coming up related to the previous article on RBAC with Ory Keto1.\nThe question was:\n how do I list the roles of a user\n At the end of the previous article, the solution allowed finding out if the user is allowed to access the resources. But, indeed, what I have not discussed was how to get the roles the user is assigned to.\nThe final tuples for Fry and Bender, after Fry was demoted, looked like this:\ndev-director#member@Fry dev-director#member@Bender These relations tell that dev-director contains Fry and Bender. We can test this out with the following query:\n1  curl --silent 'http://localhost:4466/expand?namespace=default-namespace\u0026object=dev-director\u0026relation=member\u0026max-depth=2' | jq '.'   1 2 3 4 5 6 7 8 9 10 11 12 13 14  { \"type\": \"union\", \"subject\": \"default-namespace:dev-director#member\", \"children\": [ { \"type\": \"leaf\", \"subject\": \"Bender\" }, { \"type\": \"leaf\", \"subject\": \"Fry\" } ] }   What if we want to know if the roles Fry belongs to? We could naively ask using the subject:\n1  curl --silent 'http://localhost:4466/expand?namespace=default-namespace\u0026subject=Fry\u0026relation=member\u0026max-depth=2' | jq '.'   What we find out is, the result is completely wrong:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34  { \"type\": \"union\", \"subject\": \"default-namespace:#member\", \"children\": [ { \"type\": \"leaf\", \"subject\": \"Bender\" }, { \"type\": \"leaf\", \"subject\": \"Fry\" }, { \"type\": \"leaf\", \"subject\": \"Hermes\" }, { \"type\": \"leaf\", \"subject\": \"default-namespace:it-director#member\" }, { \"type\": \"leaf\", \"subject\": \"default-namespace:it-director#member\" }, { \"type\": \"leaf\", \"subject\": \"default-namespace:dev-director#member\" }, { \"type\": \"leaf\", \"subject\": \"default-namespace:it-director#member\" } ] }   In a real-world application, this would not leak PII, because you’d use UUIDs instead of names. However, this path is a no go.\nreverse user to role binding The new additional tuple looks like this:\nFry#is-member@dev-director 1 2 3 4 5 6  curl -XPUT --data '{ \"namespace\": \"default-namespace\", \"object\": \"Fry\", \"relation\": \"is-member\", \"subject\": \"dev-director\" }' http://localhost:4467/relation-tuples   Now, we can ask:\n1  curl --silent 'http://localhost:4466/expand?namespace=default-namespace\u0026object=Fry\u0026relation=is-member\u0026max-depth=2' | jq '.'   1 2 3 4 5 6 7 8 9 10  { \"type\": \"union\", \"subject\": \"default-namespace:Fry#is-member\", \"children\": [ { \"type\": \"leaf\", \"subject\": \"dev-director\" } ] }   For each user / role mapping, there are two tuples:\nrole#member@user user#is-member@role but there’s more We could go one step further. In the previous article, the following was established:\n at the time of permission evaluation, both the object for which the permission is being validated, and the subject, are known\n When Fry has got his opportunity, we ended up with the following configuration (plus the new reverse binding):\nproduction-viewer#member@default-namespace:dev-director#member production-creator#member@default-namespace:fast-dev-director#member production-deleter#member@default-namespace:fast-dev-director#member dev-director#member@Fry fast-dev-director#member@Fry Fry#is-member@dev-director Fry#is-member@fast-dev-director What if we had this instead:\nproduction-viewer#member@dev-director # \u003c--- this is added production-creator#member@fast-dev-director # \u003c--- this is added production-deleter#member@fast-dev-director # \u003c--- this is added production-viewer#member@default-namespace:dev-director#member production-viewer#member@default-namespace:fast-dev-director#member dev-director#member@Fry fast-dev-director#member@Fry Fry#is-member@dev-director Fry#is-member@fast-dev-director where the new tuples are:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  curl -XPUT --data '{ \"namespace\": \"default-namespace\", \"object\": \"Fry\", \"relation\": \"is-member\", \"subject\": \"fast-dev-director\" }' http://localhost:4467/relation-tuples curl -XPUT --data '{ \"namespace\": \"default-namespace\", \"object\": \"production-creator\", \"relation\": \"member\", \"subject\": \"fast-dev-director\" }' http://localhost:4467/relation-tuples curl -XPUT --data '{ \"namespace\": \"default-namespace\", \"object\": \"production-deleter\", \"relation\": \"member\", \"subject\": \"fast-dev-director\" }' http://localhost:4467/relation-tuples   and all tuples together mean:\n Anybody who is dev-director or fast-dev-director, a member of dev-director or a member of fast-dev-director, can act as production-viewer. Fry is a member of both using the bi-directional mapping and also a member of production-creator and production-creator via fast-dev-director.\n We have all the benefits of the original solution with an extra superpower.\nWhen we ask if create can be performed by Fry, what we really mean is is create allowed for any role Fry holds.\nThus, after asking for Fry’s membership, we would iterate over every role and ask Keto directly for the role permission:\n1 2 3 4 5 6 7 8 9  for role in $(curl --silent 'http://localhost:4466/expand?namespace=default-namespace\u0026object=Fry\u0026relation=is-member\u0026max-depth=2' | jq '.children[].subject' -r) do echo \"$role:\" $(curl --silent -XPOST --data '{ \"namespace\": \"default-namespace\", \"object\": \"production\", \"relation\": \"create\", \"subject\": \"'$role'\" }' http://localhost:4466/check) done   dev-director: {\"allowed\":false} fast-dev-director: {\"allowed\":true} This is interesting because it allows implementing Keycloak style Authorization Services Decision Strategy2.\nWhen associating policies with a permission, you can also define a decision strategy to specify how to evaluate the outcome of the associated policies to determine access.\n Unanimous: The default strategy if none is provided. In this case, all policies must evaluate to a positive decision for the final decision to be also positive. Affirmative: In this case, at least one policy must evaluate to a positive decision for the final decision to be also positive. Consensus: In this case, the number of positive decisions must be greater than the number of negative decisions. If the number of positive and negative decisions is equal, the final decision will be negative.  The result above could be the Affirmative strategy.\nclosing words The former solution, with less tuples, is definitely easier to reason about. Ha, but let’s be honest. Once UUIDs are used instead of literals, no sane mind can follow!\nThe latter solution opens the door for extra features. If I was rooting for a high level RBAC solution, I’d go for the more complex one. The complexity is an implementation detail.\nGetting the consistency right might be tricky with Keto REST / gRPC API only. This would probably require some sort of stateful state machine, possibly on top of etcd but it’s definitely doable. The extra benefit of the decision strategy is a nice side effect.\nFood for thought.\n  RBAC with Ory Keto ↩︎\n Keycloak Authorization Services Decision Strategy ↩︎\n   ","description":"More thoughts on RBAC with Keto 0.6","tags":["ory","keto","iam","rbac","zanzibar","keycloak"],"title":"Keto RBAC - listing roles of a user","uri":"/posts/2021-05-17-keto-rbac-listing-roles-of-a-user/"},{"content":"Role-base Access Control is an access control method whereby the entity roles define the level of access. Usually when talking about RBAC, the entity is a person and the object is a resource or a task (function) granted to a person. The usual example goes like this:\n In an organization, the job functions define the roles of employees. Only employees in specific roles are allowed to execute certain tasks. The employees are given permissions to execute these tasks. Sometimes the employees may gain additional permissions to execute more tasks, sometimes certain permissions are taken away and the employees cannot execute selected tasks anymore.\n Update, 17th of May 2021: When publishing this article yesterday, I have incorrectly assumed that usersets aren’t implemented by Keto. The usersets are implemented, as explained by Patrik from the Ory team here1. What is not yet implemented is the rewrites functionality. I have updated the article to reflect this new fact.\ntheory RBAC The English Wikipedia has a very good, comprehensive entry on RBAC2. We can find a number of relevant RBAC related definitions, mainly:\n Role assignment: A subject can exercise a permission only if the subject has selected or been assigned a role. Role authorization: A subject’s active role must be authorized for the subject. With rule 1 above, this rule ensures that users can take on only roles for which they are authorized. Permission authorization: A subject can exercise a permission only if the permission is authorized for the subject’s active role. With rules 1 and 2, this rule ensures that users can exercise only permissions for which they are authorized.  These can be a subject to additional organizational hierarchy where higher-level roles incorporate the ones of the subordinates. With this in mind, we find the conventions useful:\n S = Subject = A person or automated agent R = Role = Job function or title which defines an authority level P = Permissions = An approval of a mode of access to a resource SE = Session = A mapping involving S, R and/or P SA = Subject Assignment PA = Permission Assignment RH = Partially ordered Role Hierarchy. RH can also be written: ≥ (The notation: x ≥ y means that x inherits the permissions of y.)  A subject can have multiple roles. A role can have multiple subjects. A role can have many permissions. A permission can be assigned to many roles. An operation can be assigned to many permissions. A permission can be assigned to many operations.    Further, Wikipedia suggests the following set theory notation:\n PA ⊆ P × R SA ⊆ S × R RH ⊆ R × R  Grokking these is straightforward, in order:\n permission assignment is a subset of permissions multiplied by roles subject assignment is a subset of subjects multiplied by roles role hierarchy is a subset of roles multiplied by other roles  In other words:\nthe subject (a user) is allowed (has the permission) to execute certain action (on an object) when they have certain roles; the roles can inherit permissions of other roles\nThe second part is important because it suggests that the permissions of certain roles in the hierarchy can change.\nWhat’s crucial, we can parse the first statement in reverse:\nan action can be performed on an object by a subject holding specific roles\nLet’s hold on to that thought and proceed.\nthe…ory keto Keto is an implementation of the Zanzibar whitepaper3 from Google. Zanzibar, thus Keto, is used to implement Access Control Lists (ACL). RBAC differs from ACL in that RBAC assigns permissions to operations instead of objects. Keto (Zanzibar) does not have any knowledge of the system it controls the access on behalf of. There are four major terms in the Zanzibar whitepaper:\n a relation tuple an object a relation a subject  where the relation tuple is a result of an object, permission and a subject. Straight from the whitepaper:\n〈tuple〉::=〈object〉‘#’〈relation〉‘@’〈user〉 〈object〉::=〈namespace〉‘:’〈objectid〉 〈user〉::=〈userid〉|〈userset〉 〈userset〉::=〈object〉‘#’〈relation〉 This is the notation you can see in the Keto examples on the internet. For now, I will ignore the namespace part and use a single namespace for the example further down.\nIn the relation tuple, the user and relation are pretty obvious. The object part leaves room for improvisation. A little bit higher up, we have discussed that RBAC assigns permissions to operations instead of objects. Before moving on, we need to establish a couple of facts for the purpose of this article:\n the Zanzibar object implies an RBAC operation, the Zanzibar subject is the RBAC object, the relation defines the permission.  so, RBAC with ACL? I hear you say. But why not. Zanzibar is the outcome of Google’s work on the global access control system powering things like Calendar, Cloud, Drive and so on.\nTheir Cloud products alone contain dozens of sub-products, each defining own roles. Each of those roles carries object permissions (general service actions or individual object actions) the user can be granted.\nhow to proceed From the previous theory, two statements are the strongest signals of how to move forward:\n RBAC differs from ACL in that RBAC assigns permissions to operations instead of objects. An action can be performed on an object by a subject holding specific roles.  The first one is a constraint. Actions, say granular read or write, should not be assigned to individual objects.\nThis makes sense because if a certain action should no longer be allowed for a certain role, there should be no need to iterate over every object to remove a permission. Instead, a permission should be revoked from a role and the users holding these roles should no longer be allowed to perform the revoked action.\nThe second statement is more of a clue. At the time of the decision making, two criteria are known:\n the object on which the action is to be performed the user (subject) for which the decision has to be reached  It is safe to assume that knowing the user implies knowing the roles the user belongs to. The second statement stems from the first one and shows that asking is the user allowed to execute this action is the wrong thing to do.\nThe correct question to ask is: can an action be performed on this object by someone holding these roles.\nthe scenario To visualize a PoC RBAC with Keto, let’s consider the following imaginary scenario.\n There is a SaaS business offering compute services. Compute resources can be created, viewed and deleted. The Company is the client of SaaS and has a compute environment called production. The IT director, Hermes, can create, view and delete compute resources. The Company employs two development directors: Fry and Bender. Both of them can only view the resources in production. As the time passes, the Company asks Fry to speed up the deployment to production and gives him the permissions to create and delete resources in production.  The SaaS and the Company will not be included in the implementation, they’re irrelevant to this example.\nimplementation Let’s start by launching the Ory stack locally:\n1 2 3  git clone https://github.com/radekg/ory-reference-compose.git cd ory-reference-compose/compose docker-compose -f compose.yml up psql keto-migrate keto   Keto read API runs on localhost:4466 and the write API is reachable via localhost:4467. The compose stack creates a single namespace called default-namespace.\nLet’s start with defining the production platform by creating the following relation tuples:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  curl -XPUT --data '{ \"namespace\": \"default-namespace\", \"object\": \"production\", \"relation\": \"create\", \"subject\": \"default-namespace:production-creator#member\" }' http://localhost:4467/relation-tuples curl -XPUT --data '{ \"namespace\": \"default-namespace\", \"object\": \"production\", \"relation\": \"view\", \"subject\": \"default-namespace:production-viewer#member\" }' http://localhost:4467/relation-tuples curl -XPUT --data '{ \"namespace\": \"default-namespace\", \"object\": \"production\", \"relation\": \"delete\", \"subject\": \"default-namespace:production-deleter#member\" }' http://localhost:4467/relation-tuples   In the Zanzibar notation, these are:\n production#create@(default-namespace:production-creator#member): roughly translates to: production allows create by anybody bound as a member of production-creator production#view@(default-namespace:production-viewer#member): production allows view by anybody bound as a member of production-viewer production#delete@(default-namespace:production-deleter#member): production allows delete by anybody bound as a member of production-deleter  These relations can be read as properties (or capabilities) of the production platform. The production-[creator|viewer|deleter] is a handle for a referencing object, these referencing objects, called production-[creator|viewer|deleter] respectively, can be seen as the bindings.\nLet’s bind the IT director first. The following relation tuples together define a role called it-director.\nproduction-creator#member@default-namespace:it-director#member production-viewer#member@default-namespace:it-director#member production-deleter#member@default-namespace:it-director#member What we are saying here is that any person who is a member of the it-director role will be treated as a member of the respective production-X role, which we have already bound to respective create, view and delete.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  curl -XPUT --data '{ \"namespace\": \"default-namespace\", \"object\": \"production-creator\", \"relation\": \"member\", \"subject\": \"default-namespace:it-director#member\" }' http://localhost:4467/relation-tuples curl -XPUT --data '{ \"namespace\": \"default-namespace\", \"object\": \"production-viewer\", \"relation\": \"member\", \"subject\": \"default-namespace:it-director#member\" }' http://localhost:4467/relation-tuples curl -XPUT --data '{ \"namespace\": \"default-namespace\", \"object\": \"production-deleter\", \"relation\": \"member\", \"subject\": \"default-namespace:it-director#member\" }' http://localhost:4467/relation-tuples   If there was a higher level system responsible for actual role management, the tool could present a role object called IT Director with these granular tuples hidden from view. The role management operator would be working with that abstraction instead of these granular items.\nNext, we can bind the development director. Originally, this role should only be allowed the view of the production resources:\nproduction-viewer#member@default-namespace:dev-director#member 1 2 3 4 5 6  curl -XPUT --data '{ \"namespace\": \"default-namespace\", \"object\": \"production-viewer\", \"relation\": \"member\", \"subject\": \"default-namespace:dev-director#member\" }' http://localhost:4467/relation-tuples   Let’s put the people in their actual positions:\nit-director#member@Hermes dev-director#member@Fry dev-director#member@Bender 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  curl -XPUT --data '{ \"namespace\": \"default-namespace\", \"object\": \"it-director\", \"relation\": \"member\", \"subject\": \"Hermes\" }' http://localhost:4467/relation-tuples curl -XPUT --data '{ \"namespace\": \"default-namespace\", \"object\": \"dev-director\", \"relation\": \"member\", \"subject\": \"Fry\" }' http://localhost:4467/relation-tuples curl -XPUT --data '{ \"namespace\": \"default-namespace\", \"object\": \"dev-director\", \"relation\": \"member\", \"subject\": \"Bender\" }' http://localhost:4467/relation-tuples   We can now ask Keto if the respective people can perform their tasks, for example, can Hermes create resources in production?\n1 2 3 4 5 6  curl -XPOST --data '{ \"namespace\": \"default-namespace\", \"object\": \"production\", \"relation\": \"create\", \"subject\": \"Hermes\" }' http://localhost:4466/check   1  {\"allowed\":true}   What about Fry?\n1 2 3 4 5 6  curl -XPOST --data '{ \"namespace\": \"default-namespace\", \"object\": \"production\", \"relation\": \"delete\", \"subject\": \"Fry\" }' http://localhost:4466/check   1  {\"allowed\":false}   But, both him and Bender, should be able to view:\n1 2 3 4 5 6  curl -XPOST --data '{ \"namespace\": \"default-namespace\", \"object\": \"production\", \"relation\": \"view\", \"subject\": \"Bender\" }' http://localhost:4466/check   1  {\"allowed\":true}   So far, so good.\nFry’s opportunity The Company has finally decided to give Fry the opportunity and improve the production delivery speed.\nA new role has been carved out just for Fry:\n1 2 3 4 5 6  curl -XPUT --data '{ \"namespace\": \"default-namespace\", \"object\": \"fast-dev-director\", \"relation\": \"member\", \"subject\": \"Fry\" }' http://localhost:4467/relation-tuples   He can’t yet create or delete:\n1 2 3 4 5 6  curl -XPOST --data '{ \"namespace\": \"default-namespace\", \"object\": \"production\", \"relation\": \"create\", \"subject\": \"Fry\" }' http://localhost:4466/check   1  {\"allowed\":false}   For that, the fast-dev-director#member must be explicitly allowed by the respective production-X role.\n1 2 3 4 5 6 7 8 9 10 11 12  curl -XPUT --data '{ \"namespace\": \"default-namespace\", \"object\": \"production-creator\", \"relation\": \"member\", \"subject\": \"default-namespace:fast-dev-director#member\" }' http://localhost:4467/relation-tuples curl -XPUT --data '{ \"namespace\": \"default-namespace\", \"object\": \"production-deleter\", \"relation\": \"member\", \"subject\": \"default-namespace:fast-dev-director#member\" }' http://localhost:4467/relation-tuples   Can he now create and delete?\n1 2 3 4 5 6  curl -XPOST --data '{ \"namespace\": \"default-namespace\", \"object\": \"production\", \"relation\": \"create\", \"subject\": \"Fry\" }' http://localhost:4466/check   1  {\"allowed\":true}   1 2 3 4 5 6  curl -XPOST --data '{ \"namespace\": \"default-namespace\", \"object\": \"production\", \"relation\": \"delete\", \"subject\": \"Fry\" }' http://localhost:4466/check   1  {\"allowed\":true}   He should still be able to view through the dev-director role:\n1 2 3 4 5 6  curl -XPOST --data '{ \"namespace\": \"default-namespace\", \"object\": \"production\", \"relation\": \"view\", \"subject\": \"Fry\" }' http://localhost:4466/check   1  {\"allowed\":true}   He can, indeed. What about Bender?\n1 2 3 4 5 6  curl -XPOST --data '{ \"namespace\": \"default-namespace\", \"object\": \"production\", \"relation\": \"delete\", \"subject\": \"Bender\" }' http://localhost:4466/check   1  {\"allowed\":false}   This works as expected.\nFry has fried the production As Fry isn’t very smart, he did fry the production system and the Company was forced to remove his access. It was done like this:\n1  curl --silent -X DELETE 'http://localhost:4467/relation-tuples?namespace=default-namespace\u0026object=fast-dev-director\u0026relation=member\u0026subject=Fry'   And Fry could no longer create and delete in production.\nAlternatively, the Company should have been able to remove the role binding, like this:\n1 2  curl --silent -X DELETE 'http://localhost:4467/relation-tuples?namespace=default-namespace\u0026object=production-creator\u0026relation=member\u0026subject=default-namespace:fast-dev-director#member' curl --silent -X DELETE 'http://localhost:4467/relation-tuples?namespace=default-namespace\u0026object=production-deleter\u0026relation=member\u0026subject=default-namespace:fast-dev-director#member'   Which would leave Fry the fast-dev-director role assignment but the missing binding would no longer allow him to create and delete resources. However, these two delete statements do not seem to be removing the actual binding.\nsummary How does this example compare to the RBAC theory from the beginning of the article?\n Role assignment: A subject can exercise a permission only if the subject has selected or been assigned a role: check, Fry had to be explicitly assigned using the member relation to the fast-dev-director role. Role authorization: A subject’s active role must be authorized for the subject. With rule 1 above, this rule ensures that users can take on only roles for which they are authorized: check, respective production-X explicitly allows fast-dev-director#member Permission authorization: A subject can exercise a permission only if the permission is authorized for the subject’s active role. With rules 1 and 2, this rule ensures that users can exercise only permissions for which they are authorized: check, we have verified that even though Fry was given the fast-dev-director role, he could not create nor delete before the relevant production-X binding was established; the non-functional curl -XDELETE throws a spanner in the works but it’s unrelated to the fact that Keto does provide this capability and the core issue can be fixed  wrapping up This simple example shows that a simple RBAC is doable with Ory Keto. Without the rewrites, the configuration can be quite verbose but the example shows that different relation tuples can be chained together to form a more complex decision tree.\nThe example with deletes not removing the expected tuples indicate that Keto might still not be totally bulletproof but things can only get better.\nWorking directly with tuples can probably lead to a big headache. An efficient way of working would definitely require a tool managing and verifying the relation tuples based on some higher level role and membership abstraction.\n  usersets vs rewrites clarification ↩︎\n Role-based Access Control ↩︎\n Zanzibar: Consistent, Global Authorization System ↩︎\n   ","description":"Building bare bones RBAC with Keto 0.6","tags":["ory","keto","iam","rbac","zanzibar"],"title":"RBAC with Ory Keto","uri":"/posts/2021-05-15-rbac-with-ory-keto/"},{"content":"Software almost never runs in isolation. Today’s systems integrate with a vast number of external services. Ensuring reliability is difficult because the external dependencies, be it a database or an authentication system, adds an element of unpredictability which is difficult to emulate in isolation. A reliable system should account for the behavior of its dependencies. What does it help that an API is up and running when the underlying service it talks to hasn’t been accounted for a specific edge case and is causing an unexpected latency to my clients under certain conditions?\nEmulating input and output is easy with regular unit tests. Many people rely on the so called mocks to emulate external systems. Mocks are the pieces of code emulating external dependencies and behaving almost like them. Almost being the key word.\nAn example is go httptest.Server. It gives a fully functional, easy to configure HTTP server but the handlers do not necessarily behave like the dependency. These mocks only get us so far. Often the output depends on the quirks of the input, or the configuration of the system we integrate with. These edge cases may be very difficult to cover because they require reimplementing said quirks.\nThat’s exactly what I personally dislike about mocking. Reinventing this once or twice is maybe okay. Reinventing this for one, maybe two simple dependencies is maybe okay. When we intend to iterate our systems for years, we want to focus on our business problem. Our dependencies will mature, change scope, reimplement features and grow in complexity. We will add more dependencies and maybe replace existing ones with alternatives.\nMaintaining such mocked services adds unnecessary overhead. When a new version of the dependency is released, not only we have to adapt our services. Our testing infrastructure must be adapted to cover the changes.\nYes, it does give us the opportunity to understand how our dependencies work. This is always good know but this knowledge should come via means other than rewriting the logic of the dependency. If you have to use a hash map, you don’t write your own. You use a library. Why would an OAuth server be any different?\nA library is directly called in the path of our code. When the library dependency is upgraded, there’s an instant feedback loop. If the interfaces have changed or the behavior differs, the code will not compile or existing tests will fail immediately. With an external system, things are not often so obvious. Just because the API hasn’t changed doesn’t mean the dependency is doing the same thing. What would be great is to treat an external system in tests like a library.\nSome organizations have enough resources to supply the development team with a shared integration environment to run the tests against. This is great because there is no need to maintain these mocking services anymore. Tests run against a real database, areal authentication system, a real queue, and so on.\nThe cost is the most obvious downside. That infrastructure costs money and someone has to maintain it. If you’re a solo developer or a small team, you either may not have the financial resources to simply make it happen, or simply do not have the time to maintain all of that yourself.\nThere’s also the element of inflexibility. There’s a central system configured once, sharing the state between multiple instances of tests. This can be improved on if the developers have access to good equipment and can run parts of the infrastructure locally. The elephant in the room is that this leads to the duplication in environment setup. There’s a production system and another copy of the system which needs (sometimes can’t) be configured as close to the production as possible.\ntesting with containers Containers are the third available option. Writing a system in go and need to integrate with Kafka? Writing a Ruby app and need to talk to Postgres? This is easy with Docker. Just start Kafka in Docker, connect to it in the test and run real code paths right in tests. Need Redis? No problem, start a container. Postgres? Why not a container. etcd, MySQL, Minio, Hydra and Vault together for a complex integration? Containers can do this, regardless if your system is PHP, Ruby, Rust, go or ColdFusion!\nThe ultimate method is to spin up the containers right in tests. There are libraries in virtually every programming languages enabling this.\ngo and ory/dockertest Here, I’d like to focus on go. I’ve spent some time over more than two years evaluating the Ory platform and the dockertest library from Ory became an invaluable asset.\ndockertest is a very nice abstraction layer on top of the go-dockerclient making it so much easier to configure and execute containers with the focus on short lived tests.\nEvaluating the Ory platform meant setting up Hydra, Keto and Kratos, in many different configurations. Reducing the time of every iteration, even as simple as changing the underlying configuration, was crucial. What’s the better way than spinning up a fresh setup in every test and configuring it in-test?\nI haven’t found any better method than running containers. So what does it look like to run a container with dockertest? This is an example from its readme:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  postgres, err := pool.RunWithOptions(\u0026dockertest.RunOptions{ Repository: \"postgres\", Tag: \"11\", Env: []string{ \"POSTGRES_USER=test\", \"POSTGRES_PASSWORD=test\", \"listen_addresses = '*'\", }, }, func(config *docker.HostConfig) { // set AutoRemove to true so that stopped container goes away by itself \tconfig.AutoRemove = true config.RestartPolicy = docker.RestartPolicy{ Name: \"no\", } })   This gives a running Postgres database server. It’s so easy that there is no reason not to do it.\napp-kit-orytest A couple of weeks ago I have open sourced the app-kit-orytest library. This library provides preconfigured Hydra, Keto and Kratos components running against Postgres database and is available on GitHub.\nWith app-kit-orytest, the result is a full IAM / IdP environment right in the test.\nWhat does it take to have Hydra, Kratos and Keto in the test? It’s only a few lines of code - here’s an example:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47  package main import ( \"github.com/radekg/app-kit-orytest/common\" \"github.com/radekg/app-kit-orytest/hydra\" \"github.com/radekg/app-kit-orytest/keto\" \"github.com/radekg/app-kit-orytest/kratos\" \"github.com/radekg/app-kit-orytest/mailslurper\" \"github.com/radekg/app-kit-orytest/postgres\" ketoModels \"github.com/ory/keto-client-go/models\" \"github.com/stretchr/testify/assert\" ) func Test(t *testimng.T) { // Any component requires Postgres:  postgresCtx := postgres.SetupTestPostgres(t) // Start Hydra:  hydraCtx := hydra.SetupTestHydra(t, postgresCtx) defer hydraCtx.Cleanup() // Start Keto:  ketoCtx := keto.SetupTestKeto(t, postgresCtx, \"default-namespace\") defer ketoCtx.Cleanup() // Kratos requires mailslurper SMTP server:  mailslurperCtx := mailslurper.SetupTestMailslurper(t) // Start Kratos:  kratosSelfService := kratos.DefaultKratosTestSelfService(t) defer kratosSelfService.Close() kratosCtx := kratos.SetupTestKratos(t, postgresCtx, mailslurperCtx, kratosSelfService) defer kratosCtx.Cleanup() // The contexts provide the tests with clients, for example:  _, err := ketoCtx.WriteClient().Write.CreateRelationTuple(ketoWrite. NewCreateRelationTupleParams(). WithPayload(\u0026etoModels.InternalRelationTuple{ Namespace: common.StringP(\"default-namespace\"), Object: common.StringP(\"company-a\"), Relation: common.StringP(\"employs\"), Subject: (*ketoModels.Subject)(common.StringP(\"director\")), })) assert.Nil(t, err) }   The code above starts Hydra, Keto and Kratos. It ensures the migrations are executed, the configuration files are written, the volumes are mounted and services listen on random ports making it easy to run multiple tests in parallel, if needed. With this library, I was able to very quickly test different Ory configuration.\npotential downsides? Testing using external CI/CD might get complicated because third-party CI/CD platforms very often do not allow controlling of the Docker daemon by the unit under test but integration tests have often different testing pipelines anyway.\nWriting tests using containers requires testing code hygiene. To cover potential dependency change, not too much of the test setup logic should live outside of the library. To what extent this is a downside, one has to answer themself.\nclosing words With Docker and dockertest, I was able to achieve quicker turn around when testing different configurations and approaches. That’s not only proving to be a great method for maintaining a reliable system but also enables quick exploration. A test can be also a scratch pad.\n","description":"Testing software against real system is the ultimate testing","tags":["testing","ory","docker"],"title":"On software testing with dockertest","uri":"/posts/2021-04-24-on-software-testing-with-dockertest/"},{"content":"Some two months ago, when I started the Firecracker journey, I set myself a goal to run en etcd cluster in Firecracker microVMs. Many lines of code later, after tackling the problem the hard way, there’s an outcome.\nOkay, it’s not etcd but rather HashiCorp Consul.\nHere’s how a 3 node Consul cluster is launched with firebuild:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  # Start Consul 1 with IP 192.168.127.10: sudo firebuild run \\  --profile=standard \\  --from=combust-labs/consul:1.9.4 \\  --cni-network-name=machines \\  --vmlinux-id=vmlinux-v5.8 \\  --ip-address=192.168.127.10 \\  --name=consul1 \\  --daemonize \\  -- agent -server -client 0.0.0.0 -bootstrap-expect 3 -data-dir /consul/data -retry-join 192.168.127.11 -retry-join 192.168.127.12 -node consul1 # Start Consul 2 with IP 192.168.127.11: sudo firebuild run \\  --profile=standard \\  --from=combust-labs/consul:1.9.4 \\  --cni-network-name=machines \\  --vmlinux-id=vmlinux-v5.8 \\  --ip-address=192.168.127.11 \\  --name=consul2 \\  --daemonize \\  -- agent -server -client 0.0.0.0 -bootstrap-expect 3 -data-dir /consul/data -retry-join 192.168.127.10 -retry-join 192.168.127.12 -node consul2 # Start Consul 3 with IP 192.168.127.10: sudo firebuild run \\  --profile=standard \\  --from=combust-labs/consul:1.9.4 \\  --cni-network-name=machines \\  --vmlinux-id=vmlinux-v5.8 \\  --ip-address=192.168.127.12 \\  --name=consul3 \\  --daemonize \\  -- agent -server -client 0.0.0.0 -bootstrap-expect 3 -data-dir /consul/data -retry-join 192.168.127.10 -retry-join 192.168.127.11 -node consul3   After a couple of seconds required to boot the VMs, start the services and reconcile the cluster, the cluster can be queries for status:\n1  $ curl http://192.168.127.12:8500/v1/status/leader   The output will be similar to:\n1  \"192.168.127.10:8300\"   What about the DNS? Consul nodes were launched with with -node flags. The cluster uses the default 8600 DNS port and default data center name of dc1.\nHence, we can query for a specific node:\n1  $ dig @192.168.127.10 -p 8600 consul1.node.dc1.consul   The response looks similar to:\n; \u003c\u003c\u003e\u003e DiG 9.11.3-1ubuntu1.14-Ubuntu \u003c\u003c\u003e\u003e @192.168.127.10 -p 8600 consul1.node.dc1.consul ; (1 server found) ;; global options: +cmd ;; Got answer: ;; -\u003e\u003eHEADER\u003c\u003c- opcode: QUERY, status: NOERROR, id: 13511 ;; flags: qr aa rd; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 2 ;; WARNING: recursion requested but not available ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 4096 ;; QUESTION SECTION: ;consul1.node.dc1.consul.\tIN\tA ;; ANSWER SECTION: consul1.node.dc1.consul. 0\tIN\tA\t192.168.127.10 ;; ADDITIONAL SECTION: consul1.node.dc1.consul. 0\tIN\tTXT\t\"consul-network-segment=\" ;; Query time: 1 msec ;; SERVER: 192.168.127.10#8600(192.168.127.10) ;; WHEN: Wed Apr 14 21:31:00 UTC 2021 ;; MSG SIZE rcvd: 104 in other news documentation is online There is a new firebuild documentation website online. It’s not complete but it’s a step forward from maintaining a huge readme file in a GitHub repository. The documentation is available here1.\nDocker image based builds Apart from that, there was a bit more work down in the trenches. From the very beginning, it was possible to create a root file system from a Dockerfile. But sometimes a Dockerfile is not sufficient, especially in case of a multi-stage build where the actual build happens outside of a Dockerfile.\nAn example of this is the Jaeger tracing Dockerfile with binary artifacts built in a separate make step.\nIt is now possible to build a root file system directly from a Docker image. Here’s an example of building Jaeger 1.22:\n1 2 3 4 5 6 7 8  sudo firebuild rootfs \\  --profile=standard \\  --docker-image=jaegertracing/all-in-one:1.22 \\  --docker-image-base=alpine:3.13 \\  --cni-network-name=machine-builds \\  --vmlinux-id=vmlinux-v5.8 \\  --mem=512 \\  --tag=combust-labs/jaeger-all-in-one:1.22   Right now, more details only in the huge readme2. Documentation will be updated in the following weeks.\nvminit rootfs bootstrap now with MMDS The next big update is the vminit rootfs MMDS based bootstrap. Until now, the rootfs was build via an SSH connection. This has been replaced with MMDS based bootstrap and the SSH code has been removed from firebuild. More about that in this article3.\nvms can be named As seen in the Consul cluster example at the top, the VMs can be named so it is much easier to refer to them later. No need to hunt the random VM ID anymore. Just start the VM with --name=unique1 and this is possible:\n1 2 3  VMIP=$(sudo firebuild inspect \\  --profile=standard \\  --vmm-id=unique1 | jq '.NetworkInterfaces[0].StaticConfiguration.IPConfiguration.IP' -r)   By the way, the metadata JSON uses camel case for easier integration with tools like jq.\nThere are caveats related to names. The name can be maximum 20 characters long and only alphanumeric characters are allowed.\ntest coverage There have been many tests written and the coverage, especially for builds and resource discovery, has been increased significantly.\nwhat’s coming The next big research areas over the coming weeks, in no particular order:\n expose guest ports via command line flags on the host through iptables integration run command to have support for adding files to the VM on boot additional volumes for long lived data persistence service discovery integration; first step is to integrate with Consul service catalog and DNS - this will open the door for launching more complex infrastructures with firebuild  That’s it for today, thank you for reading!\n  firebuild documentation ↩︎\n firebuild readme: build directly from a Docker image ↩︎\n firebuild rootfs - gRPC with mTLS ↩︎\n   ","description":"The goal has been reached, I have a Consul cluster running","tags":["firecracker","microvm","firebuild","consul"],"title":"Launching Consul cluster with firebuild and other news","uri":"/posts/2021-04-14-launching-consul-cluster-with-firebuild/"},{"content":"Permissions management is an interesting topic. Modern applications are often complex beasts. It doesn’t take much time to hit the point where certain functionality must be allowed only to the selected users or access to a resource should be granted only under certain conditions.\nBuilding a flexible permissions management system is not easy. They tend to be tightly coupled with the business logic and executed every time a decision whether to grant or deny access is required. There are as many requirements as software systems out there. A permissions management systems must also perform. If decisions are to be made often, the latency to make a decision must be minimal.\nORY Keto is one of the ORY platform components and a few days ago it has seen a major upgrade. Versions 0.6.0-alpha.1 is a complete reimplementation of Keto and is marketed as the first open source implementation of Zanzibar: Google’s Consistent, Global Authorization System1.\nFrom the Zanzibar abstract:\n Determining whether online users are authorized to access digital objects is central to preserving privacy. This paper presents the design, implementation, and deployment of Zanzibar, a global system for storing and evaluating access control lists. Zanzibar provides a uniform data model and configuration language for expressing a wide range of access control policies from hundreds of client services at Google, including Calendar, Cloud, Drive, Maps, Photos, and YouTube. Its authorization decisions respect causal ordering of user actions and thus provide external consistency amid changes to access control lists and object contents. Zanzibar scales to trillions of access control lists and millions of authorization requests per second to support services used by billions of people. It has maintained 95th-percentile latency of less than 10 milliseconds and availability of greater than 99.999% over 3 years of production use.\n I have been using previous Keto versions for some R\u0026D work. The new version has sparked my interest because the firebuild system I am working on is going to be in need of a permissions system at some point in time.\nTo better understand the new Keto, I wanted a simple, easy to follow scenario so a judgement can be formed quickly.\nIn one short sentence: it was very easy and it’s impressive!!\nthe scenario The scenario I was evaluating:\n a company employs a director and IT staff the director contracts a consultant the IT staff subscribes to external services  Find out what the company pays for directly and indirectly.\nHere’s the rough diagram:\nORY Keto in Docker Compose I have previously written about my reference ORY Docker Compose2 and that is what I’m using further. To start the local installation, assuming the containers are already built, as explained in the repository readme3:\n1 2 3  git clone https://github.com/radekg/ory-reference-compose.git cd ory-reference-compose/compose docker-compose -f compose.yml up   The new Keto version exposes two API servers:\n the write API: default port is 4467 the read API: default port is 4466  Before we can start querying Keto for a decision, we have to create a few relation tuples4.\nKeto does not try understanding our data, it infers the decision by inspecting and traversing tuples it knows.\nRelation tuples can be created using cURL via the write API. The Compose setup publishes both Keto APIs so we can start like this:\n1 2 3 4 5 6  curl -XPUT --data '{ \"namespace\": \"default-namespace\", \"object\": \"company-a\", \"relation\": \"employs\", \"subject\": \"director\" }' http://localhost:4467/relation-tuples   1 2 3 4 5 6  curl -XPUT --data '{ \"namespace\": \"default-namespace\", \"object\": \"company-a\", \"relation\": \"employs\", \"subject\": \"it-staff\" }' http://localhost:4467/relation-tuples   The associations above imply that the company pays for the director and the IT staff. We could model it like this:\n1 2 3 4 5 6  curl -XPUT --data '{ \"namespace\": \"default-namespace\", \"object\": \"company-a\", \"relation\": \"pays\", \"subject\": \"default-namespace:company-a#employs\" }' http://localhost:4467/relation-tuples   That subject means: the company pays for anybody it employs. Let’s see this in action by executing this request against the read API:\n1  curl --silent 'http://localhost:4466/expand?namespace=default-namespace\u0026object=company-a\u0026relation=pays\u0026max-depth=10' | jq '.'   The output is:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  { \"type\": \"union\", \"subject\": \"default-namespace:company-a#pays\", \"children\": [ { \"type\": \"union\", \"subject\": \"default-namespace:company-a#employs\", \"children\": [ { \"type\": \"leaf\", \"subject\": \"director\" }, { \"type\": \"leaf\", \"subject\": \"it-staff\" } ] } ] }   Now, the IT staff subscribes to AWS, Dropbox and GCP. These relations could be modelled like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  curl -XPUT --data '{ \"namespace\": \"default-namespace\", \"object\": \"it-staff\", \"relation\": \"subscribes\", \"subject\": \"aws\" }' http://localhost:4467/relation-tuples curl -XPUT --data '{ \"namespace\": \"default-namespace\", \"object\": \"it-staff\", \"relation\": \"subscribes\", \"subject\": \"dropbox\" }' http://localhost:4467/relation-tuples curl -XPUT --data '{ \"namespace\": \"default-namespace\", \"object\": \"it-staff\", \"relation\": \"subscribes\", \"subject\": \"gcp\" }' http://localhost:4467/relation-tuples   As these will bear the cost to the company, the JSON report should list them and can be done with this new relation tuple:\n1 2 3 4 5 6  curl -XPUT --data '{ \"namespace\": \"default-namespace\", \"object\": \"company-a\", \"relation\": \"pays\", \"subject\": \"default-namespace:it-staff#subscribes\" }' http://localhost:4467/relation-tuples   That means: the company pays for everything the IT staff subscribes to. If we execute the /expand call again, the output will be:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38  { \"type\": \"union\", \"subject\": \"default-namespace:company-a#pays\", \"children\": [ { \"type\": \"union\", \"subject\": \"default-namespace:company-a#employs\", \"children\": [ { \"type\": \"leaf\", \"subject\": \"director\" }, { \"type\": \"leaf\", \"subject\": \"it-staff\" } ] }, { \"type\": \"union\", \"subject\": \"default-namespace:it-staff#subscribes\", \"children\": [ { \"type\": \"leaf\", \"subject\": \"aws\" }, { \"type\": \"leaf\", \"subject\": \"dropbox\" }, { \"type\": \"leaf\", \"subject\": \"gcp\" } ] } ] }   Pretty cool, regardless of how many services the IT staff would subscribe to in the future, they will get listed in the output!\nKnowing about the cost of a consultant contracted by the director could be modelled in the following way:\n1 2 3 4 5 6  curl -XPUT --data '{ \"namespace\": \"default-namespace\", \"object\": \"director\", \"relation\": \"contracts\", \"subject\": \"consultant\" }' http://localhost:4467/relation-tuples   1 2 3 4 5 6  curl -XPUT --data '{ \"namespace\": \"default-namespace\", \"object\": \"company-a\", \"relation\": \"pays\", \"subject\": \"default-namespace:director#contracts\" }' http://localhost:4467/relation-tuples   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48  { \"type\": \"union\", \"subject\": \"default-namespace:company-a#pays\", \"children\": [ { \"type\": \"union\", \"subject\": \"default-namespace:company-a#employs\", \"children\": [ { \"type\": \"leaf\", \"subject\": \"director\" }, { \"type\": \"leaf\", \"subject\": \"it-staff\" } ] }, { \"type\": \"union\", \"subject\": \"default-namespace:director#contracts\", \"children\": [ { \"type\": \"leaf\", \"subject\": \"consultant\" } ] }, { \"type\": \"union\", \"subject\": \"default-namespace:it-staff#subscribes\", \"children\": [ { \"type\": \"leaf\", \"subject\": \"aws\" }, { \"type\": \"leaf\", \"subject\": \"dropbox\" }, { \"type\": \"leaf\", \"subject\": \"gcp\" } ] } ] }   And if the director decides to contract a solicitor?\n1 2 3 4 5 6  curl -XPUT --data '{ \"namespace\": \"default-namespace\", \"object\": \"director\", \"relation\": \"contracts\", \"subject\": \"solicitor\" }' http://localhost:4467/relation-tuples   the existing relation already covers that:\n1  curl --silent 'http://localhost:4466/expand?namespace=default-namespace\u0026object=company-a\u0026relation=pays\u0026max-depth=10' | jq '.'   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  ... { \"type\": \"union\", \"subject\": \"default-namespace:director#contracts\", \"children\": [ { \"type\": \"leaf\", \"subject\": \"consultant\" }, { \"type\": \"leaf\", \"subject\": \"solicitor\" } ] }, ...   Very neat. At some point, we might want to have an answer to the following direct question:\n does the company pay for the consultant?\n 1 2 3 4 5 6  curl -XPOST --data '{ \"namespace\": \"default-namespace\", \"object\": \"company-a\", \"relation\": \"pays\", \"subject\": \"consultant\" }' http://localhost:4466/check   To which the answer is:\n1 2 3  { \"allowed\": true }   If we ever had to understand what relation causes the company to pay for the consultant, we would use the expand call with sufficient depth.\nnotes on the examples The examples use literal object identifiers for readability purposes. Keto documentation suggests using UUIDv4 or SHA-1 hashes instead. The concept is better explained on this page5.\nconclusion This very simple example shows in just few steps how awesome the new version of Keto is.\nThe API is very simple to use and understand. The flexibility and applicability is endless.\nThis is something what will greatly benefit firebuild and I am sure we will see Keto being used all over the place.\nThe ORY team deserves huge applause for making this technology so easily approachable.\n  Zanzibar: Google’s Consistent, Global Authorization System ↩︎\n ORY reference Docker Compose and thoughts on the platform ↩︎\n Reference ORY Docker Compose setup readme ↩︎\n ORY Keto concepts: relation tuples ↩︎\n ORY Keto concepts: objects ↩︎\n   ","description":"First look at ORY Keto","tags":["ory","keto","zanzibar"],"title":"Looking at Zanzibar through ORY Keto","uri":"/posts/2021-04-11-looking-at-zanzibar-through-ory-keto/"},{"content":"obligatory Keycloak mention Keycloak is awesome because it provides almost everything an organization of almost any size might ever need when it comes to topics like OpenID, SSO, federation or authorization services.\nBut Keycloak is a monolith, it doesn’t do things the cloud native way1. There’s quite a few running modes one needs to fully understand how to operate: standalone, standalone clustered, domain clustered mode, cross data center replication. All very powerful but to many, pretty unapproachable. XML files, JBoss, WildFly, Java - heavyweight technologies associated with slow startup times and difficulty in scaling.\nKeycloak doesn’t provide OpenAPI specification. There are many client libraries but a lot of them are incomplete and do not cover every aspect.\nIntegration can be frustrating, especially when using more obscure Keycloak features. Upgrades can be a pain.\n It’s not surprising that alternatives appear left and right.\n ory One of them is ORY2. Unlike Keycloak, ORY is a bunch of building blocks. In contrast to Keycloak, ORY is written in golang and the pieces are relatively small footprint, zero dependency binaries.\nThey are designed to run in containers and start very fast. This makes them a perfect fit for cloud native applications. There are four major components, each providing and OpenAPI 2.0 specification allowing for relatively easy post deployment operations:\n Hydra: OpenID Certified OAuth 2.0 Server and OpenID Connect Provider Keto: is the open source implementation of Zanzibar: Google’s Consistent, Global Authorization System3; basically an access management system Kratos: Identity and User Management System Oathkeeper: an Identity \u0026 Access Proxy (IAP) and Access Control Decision API  ORY is not a one to one alternative to Keycloak. A number of examples:\n Kratos does not provide any account searching functionality there is no way to attach user account metadata in Kratos there are no groups and roles in Kratos  in theory this can be built out on Keto but there is no standard method   Keto does not provide user managed access Oathkeeper is okay for simple authentication but complex authentication flows get hairy pretty fast  a lot of JSON rule files to manage multi-tenant deployment with ad-hoc OpenID clients would be much better served using Traefik with ForwardAuth plugins Oathkeeper appears to have some aspirations to be a reverse proxy too but it’s not exactly Traefik   gluing Kratos, Hydra and Keto together is a lot of one-off work there are no plugins in ORY  adding 2FA is a custom job creating custom authentication flows is a one-off job using Keycloak mappers? there’s is nothing like this in ORY custom claims? only by using Hydra administrative Login Accept API   no SAML, no LDAP support scalability through the database  The individual components are pretty easy to get going. Using all four is actually a lot of work. Integration will require writing a lot of code.\nLike with Keycloak, starting with ORY is overwhelming. Individual components are documented rather okay but diving deep in Dockerfiles and examples on GitHub will be necessary.\nthe Docker Compose ORY reference setup I’ve been maintaining my own reference setup for more than a year.\nIt is available on GitHub today4. The reference integrates all four components and provides a couple of simple scenarios visualizing the interactions. It starts with building Docker images from sources and goes through the configuration of individual components, sometimes on purpose the hard way. The reference uses the most recent components at the time of writing:\n Hydra v1.10.1 Keto v0.6.0-alpha.1 Kratos v0.5.5-alpha.1 Oathkeeper v0.38.9-beta.1; v0.38.10-beta.1 does not build from sources  The reference is kept up to date on a best effort basis.\n  Cloud native computing ↩︎\n ORY ↩︎\n Zanzibar: Google’s Consistent, Global Authorization System ↩︎\n Reference ORY Docker Compose setup on GitHub ↩︎\n   ","description":"Thought on the ORY platform and a reference Docker compose","tags":["sso","iam","ory","hydra","keto","kratos","oathkeeper","keycloak"],"title":"ORY reference Docker Compose and thoughts on the platform","uri":"/posts/2021-04-10-ory-reference-docker-compose-and-thoughts-on-the-platform/"},{"content":"Updated on 10th of April 2021: The decision to move Apache Mesos to Attic, has been reversed.\nAn end of an era. What’s the better way to summarize that the maintainers of Apache Mesos are now voting on moving the project to Apache Attic. Attic is a place where Apache projects go when they reach end of life.\nIn other words: it’s end of the road for Apache Mesos.\nI was first introduced to Mesos in 2015, back at Technicolor Virdata. A colleague, Nathan, has been going on about for a long time. I was tasked to help him move our Virdata platform to Mesos. It was the first time I was exposed to distributed scheduling. If I recall correctly, it took us about a couple of weeks to get our heads around it. We had to work out the installation and configuration process, figure out how to deploy Marathon. DC/OS did not exist yet.\nTook us another two weeks to have the platform migrated. I have fond memories of nights spent trying to run Apache Spark on Mesos with Docker bridge networking…\nIt was great, such a big improvement over thousands of lines of Chef which we could start deprecating.\nSo yeah, end of an era. Mesos was great but I do feel it was a bit mismanaged. Not by the community though, the community put an awesome effort into maintaining it for such a long time.\nIt’s now between Kubernetes and HashiCorp Nomad to figure it out. What’s positive, they’re solving the same problem but serve different audiences.\nGreat to have a choice.\n","description":"An end of an era","tags":["mesos","kubernetes","k8s","nomad"],"title":"Apache Mesos reaches end of life","uri":"/posts/2021-04-06-apache-mesos-reaches-end-of-life/"},{"content":"If you are using golang, there’s a pretty high chance you have used the os.Expand(s string, mapping func(string) string) function in your code already. Or maybe it’s derivative, os.ExpandEnv(s string).\nThe former takes an input string and expands the shell variable-like occurrences with actual shell variable values. For example:\n1 2 3  os.Setenv(\"VARIABLE\", \"hello\") fmt.Println(os.ExpandEnv(\"${VARIABLE}, world!\")) // prints \"hello, world!\"   It uses os.Lookup(s string) as the mapping argument to os.Expand.\nPretty often, that may be what is needed and os.ExpandEnv is one of little gems of the golang standard library.\nThe problem with os.ExpandEnv is, if the variable referenced in the string does not exist, it’s replaced with an empty string.\nHowever, consider the following command:\n1 2 3 4 5 6 7 8 9  const command = `cd /tmp/build \u0026\u0026 \\ apkArch=\"$(apk --print-arch)\" \u0026\u0026 \\ case \"${apkArch}\" in \\ aarch64) consulArch='arm64' ;; \\ armhf) consulArch='armhfv6' ;; \\ x86) consulArch='386' ;; \\ x86_64) consulArch='amd64' ;; \\ *) echo \u003e\u00262 \"error: unsupported architecture: ${apkArch} (see ${HASHICORP_RELEASES}/consul/${CONSUL_VERSION}/)\" \u0026\u0026 exit 1 ;; \\ esac`   Assuming that the values of HASHICORP_RELEASES and CONSUL_VERSION are passed as environment variables:\n1 2 3  os.Setenv(\"HASHICORP_RELEASES\", \"https://releases.hashicorp.com\") os.Setenv(\"CONSUL_VERSION\", \"1.9.4\") fmt.Println(os.ExpandEnv(command))   would give the following output:\ncd /tmp/build \u0026\u0026 \\ apkArch=\"$(apk --print-arch)\" \u0026\u0026 \\ case \"\" in \\ aarch64) consulArch='arm64' ;; \\ armhf) consulArch='armhfv6' ;; \\ x86) consulArch='386' ;; \\ x86_64) consulArch='amd64' ;; \\ *) echo \u003e\u00262 \"error: unsupported architecture: (see https://releases.hashicorp.com/consul/1.9.4/)\" \u0026\u0026 exit 1 ;; \\ esac The ${apkArch} part was obliterated from the output. This command would never work.\nFortunately, os.Expand comes to the rescue!\n1 2 3 4 5 6 7 8  lookupFunc := func(placeholderName string) string { if value, ok := os.Lookup(placeholderName); ok { return value } // fallback:  return fmt.Sprintf(\"$%s\", placeholderName) } fmt.Println(os.Expand(command, lookupFunc))   Aha, now it looks better:\ncd /tmp/build \u0026\u0026 \\ apkArch=\"$(apk --print-arch)\" \u0026\u0026 \\ case \"$apkArch\" in \\ aarch64) consulArch='arm64' ;; \\ armhf) consulArch='armhfv6' ;; \\ x86) consulArch='386' ;; \\ x86_64) consulArch='amd64' ;; \\ *) echo \u003e\u00262 \"error: unsupported architecture: $apkArch (see https://releases.hashicorp.com/consul/1.9.4/)\" \u0026\u0026 exit 1 ;; \\ esac This output would definitely work. But shell strings can be much more complicated than this.\nThe problem with the lookupFunc is that one has to make an upfront decision to surround the fallback with {}.\nAnd there are cases when neither is the right choice.\nConsider the following input, a slightly modified real example coming from the official Postgres 13 Dockerfile:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  const command = `RUN set -eux; \\ export GNUPGHOME=${GNUPGHOME:=$(mktemp -d)}; \\ savedAptMark=\"$(apt-mark showmanual)\"; \\ apt-get update; \\ apt-get install -y --no-install-recommends ca-certificates wget; \\ rm -rf /var/lib/apt/lists/*; \\ dpkgArch=\"$(dpkg --print-architecture | awk -F- '{ print $NF }')\"; \\ wget -O /usr/local/bin/gosu \"https://github.com/tianon/gosu/releases/download/$GOSU_VERSION/gosu-$dpkgArch\"; \\ wget -O /usr/local/bin/gosu.asc \"https://github.com/tianon/gosu/releases/download/$GOSU_VERSION/gosu-$dpkgArch.asc\"; \\ gpg --batch --keyserver hkps://keys.openpgp.org --recv-keys B42F6819007F00F88E364FD4036A9C25BF357DD4; \\ gpg --batch --verify /usr/local/bin/gosu.asc /usr/local/bin/gosu; \\ gpgconf --kill all; \\ rm -rf \"$GNUPGHOME\" /usr/local/bin/gosu.asc; \\ apt-mark auto '.*' \u003e /dev/null; \\ [ -z \"$savedAptMark\" ] || apt-mark manual $savedAptMark \u003e /dev/null; \\ apt-get purge -y --auto-remove -o APT::AutoRemove::RecommendsImportant=false; \\ chmod +x /usr/local/bin/gosu; \\ gosu --version; \\ gosu nobody true `   There are two conflicting cases in this input: export GNUPGHOME=${GNUPGHOME:=$(mktemp -d)}; and | awk -F- '{ print $NF }'.\nIn case of the export command, the surrounding {} must be preserved. The lookupFunc could return the fallback of fmt.Sprintf(\"${%s}\", placeholderName).\nBut in case of | awk -F- '{ print $NF }', surrounding $NF with {} results in an error. The mapper argument of os.Expand fails to tell what the raw input was. Can this be fixed?\nThe answer is to look at the source of os.Expand standard library function. It looks like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  func Expand(s string, mapping func(string) string) string { var buf []byte // ${} is all ASCII, so bytes are fine for this operation. \ti := 0 for j := 0; j \u003c len(s); j++ { if s[j] == '$' \u0026\u0026 j+1 \u003c len(s) { if buf == nil { buf = make([]byte, 0, 2*len(s)) } buf = append(buf, s[i:j]...) name, w := getShellName(s[j+1:]) if name == \"\" \u0026\u0026 w \u003e 0 { // Encountered invalid syntax; eat the \t// characters. \t} else if name == \"\" { // Valid syntax, but $ was not followed by a \t// name. Leave the dollar character untouched. \tbuf = append(buf, s[j]) } else { buf = append(buf, mapping(name)...) } j += w i = j + 1 } } if buf == nil { return s } return string(buf) + s[i:] }   The case we are interested in is the final else. It says:\n if it was a valid shell variable name, replace the value with the value from the mapper\n If we replaced this code with:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41  func Expand(s string, mapping func(string) (string, bool)) string { var buf []byte // ${} is all ASCII, so bytes are fine for this operation. \ti := 0 for j := 0; j \u003c len(s); j++ { if s[j] == '$' \u0026\u0026 j+1 \u003c len(s) { if buf == nil { buf = make([]byte, 0, 2*len(s)) } buf = append(buf, s[i:j]...) shellNameInput := s[j+1:] name, w := getShellName(shellNameInput) if name == \"\" \u0026\u0026 w \u003e 0 { // Encountered invalid syntax; eat the \t// characters. \t} else if name == \"\" { // Valid syntax, but $ was not followed by a \t// name. Leave the dollar character untouched. \tbuf = append(buf, s[j]) } else { replacement, ok := mapping(name) if ok { buf = append(buf, replacement...) } else { // preserve enclosing {} \tif shellNameInput[0] == '{' { buf = append(buf, fmt.Sprintf(\"${%s}\", name)...) } else { buf = append(buf, fmt.Sprintf(\"$%s\", name)...) } } } j += w i = j + 1 } } if buf == nil { return s } return string(buf) + s[i:] }   We would get the fully correct behavior. We have added:\n1 2 3  buf = append(buf, s[i:j]...) shellNameInput := s[j+1:] // \u003c----- this line \tname, w := getShellName(shellNameInput)   and changed the final else statement to:\n1 2 3 4 5 6 7 8 9 10 11  replacement, ok := mapping(name) if ok { buf = append(buf, replacement...) } else { // preserve enclosing {} \tif shellNameInput[0] == '{' { buf = append(buf, fmt.Sprintf(\"${%s}\", name)...) } else { buf = append(buf, fmt.Sprintf(\"$%s\", name)...) } }   This bit reads as follows:\n if the mapper found the value, use it; otherwise fall back to the original value but preserve surrounding braces\n The result of the custom Expand:\n1 2  os.Setenv(\"GOSU_VERSION\", \"1.12\") fmt.Println(Expand(command, os.Lookup))   is correct:\nRUN set -eux; \\ export GNUPGHOME=${GNUPGHOME:=$(mktemp -d)}; \\ savedAptMark=\"$(apt-mark showmanual)\"; \\ apt-get update; \\ apt-get install -y --no-install-recommends ca-certificates wget; \\ rm -rf /var/lib/apt/lists/*; \\ dpkgArch=\"$(dpkg --print-architecture | awk -F- '{ print $NF }')\"; \\ wget -O /usr/local/bin/gosu \"https://github.com/tianon/gosu/releases/download/1.12/gosu-$dpkgArch\"; \\ wget -O /usr/local/bin/gosu.asc \"https://github.com/tianon/gosu/releases/download/1.12/gosu-$dpkgArch.asc\"; \\ gpg --batch --keyserver hkps://keys.openpgp.org --recv-keys B42F6819007F00F88E364FD4036A9C25BF357DD4; \\ gpg --batch --verify /usr/local/bin/gosu.asc /usr/local/bin/gosu; \\ gpgconf --kill all; \\ rm -rf \"$GNUPGHOME\" /usr/local/bin/gosu.asc; \\ apt-mark auto '.*' \u003e /dev/null; \\ [ -z \"$savedAptMark\" ] || apt-mark manual $savedAptMark \u003e /dev/null; \\ apt-get purge -y --auto-remove -o APT::AutoRemove::RecommendsImportant=false; \\ chmod +x /usr/local/bin/gosu; \\ gosu --version; \\ gosu nobody true A full implementation is here.\n","description":"They are awesome but could be better","tags":["golang"],"title":"On golang os.Expand and os.ExpandEnv","uri":"/posts/2021-04-06-on-golang-osexpand-and-osexpandenv/"},{"content":"Well, sort of. But bear with me.\nbackground A couple of days ago, Confluent announced a ZooKeeper free Kafka 2.8 RC0 available for testing. A fantastic effort, great achievement by all the contributors who made it happen.\nIn the typical Hacker News fashion, a post about Kafka always triggers an inevitable “Puslar vs Kafka” discussion. These always remind me of one of my main gripes related to Kafka: no infinite retention. I’ve written about it close to five years ago1.\nSo, apparently there is a KIP for open source Kafka tiered storage2 which would enable this kind of behavior and take it even further. There is definitely a paid feature on the Confluent platform enabling tiered storage3.\nHowever, as a user of the open source Kafka, I can’t use it.\nafter 5 years of waiting I kinda hacked it myself in. Here’s how I’ve done it.\nI have forked Kafka from https://github.com/apache/kafka and checked out the 2.8 branch, the one with no ZooKeeper. To be exact, the 08849bc3909d4fabda965c8ca7f78b0feb5473d2 commit.\nI then applied this diff:\n build Kafka from sources 1  ./gradlewAll releaseTarGz   Now, I can start my new Kafka like this:\n1 2 3 4 5 6  AWS_SHARED_CREDENTIALS_FILE=~/.aws/credentials \\  AWS_PROFILE=a-profile-with-bucket-access \\  KAFKA_AWS_S3_UPLOADS=1 \\  KAFKA_AWS_S3_REGION=eu-central-1 \\  KAFKA_AWS_S3_BUCKET=my-kafka-logs-bucket \\  ./kafka_2.13-2.8.0-SNAPSHOT/bin/kafka-server-start.sh ./kafka_2.13-2.8.0-SNAPSHOT/config/kraft/server.properties   Now, every time Kafka is about to delete a log segment, it will put it in S3 first. Only the log files are stored because there is no need to have an index. Whenever I need data from an older segment, I can download the segment from S3, rebuild the index and read out all data from the segment.\nI can process dozens of segments in parallel, regardless of the total partition size.\nquick and dirty test method Create a topic with a rather small segment:\n~/dev/kafka-2.8.0/kafka_2.13-2.8.0-SNAPSHOT/bin/kafka-topics.sh \\ --create \\ --topic test-topic \\ --partitions 1 \\ --replication-factor 1 \\ --bootstrap-server localhost:9092 \\ --config segment.bytes=524288 Write some data to the topic with a tool of choice. I can see new segments rolling in:\n[2021-04-02 23:32:57,551] INFO [BrokerLifecycleManager id=1] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager) [2021-04-02 23:32:57,552] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(test-topic-0) (kafka.server.ReplicaFetcherManager) [2021-04-02 23:36:38,025] INFO [ProducerStateManager partition=test-topic-0] Writing producer snapshot at offset 3830 (kafka.log.ProducerStateManager) [2021-04-02 23:36:38,028] INFO [Log partition=test-topic-0, dir=/tmp/kraft-combined-logs] Rolled new log segment at offset 3830 in 5 ms. (kafka.log.Log) [2021-04-02 23:36:44,082] INFO [ProducerStateManager partition=test-topic-0] Writing producer snapshot at offset 7646 (kafka.log.ProducerStateManager) [2021-04-02 23:36:44,083] INFO [Log partition=test-topic-0, dir=/tmp/kraft-combined-logs] Rolled new log segment at offset 7646 in 3 ms. (kafka.log.Log) Finally, delete the topic from Kafka:\n[2021-04-02 23:37:52,732] INFO [Controller 1] Removed topic test-topic with ID H6YadkN7SUGXheMu6MJ1uA. (org.apache.kafka.controller.ReplicationControlManager) [2021-04-02 23:37:52,759] INFO [BrokerMetadataListener id=1] Processing deletion of topic test-topic with id H6YadkN7SUGXheMu6MJ1uA (kafka.server.metadata.BrokerMetadataListener) [2021-04-02 23:37:52,761] INFO [GroupCoordinator 1]: Removed 0 offsets associated with deleted partitions: test-topic-0. (kafka.coordinator.group.GroupCoordinator) [2021-04-02 23:37:52,768] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(test-topic-0) (kafka.server.ReplicaFetcherManager) [2021-04-02 23:37:52,768] INFO [ReplicaAlterLogDirsManager on broker 1] Removed fetcher for partitions Set(test-topic-0) (kafka.server.ReplicaAlterLogDirsManager) [2021-04-02 23:37:52,776] INFO Log for partition test-topic-0 is renamed to /tmp/kraft-combined-logs/test-topic-0.fe590aa674124d0c81bb5b3d718a73ab-delete and is scheduled for deletion (kafka.log.LogManager) After about 60 seconds, the segments are uploaded to S3 and deleted from disk:\n[2021-04-02 23:38:52,781] INFO [Log partition=test-topic-0, dir=/tmp/kraft-combined-logs] Deleting segments as the log has been deleted: LogSegment(baseOffset=0, size=524285, lastModifiedTime=1617399398000, largestRecordTimestamp=Some(1617399398018)),LogSegment(baseOffset=3830, size=524270, lastModifiedTime=1617399404000, largestRecordTimestamp=Some(1617399404079)),LogSegment(baseOffset=7646, size=48257, lastModifiedTime=1617399404000, largestRecordTimestamp=Some(1617399404638)) (kafka.log.Log) [2021-04-02 23:38:52,786] INFO [Log partition=test-topic-0, dir=/tmp/kraft-combined-logs] Deleting segment files LogSegment(baseOffset=0, size=524285, lastModifiedTime=1617399398000, largestRecordTimestamp=Some(1617399398018)),LogSegment(baseOffset=3830, size=524270, lastModifiedTime=1617399404000, largestRecordTimestamp=Some(1617399404079)),LogSegment(baseOffset=7646, size=48257, lastModifiedTime=1617399404000, largestRecordTimestamp=Some(1617399404638)) (kafka.log.Log) [2021-04-02 23:38:57,798] INFO Uploading segment noan.local/test-topic-0.fe590aa674124d0c81bb5b3d718a73ab-delete/00000000000000000000.log.deleted with size 524285 prior to delete... (kafka.log.LogSegment) [2021-04-02 23:38:59,210] INFO Uploaded segment prior to delete, S3 ETag: 769bf025a6e85a7402509142de3d149a, took 6417ms (kafka.log.LogSegment) [2021-04-02 23:38:59,212] INFO Deleted log /tmp/kraft-combined-logs/test-topic-0.fe590aa674124d0c81bb5b3d718a73ab-delete/00000000000000000000.log.deleted. (kafka.log.LogSegment) [2021-04-02 23:38:59,219] INFO Deleted offset index /tmp/kraft-combined-logs/test-topic-0.fe590aa674124d0c81bb5b3d718a73ab-delete/00000000000000000000.index.deleted. (kafka.log.LogSegment) [2021-04-02 23:38:59,219] INFO Deleted time index /tmp/kraft-combined-logs/test-topic-0.fe590aa674124d0c81bb5b3d718a73ab-delete/00000000000000000000.timeindex.deleted. (kafka.log.LogSegment) [2021-04-02 23:39:04,223] INFO Uploading segment noan.local/test-topic-0.fe590aa674124d0c81bb5b3d718a73ab-delete/00000000000000003830.log.deleted with size 524270 prior to delete... (kafka.log.LogSegment) [2021-04-02 23:39:04,840] INFO Uploaded segment prior to delete, S3 ETag: 06d987e417031ca3474a4fcf6efc72de, took 5620ms (kafka.log.LogSegment) [2021-04-02 23:39:04,842] INFO Deleted log /tmp/kraft-combined-logs/test-topic-0.fe590aa674124d0c81bb5b3d718a73ab-delete/00000000000000003830.log.deleted. (kafka.log.LogSegment) [2021-04-02 23:39:04,842] INFO Deleted offset index /tmp/kraft-combined-logs/test-topic-0.fe590aa674124d0c81bb5b3d718a73ab-delete/00000000000000003830.index.deleted. (kafka.log.LogSegment) [2021-04-02 23:39:04,843] INFO Deleted time index /tmp/kraft-combined-logs/test-topic-0.fe590aa674124d0c81bb5b3d718a73ab-delete/00000000000000003830.timeindex.deleted. (kafka.log.LogSegment) [2021-04-02 23:39:09,848] INFO Uploading segment noan.local/test-topic-0.fe590aa674124d0c81bb5b3d718a73ab-delete/00000000000000007646.log.deleted with size 48257 prior to delete... (kafka.log.LogSegment) [2021-04-02 23:39:10,103] INFO Uploaded segment prior to delete, S3 ETag: 624d8699a9f53ecb24143e197c4f1ccf, took 5259ms (kafka.log.LogSegment) [2021-04-02 23:39:10,104] INFO Deleted log /tmp/kraft-combined-logs/test-topic-0.fe590aa674124d0c81bb5b3d718a73ab-delete/00000000000000007646.log.deleted. (kafka.log.LogSegment) [2021-04-02 23:39:10,104] INFO Deleted offset index /tmp/kraft-combined-logs/test-topic-0.fe590aa674124d0c81bb5b3d718a73ab-delete/00000000000000007646.index.deleted. (kafka.log.LogSegment) [2021-04-02 23:39:10,105] INFO Deleted time index /tmp/kraft-combined-logs/test-topic-0.fe590aa674124d0c81bb5b3d718a73ab-delete/00000000000000007646.timeindex.deleted. (kafka.log.LogSegment) [2021-04-02 23:39:10,111] INFO Deleted log for partition test-topic-0 in /tmp/kraft-combined-logs/test-topic-0.fe590aa674124d0c81bb5b3d718a73ab-delete. (kafka.log.LogManager) The upload would take less time if I was running my Kafka in EC2. This method works for deleted topics, regular segment rolls and also compacted topics. Basically, every time a segment is about to be removed from disk.\n  The case for Kafka cold storage ↩︎\n KIP-405: Kafka Tiered Storage ↩︎\n Confluent Platform infinite Kafka retention ↩︎\n   ","description":"Well, sort of…","tags":["kafka"],"title":"I hacked infinite retention into my open source Kafka","uri":"/posts/2021-04-02-kafka-infinite-retention/"},{"content":"This is a personal blog of Radek Gruchalski. This privacy policy explains how the collected personal data collected is used.\nwhat data is collected? newsletter This website collects email addresses of the users who explicitly subscribe to the newsletter present on article pages.\nThe newsletter is handled with MailChimp, a service provided by:\nThe Rocket Science Group, LLC\n675 Ponce de Leon Ave NE\nSuite 5000\nAtlanta, GA 30308 USA\nThe information you provide is stored only for the purpose of newsletter processing and is never shared or sold to any third party.\nanalytics This website uses Google Analytics but it respects your Do Not Track settings, uses IP address anonymization and localStorage instead of cookies for analytics client ID storage.\nCDN contents The blog uses jsDelivr CDN for JavaScript resources. The jsDelivr privacy policy is available here.\nhow is the data collected? You explicitly provide the data collected during the newsletter sign up process.\nIn order to be signed up, you are required to confirm the choice via link sent to your email address provided during the subscription process.\nThe final email confirming your subscription contains the web address which you can use to unsubscribe from the newsletter at any time.\nhow is the data used? The data is collected only for the purpose of newsletter processing and is not shared or sold to any third party.\nopt-out You can choose to opt out from the newsletter at any time. The information you have provided will be removed from the newsletter processing list within a reasonable time but no longer than 14 days from the moment you have decided to unsubscribe. The data may be preserved for a longer period of time by the MailChimp service.\nTo find out more, please review the Privacy Policy of the MailChimp service, section 3 and relevant.\ncookies This website does not use cookies.\nhow to contact me If you have any questions, you can contact, Radek Gruchalski, directly via:\n email address: radek@gruchalski.com post: Hermesstrasse 39, 52156 Monschau, Germany call: +49 2472 6084960  ","description":"","tags":null,"title":"Privacy","uri":"/privacy/"},{"content":"Long time coming but the KIP-5001 has finally landed. It’s official, Apache Kafka does not require ZooKeeper anymore. The KRaft, the Kafka Raft implementation, is not recommended for production yet. Full announcement from Confluent is here2.\nRegardless, this is a fantastic milestone and a kudos to all the contributors for making this happen as the simplification in the operations will be significant.\ntaking it for a test drive First, generate a new cluster ID:\n1 2  $ ~/dev/kafka-2.8/bin/kafka-storage.sh random-uuid HCsQovjcTs-8xhS1DSU5Gw   Next, format the storage directory. The default directory is /tmp/kraft-combined-logs and the setting can be found under the log.dirs property of the new config/kraft/server.properties file:\n1 2  $ ~/dev/kafka-2.8/bin/kafka-storage.sh format -t HCsQovjcTs-8xhS1DSU5Gw -c ~/dev/kafka-2.8/config/kraft/server.properties Formatting /tmp/kraft-combined-logs   And simply start the broker:\n1  $ ~/dev/kafka-2.8/bin/kafka-server-start.sh ~/dev/kafka-2.8/config/kraft/server.properties   Which produces the following output:\n[2021-03-31 23:25:56,097] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$) [2021-03-31 23:25:56,425] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util) [2021-03-31 23:25:56,664] INFO [Log partition=@metadata-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log) [2021-03-31 23:25:56,727] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper) [2021-03-31 23:25:56,914] INFO [RaftManager nodeId=1] Completed transition to Unattached(epoch=0, voters=[1], electionTimeoutMs=1165) (org.apache.kafka.raft.QuorumState) [2021-03-31 23:25:56,917] INFO [RaftManager nodeId=1] Completed transition to Candidate(localId=1, epoch=1, retries=1, electionTimeoutMs=1680) (org.apache.kafka.raft.QuorumState) [2021-03-31 23:25:56,921] INFO [RaftManager nodeId=1] Completed transition to Leader(localId=1, epoch=1, epochStartOffset=0) (org.apache.kafka.raft.QuorumState) [2021-03-31 23:25:57,004] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler) [2021-03-31 23:25:57,009] INFO [kafka-raft-outbound-request-thread]: Starting (kafka.raft.RaftSendThread) ... [2021-03-31 23:25:57,959] INFO Kafka version: 2.8.0-SNAPSHOT (org.apache.kafka.common.utils.AppInfoParser) [2021-03-31 23:25:57,959] INFO Kafka commitId: 08849bc3909d4fab (org.apache.kafka.common.utils.AppInfoParser) [2021-03-31 23:25:57,959] INFO Kafka startTimeMs: 1617225957958 (org.apache.kafka.common.utils.AppInfoParser) [2021-03-31 23:25:57,959] INFO [Controller 1] The request from broker 1 to unfence has been granted because it has caught up with the last committed metadata offset 1. (org.apache.kafka.controller.BrokerHeartbeatManager) [2021-03-31 23:25:57,960] INFO Kafka Server started (kafka.server.KafkaRaftServer) [2021-03-31 23:25:57,963] INFO [Controller 1] Unfenced broker: UnfenceBrokerRecord(id=1, epoch=0) (org.apache.kafka.controller.ClusterControlManager) [2021-03-31 23:25:57,989] INFO [BrokerLifecycleManager id=1] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager) It’s alive. A topic can be created using the usual tooling:\n1 2 3 4 5  $ ~/dev/kafka-2.8/bin/kafka-topics.sh --create \\  --topic test-topic \\  --partitions 1 \\  --replication-factor 1 \\  --bootstrap-server localhost:9092   [2021-03-31 23:28:21,836] INFO [Controller 1] createTopics result(s): CreatableTopic(name='test-topic', numPartitions=1, replicationFactor=1, assignments=[]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager) [2021-03-31 23:28:21,838] INFO [Controller 1] Created topic test-topic with ID H6YadkN7SUGXheMu6MJ1uA. (org.apache.kafka.controller.ReplicationControlManager) [2021-03-31 23:28:21,838] INFO [Controller 1] Created partition H6YadkN7SUGXheMu6MJ1uA:0 with PartitionControlInfo(replicas=[1], isr=[1], removingReplicas=null, addingReplicas=null, leader=1, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager) [2021-03-31 23:28:21,904] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(test-topic-0) (kafka.server.ReplicaFetcherManager) [2021-03-31 23:28:21,926] INFO [Log partition=test-topic-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log) [2021-03-31 23:28:21,930] INFO Created log for partition test-topic-0 in /tmp/kraft-combined-logs/test-topic-0 with properties {compression.type -\u003e producer, message.downconversion.enable -\u003e true, min.insync.replicas -\u003e 1, segment.jitter.ms -\u003e 0, cleanup.policy -\u003e [delete], flush.ms -\u003e 9223372036854775807, etention.ms -\u003e 604800000, flush.messages -\u003e 9223372036854775807, message.format.version -\u003e 2.8-IV1, file.delete.delay.ms -\u003e 60000, max.compaction.lag.ms -\u003e 9223372036854775807, max.message.bytes -\u003e 1048588, min.compaction.lag.ms -\u003e 0, message.timestamp.type -\u003e CreateTime, preallocate -\u003e false, min.cleanable.dirty.ratio -\u003e 0.5, index.interval.bytes -\u003e 4096, unclean.leader.election.enable -\u003e false, retention.bytes -\u003e -1, delete.retention.ms -\u003e 86400000, segment.ms -\u003e 604800000, message.timestamp.difference.max.ms -\u003e 9223372036854775807, segment.index.bytes -\u003e 10485760}. (kafka.log.LogManager) [2021-03-31 23:28:21,931] INFO [Partition test-topic-0 broker=1] No checkpointed highwatermark is found for partition test-topic-0 (kafka.cluster.Partition) [2021-03-31 23:28:21,932] INFO [Partition test-topic-0 broker=1] Log loaded for partition test-topic-0 with initial high watermark 0 (kafka.cluster.Partition) Impressive.\n  KIP-500  ↩︎\n Apache Kafka Made Simple: A First Glimpse of a Kafka Without ZooKeeper ↩︎\n   ","description":"KIP-500 is implemented and Kafka is now completely standalone","tags":["kafka","zookeeper"],"title":"Kafka 2.8 is out in the wild and does not need ZooKeeper anymore","uri":"/posts/2021-03-31-kafka-2.8-does-not-need-zookeeper-anymore/"},{"content":"the problem Currently, when a rootfs is built, the guest is started with an SSH server and the bootstrap process executes via an SSH connection. I don’t like this and want to replace the SSH method with an MMDS based solution. MMDS is already present in the firebuild run command.\nrun uses the vminit component from firebuild-mmds. When the guest starts, the vminit guest service connects to the MMDS endpoint, downloads the metadata and configures the VM. This is pretty similar to cloud-init but I don’t want cloud-init at this stage. Writing a cloud-init provider in Python is a bit of a head scratcher, can be done but maybe some other time.\nrootfs bootstrap is pretty similar to run in the sense that it also starts a guest VM. There is no reason why it should not work the same way. Bye SSH, welcome MMDS, easy peasy. Not so… The big difference between run and rootfs is:\n rootfs requires access to RUN commands and ADD / COPY resources present in the Docker artifact\n Multiple approaches are possible. Firecracker supports vsock devices. The guest can connect to the host and vice-versa, even without a network interface. Very nice but vsock is pretty low level and since the guest requires at least egress—Dockerfiles are full of package installation and pulling random stuff from the Internet—there was really no point going that way.\nAn alternative is a host service which the guest can connect to and fetch whatever is needed. I originally wanted a HTTP service but since there is a need of bi-directional communication without much protocol overhead, gRPC seems to be a better fit.\nkiss, keep it simply secure I opted for the following:\n firebuild will start a bootstrap only gRPC server, one per rootfs command run firebuild will put the bootstrap endpoint in MMDS the guest will connect via vminit to MMDS and discover the bootstrap endpoint the guest will connect to the gRPC service via vminit bootstrap, download commands and resources and execute the bootstrap  What I wanted was that every connection is always TLS protected, even when the operator would not configure TLS for the bootstrap process. In fact, mutual TLS is preferred so I made a decision to never allow a non-TLS connection or insecure certificates.\n the CA chain and client certificate will be delivered via MMDS metadata  the solution, embedded CA No insecure certificates imply a certificate authority being available. I’ve written about certificate authorities before1. Deploying something like Vault is not really difficult but during testing and development, considering the requirements, adds some friction.\nI don’t like managing development dependency certificate files. I mean, I’ve done it but it’s always a bit messy. It requires extra tools, documentation and there are those pesky extra steps to follow in the readme, or make steps to execute.\nfirebuild is written in Golang which has an awesome first class support for anything TLS/PKI/x509 related. Turns out a mini CA is less than 300 lines of code!\nfirebuild will use an embedded certificate authority. It’s lightweight and does only bare minimum to look like a CA but support a short-lived rootfs build process. If cacert, server cert and server key are not provided, it does the following:\n on start, generate the root CA certificate optionally, when configured, generate an intermediate CA generate a server *tls.Config with a newly generated server certificate generate a client *tls.Config with a newly generated client certificate automatically configures the certificate and the client *tls.Config to fulfill the gRPC server name requirement  Here’s how to use it:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59  package main import ( \"github.com/combust-labs/firebuild-embedded-ca/ca\" \"github.com/hashicorp/go-hclog\" \"google.golang.org/grpc\" \"google.golang.org/grpc/credentials\" ) func main() { logger := hclog.Default() grpcServiceName := \"grpc-service-name\" grpcServerOptions := []grpc.ServerOption{} embeddedCA, embeddedCAErr := ca.NewDefaultEmbeddedCAWithLogger(\u0026ca.EmbeddedCAConfig{ Addresses: []string{grpcServiceName}, KeySize: 4096, }, logger.Named(\"embdedded-ca\")) if embeddedCAErr != nil { panic(embeddedCAErr) } serverTLSConfig, tlsConfigErr := embeddedCA.NewServerCertTLSConfig() if tlsConfigErr != nil { panic(tlsConfigErr) } clientTLSConfig, err := embeddedCA.NewClientCertTLSConfig(grpcServiceName) if err != nil { panic(embeddedCAErr) } grpcServerOptions = append(grpcServerOptions, grpc.Creds(credentials.NewTLS(serverTLSConfig))) listener, listenerErr := net.Listen(\"tcp\", \"127.0.0.1:0\") if listenerErr != nil { panic(listenerErr) } grpcServer = grpc.NewServer(grpcServerOptions...) ///proto.Register...(grpcServer, ...)  chanErr := make(chan struct{}) go func() { if err := s.srv.Serve(listener); err != nil { logger.Error(\"failed grpc serve\", \"reason\", err) close(chanErr) } }() grpcConn, _ := grpc.Dial(listener.Addr().String(), grpc.WithTransportCredentials(credentials.NewTLS(clientTLSConfig))) // ...  }   With key sizes of 2048 bits, it takes a reasonable time to start, the overhead isn’t significant. Considering that rootfs build is not very time sensitive, this seems pretty okay. Of course, there will be an option to use an already deployed CA instead of this one.\nThe embedded CA is available on GitHub2 under an Apache 2 license.\n  Certificate Authority is not Voodoo ↩︎\n embedded CA on GitHub ↩︎\n   ","description":"There’s a certificate authority right in firebuild","tags":["ca","firebuild","go","grpc","pki","tls"],"title":"firebuild rootfs - gRPC with mTLS","uri":"/posts/2021-03-28-firebuild-rootfs-grpc-with-mtls/"},{"content":"what is Firecracker Firecracker is a virtualization technology for creating and managing secure, multi-tenant services suited for container-like and serverless scenarios. Firecracker workloads run in virtual machines, not containers. Unlike containers, they benefit from extra isolation properties provided by the hardware virtualization. Similar to containers, Firecracker VMs—microVMs—are lightweight and fast to boot. Like containers, they can be treated like cattle. They combine the flexibility of containers and security of virtual machines. These little things can be started in as little as 125 milliseconds and a single host can manage thousands of them! Firecracker was developed at Amazon Web Services primarily for Lambda and Fargate offerings.\nFirecracker uses Kernel Virtual Machine (KVM) to create and run microVMs. A minimalist design is achieved by removing unnecessary devices and guest-facing functionality. This reduces the memory footprint and attack surface of each individual VM leading to better utilization and increased security. At minimum, a microVM requires a Linux kernel image and a root file system. Networking can be provided by setting up interfaces manually or with container network interface (CNI).\nFirecracker is a couple of years old. Pretty young in the technology world but there are already interesting integrations out there. Kata Containers and WeaveWorks Ignite are the major ones.\nfirebuild There is only so much one can learn by looking at existing tools. The best way is to take something and build another useful thing on top of it. Only this way one can hit roadblocks cleared by others. Only this way one can investigate alternative avenues, possibly not considered before. That is why a few weeks ago I have started working on firebuild. The source code is on GitHub1.\nWith firebuild it is possible to:\n build root file systems directly from Dockerfiles tag and version root file systems run and manage microVMs on a single host define run profiles  The concept of firebuild is to leverage as much of the existing Docker world as possible. There are thousands of Docker images out there. Docker images are awesome because they encapsulate the software we want to run in our workloads, they also encapsulate dependencies. Dockerfiles are what Docker images are built from. Dockeriles are the blueprints of the modern infrastructure. There are thousands of them for almost anything one can imagine and new ones are very easy to write.\nan image is worth more than a thousand words Ah, but the idea is pretty difficult to visualize with a single image. So, instead, let me walk you though this example of running HashiCorp Consul 1.9.4 on Firecracker. I promise, any questions are answered further.\nBefore going all in, some prerequisites2.\ncreate a firebuild profile 1 2 3 4 5 6 7 8 9  sudo $GOPATH/bin/firebuild profile-create \\ \t--profile=standard \\ \t--binary-firecracker=$(readlink /usr/bin/firecracker) \\ \t--binary-jailer=$(readlink /usr/bin/jailer) \\ \t--chroot-base=/fc/jail \\ \t--run-cache=/fc/cache \\ \t--storage-provider=directory \\ \t--storage-provider-property-string=\"rootfs-storage-root=/fc/rootfs\" \\ \t--storage-provider-property-string=\"kernel-storage-root=/fc/vmlinux\"   create a base operating system root file system (baseos) firebuild uses the Docker metaphor. An image of an application is built FROM a base. An application image can be built FROM alpine:3.13, for example. Or FROM debian:buster-slim, or FROM registry.access.redhat.com/ubi8/ubi-minimal:8.3 and dozens others.\nIn order to fulfill those semantics, a base operating system image must be built before the application root file system can be created.\n1 2 3  sudo $GOPATH/bin/firebuild baseos \\  --profile=standard \\  --dockerfile $(pwd)/baseos/_/alpine/3.12/Dockerfile   create a root file system of the application (rootfs) To run an instance of HashiCorp Consul, firebuild requires the Consul application root file system. To build one:\n1 2 3 4 5 6 7  sudo $GOPATH/bin/firebuild rootfs \\  --profile=standard \\  --dockerfile=git+https://github.com/hashicorp/docker-consul.git:/0.X/Dockerfile \\  --cni-network-name=machine-builds \\  --ssh-user=alpine \\  --vmlinux-id=vmlinux-v5.8 \\  --tag=combust-labs/consul:1.9.4   start the application 1 2 3 4 5  sudo $GOPATH/bin/firebuild run \\  --profile=standard \\  --from=combust-labs/consul:1.9.4 \\  --cni-network-name=machines \\  --vmlinux-id=vmlinux-v5.8   query Consul First, find the VM ID:\n1 2 3  sudo $GOPATH/bin/firebuild ls \\  --profile=standard \\  --log-as-json 2\u003e\u00261 | jq '.id' -r   In my case, the value is wcabty1922gloailwrce. I used it to get the IP address of the VM:\n1 2 3  $ sudo $GOPATH/bin/firebuild inspect \\  --profile=standard \\  --vmm-id=wcabty1922gloailwrce | jq '.NetworkInterfaces[0].StaticConfiguration.IPConfiguration.IP' -r   The command returned 192.168.127.89. I could query Consul via REST API:\n1 2  curl http://192.168.127.89:8500/v1/status/leader \"127.0.0.1:8300\"   what the heck happened I have started by creating a firebuild profile. Technically firebuild does not require one. Common arguments may be provided on every execution. The profile exists for two reasons:\n it makes subsequent operations more concise by moving the tedious arguments away provides extra isolation with different chroots, cache directories, and image / kernel catalogs  The directories referenced in the profile must exist before a profile can be created.\nIn the next step, I have built a base operating system root file system. The elephant in the room question is:\n Why does this tool even require that step?\n Typical Linux in Docker has many parts removed. For example, there is no init system. Further, different base Docker images have often completely different sets of tools available. All that is for a good reason: Docker images supposed to be small, must start fast and limit the potential attack surface by removing what’s unnecessary.\nfirebuild builds Firecracker virtual machines. It does so from Dockerfile blueprints.\nIn order to provide a consistent experience, it requires a more or less functional multi-user Linux installation with components otherwise hidden in the Docker or OCI runtime. These base Linux installations are built from firebuild provided Dockerfiles, the --dockerfile $(pwd)/baseos/_/alpine/3.12/Dockerfile is a base Alpine 3.12. All the commands above were executed from $GOPATH/src/github.com/combust-labs/firebuild directory, hence the use of $(pwd) in the baseos build.\nfirebuild uses Docker to build the base operating root file system by:\n building a Docker image from the provided Dockerfile starting a container from newly built image exporting the root file system of the container to the ext4 file on the host using Docker API exec removing the container and the image persisting the built file in the storage provider and namespacing it, the example above results in the root file system stored in /fc/rootfs/_/alpine/3.12/rootfs persisting the build metadata next to the root file system file, above example gives /fc/rootfs/_/alpine/3.12/metadata.json  This custom firebuild provided Dockerfile is based on an upstream alpine:3.12 from Docker Hub.\nThe primary reason for following this path is to enable building Firecracker VMs from upstream Dockerfiles as often as possible. Other tools out there enable converting a Docker container into a rootfs file but to achieve that full VM experience, a Docker container has to be launched from a hand crafted Dockerfile or extra packages have to be installed on the running container before the export. Dockerfiles are fully auditable but these extra steps are not. The steps often differ between containers. It might be difficult to track how the rootfs was built, some benefits of using a blueprint could be lost.\nThe step 2 of the example builds Consul directly from the official HashiCorp Docker images GitHub repository. The application root file system was built using the rootfs command.\n Note: I refer to the application root file system as rootfs. Bit confusing at first because the result of the baseos command is technically also a rootfs. However, to mentally distinguish one from the other, I refer to to the base OS using the term baseos and an application is a rootfs. This may change in the future.\n The rootfs command does much more work than the baseos command.\nIt starts by fetching a Dockerfile from a source given via the --dockerfile argument. The source can be one of:\n a git+http(s):// style URL pointing at a git repository (does not have to be GitHub) a http:// of https:// URL, be careful here: There Will Be Dragons (read more3) a local file an inline Dockerfile standard ssh://, git:// and git+ssh:// URL with a Dockerfile path appended via :/path/to/Dockerfile  The most convenient is the local file system build or a git repository. If a git repository is used, firebuild will clone a complete repository to a temporary directory and treat the build further as a local file system build. Once the sources are on disk, firebuild loads and parses the Dockerfile. This part is preliminary and will change in favor of unattended bootstrap without SSH requirement: Next, a build time VM is started, firebuild connects to it via SSH and runs all commands from the Dockerfile against that VM.\nResources referenced with ADD and COPY commands are treated likewise and supported. Remote resources are supported. firebuild does its best to properly reflect any WORKDIR, USER and SHELL conditions. It supports --chown flags for ADD and COPY.\nWhat’s more, firebuild supports multi-stage builds. firebuild will build any stages with FROM ... as as regular Docker images and extract resources from the stage to the main build when COPY --from= is found. For example, it’s perfectly fine to build a Kafka Proxy root file system from:\n1 2 3 4 5 6 7  sudo $GOPATH/bin/firebuild rootfs \\  --profile=standard \\  --dockerfile=git+https://github.com/grepplabs/kafka-proxy.git:/Dockerfile#v0.2.8 \\  --cni-network-name=machine-builds \\  --ssh-user=alpine \\  --vmlinux-id=vmlinux-v5.8 \\  --tag=combust-labs/kafka-proxy:0.2.8   The Dockerfile commands statements which are not supported: ONBUILD, HEALTHCHECK and STOPSIGNAL (although the last one will be supported at a later stage).\nOnce all of that is finished, the build VM will be stopped, cleaned up and the resulting root file system will be persisted in the storage provider. A metadata file is stored next to the root file system. Currently, only the directory based storage provider is available.\nFinally, a resulting application is launched with the run command. The run command uses an unattended, cloud-init like mechanism. The metadata of the baseos and rootfs is combined. A guest facing version is put in MMDS (the Firecracker machine metadata service). MMDS provides a HTTP API available to both: the host and the guest. By default, if the guest was started with --allow-mmds flag, it can reach that API via 169.254.169.254 IP address. firebuild uses MMDS by default for all guests but this can be disabled. The guest facing metadata contains a bunch of information required to bootstrap the VM in a cloud-init style. These are fairly short so let’s look at an example:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44  { \"latest\": { \"meta-data\": { \"Drives\": { \"1\": { \"DriveID\": \"1\", \"IsReadOnly\": \"false\", \"IsRootDevice\": \"true\", \"Partuuid\": \"\", \"PathOnHost\": \"rootfs\" } }, \"EntrypointJSON\": \"{\\\"Cmd\\\":[\\\"agent\\\",\\\"-dev\\\",\\\"-client\\\",\\\"0.0.0.0\\\"],\\\"EntryPoint\\\":[\\\"docker-entrypoint.sh\\\"],\\\"Env\\\":{\\\"HASHICORP_RELEASES\\\":\\\"https://releases.hashicorp.com\\\"},\\\"Shell\\\":[\\\"/bin/sh\\\",\\\"-c\\\"],\\\"User\\\":\\\"0:0\\\",\\\"Workdir\\\":\\\"/\\\"}\", \"Env\": {}, \"ImageTag\": \"combust-labs/consul:1.9.4\", \"LocalHostname\": \"sharp-mirzakhani\", \"Machine\": { \"CPU\": \"1\", \"CPUTemplate\": \"\", \"HTEnabled\": \"false\", \"KernelArgs\": \"console=ttyS0 noapic reboot=k panic=1 pci=off nomodules rw\", \"Mem\": \"128\", \"VMLinux\": \"vmlinux-v5.8\" }, \"Network\": { \"CniNetworkName\": \"machines\", \"Interfaces\": { \"b6:16:f2:3d:29:cf\": { \"Gateway\": \"192.168.127.1\", \"HostDeviceName\": \"tap0\", \"IfName\": \"\", \"IP\": \"192.168.127.89\", \"IPAddr\": \"192.168.127.89/24\", \"IPMask\": \"ffffff00\", \"IPNet\": \"ip+net\", \"NameServers\": \"\" } } }, \"Users\": {}, \"VMMID\": \"wcabty1922gloailwrce\" } } }   The metadata contains information about attached drives, network interfaces, simple machine data, entrypoint info and user’s SSH keys, if --identity-file and --ssh-user arguments were provided. The component responsible for bootstrapping the VM from this data is called vminit and can be found in this GitHub repository4. The compiled binary is baked into the baseos (suboptimal but it’s a first iteration) and invoked as a system service on VM start.\nCurrently, vminit does the following:\n update /etc/hosts file if the VM has a network interface and make sure the VM resolves itself via configured hostname on the interface IP address update /etc/hostname to the configured hostname create an environment variables /etc/profile.d/run-env.sh file for any variables passed via --env and --env-file flags of the run command when users contains a user entry with SSH keys, write those SSH keys to the respective authorized_keys file to enable SSH access; an example of a user entry:  1 2 3 4 5  \"Users\": { \"alpine\": { \"SSHKeys\": \"ssh-rsa ... \\nssh-rsa ...\\n\" } }    write the /usr/bin/firebuild-entrypoint.sh program responsible for invoking the entrypoint from MMDS data  When the machine starts, vminit looks for the /usr/bin/firebuild-entrypoint.sh and if one is found, executes it. Fingers crossed, things went well and the application starts automatically.\nThat was a high level overview of the process.\nother useful VM related commands List running VMs:\n1  sudo firebuild ls --profile=standard   Inspect the metadata of a running VM:\n1  sudo firebuild inspect --profile=standard --vmm-id=...   Terminate a running VM:\n1  sudo firebuild kill --profile=standard --vmm-id=...   unclean shutdowns Firecracker VMs will stop when a reboot command is issued in the guest. I call these unclean meaning that they will leave a bunch of VM related directories on disk:\n the jail directory the run cache directory the CNI cache for the VM interface and a veth pair  To mass-clean all these for all exited VMs, run:\n1  sudo firebuild purge --profile=standard   profile commands List profiles:\n1  sudo firebuild profile-ls   Inspect a profile:\n1  sudo firebuild profile-inspect --profile=...   Profiles may be updated by issuing subsequent profile-create commands with a name of an existing profile.\nwhat’s coming next These are still early stages for firebuild. There are many things to improve.\nshort term  tests, tests, tests, …, end to end tests remove the requirement to have SSH access during rootfs build and move to the MMDS / vminit build add support for building directly from Docker images for special cases where the Dockerfile might not be available or is difficult to handle, and example is Jaeger Docker image where the Dockerfile does not incorporate the binary artifact build add a command to build a Linux kernel image directly from the tool manage resolv.conf and nsswitch.conf on the guest  mid term  add service catalog support for service service discovery add support for additional disks a VM management API an event bus / hook to be able to react to events originating in firebuild  long term  enable rootfs build and run related operation split via remote build and run operators provide a remote registry type of system to host rootfs and kernel files externally add networking tools to create CNI bridge and overlay networks and expose VMs on outside of the host  And probably many, many more as the time goes by. I’ll be writing more as firebuild develops.\nThanks for reading. Stay safe.\n  The source code is on GitHub ↩︎\n firebuild prerequisites ↩︎\n Caveats when building from the URL ↩︎\n firebuild-mmds GitHub repository ↩︎\n   ","description":"Manage firecracker root file systems and VMMs","tags":["firecracker","microvm","firebuild","docker"],"title":"Introducing firebuild","uri":"/posts/2021-03-23-introducing-firebuild/"},{"content":"This article describes the prerequisites to the Introducing firebuild.\ninstall Firecracker and Jailer on the host Firecracker works only on Linux. You can use this program to install and link the binaries on your system.\ninstall and configure golang 1.16+ The tc-redirect-tap CNI plugin (mentioned below) requires golang to build, as does firebuild. firebuild requires golang 1.16+ so install it:\n1 2 3 4  rm -rf /usr/local/go \u0026\u0026 tar -C /usr/local -xzf go1.16.2.linux-amd64.tar.gz mkdir -p $HOME/dev/golang/{bin,src} export PATH=$PATH:/usr/local/go/bin:$HOME/dev/golang/bin export GOPATH=$HOME/dev/golang   The most recent version can be downloaded from golang website.\ninstall CNI plugins firebuild assumes CNI availability. Installing the plugins is very straightforward. Create /opt/cni/bin/ directory and download the plugins:\n1 2 3  mkdir -p /opt/cni/bin curl -O -L https://github.com/containernetworking/plugins/releases/download/v0.9.1/cni-plugins-linux-amd64-v0.9.1.tgz tar -C /opt/cni/bin -xzf cni-plugins-linux-amd64-v0.9.1.tgz   Firecracker requires also the tc-redirect-tap plugin. Unfortunately, this one does not offer downloadable binaries and has to be built from sources.\n1 2 3 4  mkdir -p $GOPATH/src/github.com/awslabs/tc-redirect-tap cd $GOPATH/src/github.com/awslabs/tc-redirect-tap git clone https://github.com/awslabs/tc-redirect-tap.git . make install   create CNI network configurations The article assumes two different CNI networks:\n one for machine builds one for running machines  CNI network config lists are stored in /etc/cni/conf.d. Create both like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57  mkdir -p /etc/cni/conf.d cat \u003c\u003cEOF \u003e /etc/cni/conf.d/machines.conflist { \"name\": \"machines\", \"cniVersion\": \"0.4.0\", \"plugins\": [ { \"type\": \"bridge\", \"name\": \"machines-bridge\", \"bridge\": \"machines0\", \"isDefaultGateway\": true, \"ipMasq\": true, \"hairpinMode\": true, \"ipam\": { \"type\": \"host-local\", \"subnet\": \"192.168.127.0/24\", \"resolvConf\": \"/etc/resolv.conf\" } }, { \"type\": \"firewall\" }, { \"type\": \"tc-redirect-tap\" } ] } EOF cat \u003c\u003cEOF \u003e /etc/cni/conf.d/machine-builds.conflist { \"name\": \"machine-builds\", \"cniVersion\": \"0.4.0\", \"plugins\": [ { \"type\": \"bridge\", \"name\": \"machine-builds-bridge\", \"bridge\": \"builds0\", \"isDefaultGateway\": true, \"ipMasq\": true, \"hairpinMode\": true, \"ipam\": { \"type\": \"host-local\", \"subnet\": \"192.168.128.0/24\", \"resolvConf\": \"/etc/resolv.conf\" } }, { \"type\": \"firewall\" }, { \"type\": \"tc-redirect-tap\" } ] } EOF   build and install firebuild from sources At this moment, there are no binaries for firebuild. It must be built from sources:\n1 2 3 4  mkdir -p $GOPATH/src/github.com/combust-labs/firebuild cd $GOPATH/src/github.com/combust-labs/firebuild git clone https://github.com/combust-labs/firebuild . go install   make the $GOPATH system wide 1  echo \\$GOPATH=$GOPATH \u003e\u003e /etc/profile   That’s it.\n","description":"Read this first before reading about firebuild","tags":["firebuild"],"title":"firebuild prerequisites","uri":"/posts/2021-03-22-firebuild-prerequisites/"},{"content":"Dockerfiles are awesome There is so much software out there packaged as Docker images. Operating systems, SQL and NoSQL databases, reverse proxies, compilers, everything. Safe to say, most of that software available as Docker containers is built from the common file format - the Dockerfile. Dockerfiles are awesome. They are recipes for getting a bit of software functional.\nhow have I been building Firecracker VMMs so far So far, all of my VMMs were built from Docker images using the following steps:\n pull / build a Docker image start a container with an additional volume where the host directory is a mounted ext4 file system copy the operating system directories I needed to this other volume stop the container use the resulting ext4 file system as the root VMM volume  Here’s an example.\nThere isn’t much wrong with this process and it does work surprisingly well for the majority of the containers out there. This is also how Weaveworks Ignite works, this is what the official Firecracker documentation suggests and what many write ups on Firecracker describe.\nunder the magnifier The above approach gets us the first 98% of the work done. It’s okay. But, there are certain important details missing.\nThe most obvious one: after the conversion by copying, we lose the ENTRYPOINT information. The Docker image provides us with two commands: ENTRYPOINT and CMD. Both instruct the container which program to run and what arguments to pass when the container starts. Without the ENTRYPOINT and optionally the CMD, the resulting VMM will start but it will not execute anything. We have to somehow modify the file system, post-copy, and add the command to start what we want to start.\nIn my previous write ups, I was adding a local service definition to the VMM during the copy stage. That is really cool but the problem with this approach is that virtually every image out there has its own dedicated configuration. Even if we could assume that 99% of all Docker images use the docker-entrypoint.sh as a conventional ENTRYPOINT, the CMD is going to differ.\nThen, there are additional parameters affecting the ENTRYPOINT. There is the WORKDIR and the USER command, there is the SHELL command, there are build arguments and environment variables.\nBy just copying the container file system, yes, we do get the final product. However, we are losing a lot of context and insight into what the result really is when it is to be started.\nFinally, plenty of containers come with additional labels and exposed ports information. All that information is lost if we are not correlating the file system copy against the original Dockerfile.\nFor sure, the manual build is doable but it won’t scale.\nanatomy of a Dockerfile Back to a Dockerfile. Let’s have a look at the official HashiCorp Consul Dockerfile as an example. Well, it ain’t 10 lines of code but it ain’t rocker science either. If we focus on the structure, it turns out to be fairly easy to understand:\n use base Alpine 3.12 run some commands Linux commands … and, … that’s it, really, sprinkled with some environment variables and labels  The contract is: given a clean installation of the Alpine Linux 3.12, after executing the RUN commands, one can execute the ENTRYPOINT and have Consul up and running.\nNot all Dockerfiles are that easy. There is a lot of software out there built with multi-stage builds. To keep it simple and easy to track, let’s look at Kafka Proxy Dockerfile. Or Minio for that matter.\nWe can find two FROM commands there. The first FROM defines the named stage, people often call it builder. Docker will basically build an image using every command until the next FROM and save it on disk.\nThe next stage, the one without as ..., let’s call it the main stage, is created again from the base operating system. In case of kafka Proxy, it’s Alpine 3.12. In case of Minio, it is Red Hat UBI 8. The main stage can copy the resources from the previous stages using the COPY --from=$stage-name command. When such command is processed, Docker will reach into the first image it built and copy the selected resources into the main stage image. Clever and very effective.\nThe builder stage is essentially a cache. In both cases, it is a golang program that is compiled only once and the main stage can be built quicker, assuming that the compiled output in the builder stage hasn’t changed.\nwe can build a VMM from a Dockerfile It’s possible to take a Dockerfile, parse it and apply all the relevant operations on a clean base operating system installation. The single stage build files are easy. Multi-stage builds can be a little more complex. Let’s consider what the process might look like.\nThere are two types of artifacts:\n named stages serve as resource cache the rootfs, the final build when all previous stages are built  There can be only one unnamed build stage in a Dockerfile and it will always be built last.\nNamed stages:\n given a Dockerfile, parse it using the BuildKit dockerfile parser find explicit stages delimited with the respective FROM commands every build stage with FROM ... as ... can be built as a Docker image using the Moby client  for such build, remove the as ... part from the FROM command and save using a random image name   build named stages as Docker images, no need to have a container  for each stage  export the image to a tar file search the layers for all resources required by COPY --from commands; the layers are just tar files embedded in the main tar file   extract matched resources to a temporary build directory remove temporary image    Main build stage:\n requires an existing implementation of the underlying OS, think: alpine:3.12 or alpine:3.13  this is the only part which has to be built by hand   execute relevant RUN commands in order, pay attention to ARG and ENV commands such that the RUN commands are expanded correctly execute ADD / COPY commands, pay attention to the --from flag in both cases, keep track of WORKDIR and USER changes such that added / copied resources are placed under correct paths and commands are executed as correct users in correct locations  why By building the rootfs in this way, it is possible to infer additional information which is otherwise lost when copying the file system from a running container. For example:\n correctly set up a local service to automatically start the application on VMM boot start the application user the uid / gid defined in the Dockerfile infer a shell from the SHELL command extract otherwise missing metadata hiding in LABEL and EXPOSE commands  Sounds doable. Does it make sense? Good question. Is the Docker image the right medium to source the Firecracker VMM root file system from? It gets us the first 98% of the work done but the devil is in details. Dockerfile can get us all the way there.\n","description":"TL’DR: won’t scale","tags":["docker","firecracker","microvm"],"title":"Thoughts on creating VMMs from Docker images","uri":"/posts/2021-03-03-thoughts-on-creating-vmms-from-docker-images/"},{"content":"A Firecracker release comes with two binaries - the firecracker and the jailer programs. The jailer brings even more isolation options to Firecracker by creating and securing a unique execution environment for each VMM.\nwhat can it do  check the uniqueness and validity of the VMM id, maximum length of 64 characters, alphanumeric only assign NUMA node check the existence of the exec_file run the VMM as a specific user / group assign cgroups assign the VMM into a dedicated network namespace a VMM can be damonized  what does it do This part comes from the jailer documentation1. When the jailer starts, it goes through the following process:\n all paths and the VMM id will validated all open file descriptors based on /proc/\u003cjailer-pid\u003e/fd except input, output and error will be closed the \u003cchroot_base\u003e/\u003cexec_file_name\u003e/\u003cid\u003e/root directory will be created - this is the chroot_dir  exec_file_name is the last path component of exec_file (for example, that would be firecracker for /usr/bin/firecracker) if the path already exists, the jailer will fail to start the VMM because the assumption is that the VMM IDs are unique if exec_file is a link, jailer will readlink the value and use the name of the link source   the exec_file will copied to \u003cchroot_base\u003e/\u003cexec_file_name\u003e/\u003cid\u003e/root/\u003cexec_file_name\u003e cgroups folder structure will be created; right now the jailer uses cgroup v1  On most systems, this is mounted by default in /sys/fs/cgroup (should be mounted by the user otherwise). The jailer will parse /proc/mounts to detect where each of the controllers required in --cgroup can be found (multiple controllers may share the same path). For each identified location (referred to as \u003ccgroup_base\u003e), the jailer creates the \u003ccgroup_base\u003e/\u003cexec_file_name\u003e/\u003cid\u003e subfolder, and writes the current pid to \u003ccgroup_base\u003e/\u003cexec_file_name\u003e/\u003cid\u003e/tasks. Also, the value passed for each \u003ccgroup_file\u003e is written to the file. If --node is used the corresponding values are written to the appropriate cpuset.mems and cpuset.cpus files.\n unshare() into a new mount namespace will be called, use pivot_root() to switch the old system root mount point with a new one base in chroot_dir, switch the current working directory to the new root, unmount the old root mount point, and call chroot into the current directory /dev/net/tun will be created inside of the jail using mknod /dev/kvm will be created inside of the jail using mknod the ownership of the chroot_dir, /dev/net/tun and /dev/kvm will be changed using chown based on the provided uid:gid if --netns \u003cnetns\u003e is present, attempt to join the specified network namespace if --daemonize is specified, call setsid() and redirect STDIN, STDOUT, and STDERR to /dev/null. privileges will be dropped by setting the provided uid:gid exec into \u003cexec_file_name\u003e --id=\u003cid\u003e --start-time-us=\u003copaque\u003e --start-time-cpu-us=\u003copaque\u003e and forward any extra arguments provided to the jailer after --, where:  id: (string) - the id argument provided to jailer opaque: (number) time calculated by the jailer that it spent doing its work    The jailer seems to be the proper way of running Firecracker VMMs. firectl, which I have discussed previously, has the jailer support. It was pretty easy to convert existing VMMs. There’s a couple of quirks to the firectl configuration, mostly - arguments must be explicitly assigned. The Golang SDK supports the defaults, like /srv/jailer for the chroot_base but firectl does not properly use them internally so just make sure you always pass them.\nhow to do it Here’s how I run my VMM via the jailer:\n1 2 3 4 5 6 7 8 9 10 11  sudo $GOPATH/bin/firectl \\  --jailer=/usr/bin/jailer \\  --exec-file=$(readlink /usr/bin/firecracker) \\  --id=alpine1 \\  --chroot-base-dir=/srv/jailer \\  --kernel=/firecracker/kernels/vmlinux-v5.8 \\  --root-drive=/firecracker/filesystems/alpine-base-root.ext4 \\  --cni-network=alpine \\  --veth-iface-name=alpine1 \\  --ncpus=1 \\  --memory=128   The above will start the Firecracker VMM via the /usr/bin/jailer binary.\nI use readlink because my /usr/bin/firecracker is a link to /usr/bin/firecracker-v0.22.4-x86_64. If I don’t use readlink, the jailer for whatever reason creates \u003cchroot_dir\u003e/firecracker but attempts to launch the VMM from \u003cchroot_dir\u003e/firecracker-v0.22.4-x86_64 directory. readlink avoids that problem in my setup.\nI have assigned a unique id to my VMM and explicitly passed the --chroot-base-dir. If I would not, this would have happened. The rest is the standard Firecracker firectl stuff discussed in the previous write ups.\nAll omitted arguments are set to their defaults so things like uid:gid and NUMA node will be all 0. Good for now.\nHere’s what the chroot_dir structure looks like for a VMM with only a root file system:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  $ sudo tree /srv/jailer/ /srv/jailer/ └── firecracker-v0.22.4-x86_64 └── alpine1 └── root ├── alpine-base-root.ext4 ├── dev │ ├── kvm │ └── net │ └── tun ├── firecracker-v0.22.4-x86_64 ├── run │ └── firecracker.socket └── vmlinux-v5.8    the root/alpine-base-root.ext4 is a link to the actual file system the root/vmlinux-v5.8 is the a link to the actual kernel  chroot strategy The file system and the kernel linking is not done by the jailer. It’s the firectl doing it via the chroot strategy mechanism. The Golang SDK provides a default naive strategy,. It’s actually called like that, I’m not being cocky. The default strategy can be replace with a custom logic implementing the firecracker.HandlerAdapter interface.\nSo in AWS, one selects a base AMI and launches a VM from it. That creates a volume and subsequent VM starts use that volume. This could be a way forward to build something similar for Firecracker.\nclosing words I have subconsciously avoided touching the jailer before as I have seen it as a pretty complex feature. Considering what it gives, I must admit, it was very easy to get it in. I haven’t yet tried launching anything under a specific uid:gid but I do not expect any issues there.\n  The jailer documentation ↩︎\n   ","description":"Making the Firecracker VMMs even more secure","tags":["firecracker","jailer","microvm"],"title":"The jailer","uri":"/posts/2021-02-19-the-jailer/"},{"content":"Last night’s problem with the second VMM conflicting on the network layer with the first one was indeed the veth0 name hard coded in firectl. I’ve added the --veth-iface-name argument to firectl and I am now able to start multiple VMMs on a single bridge.\n1 2 3 4 5 6 7 8 9  sudo firectl \\  --firecracker-binary=/usr/bin/firecracker \\  --kernel=/firecracker/kernels/vmlinux-v5.8 \\  --root-drive=/firecracker/filesystems/alpine-base-root.ext4 \\  --cni-network=alpine \\  --socket-path=/tmp/alpine.sock \\  --ncpus=1 \\  --memory=128 \\  --veth-iface-name=vethalpine1   1 2 3 4 5 6 7 8 9  sudo firectl \\  --firecracker-binary=/usr/bin/firecracker \\  --kernel=/firecracker/kernels/vmlinux-v5.8 \\  --root-drive=/firecracker/filesystems/alpine-base-root.ext4-2 \\  --cni-network=alpine \\  --socket-path=/tmp/alpine2.sock \\  --ncpus=1 \\  --memory=128 \\  --veth-iface-name=vethalpine2   The bit to looks at is the way to handle multiple copies of the root file system.\nOne step closer to the ETCD cluster on Firecracker.\n","description":"Multiple VMMs on a single bridge are working","tags":["firecracker","microvm"],"title":"It’s all about the the Iface name","uri":"/posts/2021-02-18-its-all-about-the-iface-name/"},{"content":"Today I have looked at creating my own bridge networks for Firecracker VMMs. I already used CNI setups when evaluating the HashiCorp Nomad firecracker task driver1. Back then I incorrectly stated that Firecracker depends on certain CNI plugins. It doesn’t, it can take advantage of any CNI setup as long as the tc-redirect-tap is in the chained plugins.\nThe Nomad task driver had some issues, briefly:\n every now and then, oddly, the task would never shut the VMM down and the only way to make the VMM gow down was to sudo kill nomad I tried updating the task driver to latest SDK version but I was not able to upgrade the Firecracker dependency past a specific commit, any version after that specific commit makes the VMM come up, the network setup to be there but the VMM is not reachable, really, really weird issue - reported it here  So today, I took a different route.\nfirectl firectl2 is a command line utility for launching VMMs and a reference implementation of an application built on top of the Firecracker Golang SDK3. I have some exposure to the firectl from when I was trying to upgrade the Nomad task driver. The task driver uses parts of firectl code internally.\nOne thing missing from thefirectl is the option to define the CNI network name to use. Something similar to this snippet from the SDK readme:\n1 2 3 4 5 6 7 8  { NetworkInterfaces: []firecracker.NetworkInterface{{ CNIConfiguration: \u0026firecracker.CNIConfiguration{ NetworkName: \"fcnet\", IfName: \"veth0\", }, }} }   The first thing to do was to add support for that. I’ve created a firectl fork and pushed my changes to GitHub, here4.\nMy changes:\n a new argument to declare the --cni-network to use add the network interface based on the new argument added --netns argument required when using the CNI networks  Having built my version of firectl, I’ve declared this CNI conflist (in /firecracker/cni/conf.d/alpine.conflist):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  { \"name\": \"alpine\", \"cniVersion\": \"0.4.0\", \"plugins\": [ { \"type\": \"bridge\", \"name\": \"alpine-bridge\", \"bridge\": \"alpinebridge0\", \"isDefaultGateway\": true, \"ipMasq\": true, \"hairpinMode\": true, \"ipam\": { \"type\": \"host-local\", \"subnet\": \"192.168.127.0/24\", \"resolvConf\": \"/etc/resolv.conf\" } }, { \"type\": \"firewall\" }, { \"type\": \"tc-redirect-tap\" } ] }   and started the VMM via firectl, like this:\n1 2 3 4 5 6 7 8  sudo ./firectl \\  --firecracker-binary=/usr/bin/firecracker \\  --kernel=/firecracker/kernels/vmlinux-v5.8 \\  --root-drive=/firecracker/filesystems/alpine-base-root.ext4 \\  --cni-network=alpine \\  --socket-path=/tmp/alpine.sock \\  --ncpus=1 \\  --memory=128   This works line a charm. I can SSH to the VMM via the IP address which can be found with:\n1  cat /var/lib/cni/networks/alpine/last_reserved_ip.0   The VMM can reach the outside world, no degradation here.\nhost-local IPAM Reading more about the host-local IPAM, I figured that the issue with IP addresses changing on every start of the VMM is the normal behaviour. Basically, it stores the allocations on disk, under the /var/lib/cni/networks/\u003cnetwork-name\u003e directory. This is, I think, referred to as IP address leakage.\nOne way to take care of that, is to have a custom operator listening for when the VMM is stopped and removing that IP allocation manually. How, it’s undefined but it should be rather straightforward by using a combination of MMDS and some custom agent.\nThe bridge The other thing to note is, I’m no longer using the ptp plugin. I’m using the bridge instead and longer the docker0 one. This was my little attempt at launching two VMMs on the same bridge. Well, this didn’t work…\nFirecracker Golang SDK tries to remove the existing network configuration when launching another VMM with the same network name. Here’s the error I have seen:\n1 2 3 4 5 6 7 8  sudo ./firectl \\  --firecracker-binary=/usr/bin/firecracker \\  --kernel=/firecracker/kernels/vmlinux-v5.8 \\  --root-drive=/firecracker/filesystems/alpine-base-root.ext4-2 \\  --cni-network=alpine \\  --socket-path=/tmp/alpine2.sock \\  --ncpus=1 \\  --memory=128   WARN[0000] Failed handler \"fcinit.SetupNetwork\": failure when invoking CNI: failed to delete pre-existing CNI network {NetworkName:alpine NetworkConfig:\u003cnil\u003e IfName:veth0 VMIfName: Args:[] BinPath:[/opt/cni/bin] ConfDir:/etc/cni/conf.d CacheDir:/var/lib/cni/32171e6d-2b1b-4060-9555-e17314972ace containerID:32171e6d-2b1b-4060-9555-e17314972ace netNSPath:/var/run/netns Force:false}: failed to delete CNI network list \"alpine\": running [/sbin/iptables -t nat -D POSTROUTING -s 192.168.127.2 -j CNI-166dba6e0b91a8f3d41c9a89 -m comment --comment name: \"alpine\" id: \"32171e6d-2b1b-4060-9555-e17314972ace\" --wait]: exit status 2: iptables v1.6.1: Couldn't load target `CNI-166dba6e0b91a8f3d41c9a89':No such file or directory Try `iptables -h' or 'iptables --help' for more information. FATA[0000] Failed to start machine: failure when invoking CNI: failed to delete pre-existing CNI network {NetworkName:alpine NetworkConfig:\u003cnil\u003e IfName:veth0 VMIfName: Args:[] BinPath:[/opt/cni/bin] ConfDir:/etc/cni/conf.d CacheDir:/var/lib/cni/32171e6d-2b1b-4060-9555-e17314972ace containerID:32171e6d-2b1b-4060-9555-e17314972ace netNSPath:/var/run/netns Force:false}: failed to delete CNI network list \"alpine\": running [/sbin/iptables -t nat -D POSTROUTING -s 192.168.127.2 -j CNI-166dba6e0b91a8f3d41c9a89 -m comment --comment name: \"alpine\" id: \"32171e6d-2b1b-4060-9555-e17314972ace\" --wait]: exit status 2: iptables v1.6.1: Couldn't load target `CNI-166dba6e0b91a8f3d41c9a89':No such file or directory In the process, some of the underlying network configuration for the running VMM was wiped so my SSH connection was handing. Similar to what happens when one disconnects the network cable or disables WiFi.\nMaybe it’s related to the fact that my veth0 interface name is hard coded in my firectl. Something to look at.\nWhat would be nice to have is to decouple the network setup into two steps:\n create the bridge before launching VMMs, like what docker network create does setup the tap device at the VMM launch time  Maybe the CNI implementation from Weaveworks Ignite5 can serve as an inspiration (thanks, Michał…)\nFood for thought.\n  Vault on Firecracker with CNI plugins and Nomad ↩︎\n firectl GitHub repository ↩︎\n Firecracker Golang SDK GitHub repository ↩︎\n CNI network support for firectl ↩︎\n the CNI implementation from Weaveworks Ignite ↩︎\n   ","description":"Because docker0 is not the right choice","tags":["firecracker","microvm"],"title":"Bridging the Firecracker network gap","uri":"/posts/2021-02-17-bridging-the-firecracker-network-gap/"},{"content":"Towards the end of the Firecracker VMM with additional disks article1 I concluded that I didn’t know how to live resize an attached drive. It turns out it is possible and it’s very easy to do using the Firecracker VMM API.\nTo launch the VMM with the API, I have to drop the --no-api argument (obviously) and use --api-sock with the path to the socket file. In a production system, I’d use a directory other than /tmp.\n1 2 3  sudo firecracker \\  --config-file /firecracker/configs/alpine-config.json \\  --api-sock /tmp/alpine-base.sock   The VMM API Firecracker server exposes a Swagger documented API on a unix socket available under the --api-sock path. There is an instance of the API per the socket file. In essence - one API for every VMM instance. A VMM will not start if the socket file exists.\nBecause firecracker command is executed with elevated privileges and the socket file is owned by the elevated user, the curl command has to be executed with elevated privileges.\nFirecracker server API consumes and returns JSON. At the time of writing, the Swagger file can be looked at on GitHub2. Some introductory examples are available here3.\nThe API offers two classes of operations: pre-boot and post-boot. The pre-boot ones are interesting. They are clearly documented as such. Quick glance shows that the boot device, network interfaces, drives and the balloon device can be called at the pre-boot stage. A VMM can also be launched from a snapshot.\nThere clearly exists the potential for an EBS, snapshots or ENI AWS-like management plane.\nLive resize the drive Back to the topic. So, right now, there is no API call to list the drives attached to the VMM. Neither the / nor the /machine-config returns that info.\nOne has to either have access to the VMM config file or could use the MMDS (Microvm Metadata Service) to store that info on boot. I’ll have a look at the MMDS at some other time.\nYes, the drive. I do know the drive ID I was trying to previously resize. It’s the vol2 stored at /firecracker/filesystems/alpine-vol2.ext4.\nThe API endpoint I’m investigating is the /drives/{drive_id}. The POST operation is the pre-boot one and it does complain when executing it on a running instance.\nHTTP/1.1 400 Server: Firecracker API Connection: keep-alive Content-Type: application/json Content-Length: 88 {\"fault_message\":\"The requested operation is not supported after starting the microVM.\"} Just PATCH it? The PATCH allows me to tell the VMM that something about the underlying volume has changed. First, a glance at fdisk -l output of the running VMM:\n1  172:~$ sudo fdisk -l   Disk /dev/vda: 500 MiB, 524288000 bytes, 1024000 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk /dev/vdb: 500 MiB, 524288000 bytes, 1024000 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes The device I’m interested in is the /dev/vdb. Currently 500MiB.\n So is live resize as simple as…, surely not…, is it … ?\n 1  dd if=/dev/zero bs=1M count=50 \u003e\u003e /firecracker/filesystems/alpine-vol2.ext4   50+0 records in 50+0 records out 52428800 bytes (52 MB, 50 MiB) copied, 0.0442937 s, 1.2 GB/s 1 2 3 4 5 6 7 8  sudo curl \\  --unix-socket /tmp/alpine-base.sock -i \\  -X PATCH \"http://localhost/drives/vol2\" \\  -H \"Accept: application/json\" \\  -d \"{ \\\"drive_id\\\": \\\"vol2\\\", \\\"path_on_host\\\": \\\"/firecracker/filesystems/alpine-vol2.ext4\\\" }\"   HTTP/1.1 204 Server: Firecracker API Connection: keep-alive Firecracker logs the following:\n[ 148.225812] virtio_blk virtio1: [vdb] new size: 1126400 512-byte logical blocks (577 MB/550 MiB) [ 148.227542] vdb: detected capacity change from 524288000 to 576716800 A quick look at fdisk -l again:\n1  172:~$ sudo fdisk -l /dev/vdb   Disk /dev/vdb: 550 MiB, 576716800 bytes, 1126400 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Ah, yes! I could now use resize2fs to extends the existing ext4 partition to fit the new size. Awesome!\nOh, and I can now shut the VMM down without SSH:\n1 2 3 4 5 6 7  sudo curl --unix-socket /tmp/alpine-base.sock -i \\  -X PUT \"http://localhost/actions\" \\  -H \"Accept: application/json\" \\  -H \"Content-Type: application/json\" \\  -d \"{ \\\"action_type\\\": \\\"SendCtrlAltDel\\\" }\"   HTTP/1.1 204 Server: Firecracker API Connection: keep-alive 1  sudo rm /tmp/alpine-base.sock     Firecracker VMM with additional disks ↩︎\n Firecracker server Swagger file at the time of writing the article, version v0.22.4 ↩︎\n Firecracker API usage examples ↩︎\n   ","description":"It’s possible to live resize Firecracker VMM drive with the API","tags":["firecracker","microvm"],"title":"Live resize Firecracker VMM drive","uri":"/posts/2021-02-16-live-resize-firecracker-vmm-drive/"},{"content":"Before looking at the networking options, I have looked at adding extra drives to my Firecracker VMMs. Storing data on the root file system will not scale well long term. Additional disks will be a good solution to persist application specific data across reboots and upgrades.\nCreate the disk on the host First, create an additional file system on the host:\n1 2  dd if=/dev/zero of=\"/firecracker/filesystems/alpine-vol2.ext4\" bs=1M count=500 mkfs.ext4 \"/firecracker/filesystems/alpine-vol2.ext4\"   Reconfigure the VMM Change the VMM drives configuration:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  \"drives\": [ { \"drive_id\": \"rootfs\", \"path_on_host\": \"/firecracker/filesystems/alpine-base-root.ext4\", \"is_root_device\": true, \"is_read_only\": false }, { \"drive_id\": \"vol2\", \"path_on_host\": \"/firecracker/filesystems/alpine-vol2.ext4\", \"is_root_device\": false, \"is_read_only\": false } ],   and relaunch the VMM.\nVerify the configuration In another terminal:\n1  172:~$ sudo fdisk -l   Disk /dev/vda: 500 MiB, 524288000 bytes, 1024000 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk /dev/vdb: 500 MiB, 524288000 bytes, 1024000 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes I can mount it:\n1 2  172:/home/alpine# mkdir /mnt 172:/home/alpine# mount /dev/vdb /mnt   Firecracker shows me:\n[ 1800.756913] EXT4-fs (vdb): mounted filesystem with ordered data mode. Opts: (null) [ 1800.758116] ext4 filesystem being mounted at /mnt supports timestamps until 2038 (0x7fffffff) I can also write to it:\n1  172:/home/alpine# ls -la /mnt/   total 14 drwxr-xr-x 3 root root 1024 Feb 14 15:07 . drwxr-xr-x 16 root root 1024 Feb 14 15:49 .. drwx------ 2 root root 12288 Feb 14 15:07 lost+found 1 2  172:/home/alpine# echo 1 \u003e /mnt/test 172:/home/alpine# ls -la /mnt/   total 15 drwxr-xr-x 3 root root 1024 Feb 14 15:50 . drwxr-xr-x 16 root root 1024 Feb 14 15:49 .. drwx------ 2 root root 12288 Feb 14 15:07 lost+found -rw-r--r-- 1 root root 2 Feb 14 15:50 test Live resize does not work Not sure at this stage if this is because of how the system is built or if it’s a limitation of the VMM but I am not able to live resize the disk. Executing on the host:\n1  dd if=/dev/zero bs=1M count=100 \u003e\u003e /firecracker/filesystems/alpine-vol2.ext4   100+0 records in 100+0 records out 104857600 bytes (105 MB, 100 MiB) copied, 0.0798364 s, 1.3 GB/s and in the VMM:\n1 2  172:/home/alpine# sudo blockdev --flushbufs /dev/vdb 172:/home/alpine# sudo blockdev --rereadpt /dev/vdb   does not reflect the change. The change only becomes visible after VMM restart.\n","description":"Adding more disks to the Firecracker VMM","tags":["firecracker","microvm"],"title":"Firecracker VMM with additional disks","uri":"/posts/2021-02-14-firecracker-vmm-with-additional-disks/"},{"content":"The quest to launch an ETCD cluster on Firecracker starts here.\nIn this post, I’m describing how I’ve built my initial Alpine 3.13 VMM with OpenSSH and a dedicated sudoer user. In AWS, when one launches a Ubuntu instance, one can access it via ssh ubuntu@\u003caddress\u003e, a CentOS VM is ssh centos@\u003caddress\u003e. At the end of this write up, I’ll have ssh alpine@\u003caddress\u003e. This VMM will have access to the outside world so I can install additional software and even ping the BBC! For the networking, I’ll use the Docker docker0 bridge; inspired again by Julia Evans, the Day 41: Trying to understand what a bridge is1 was very helpful. I will look at my own networking setup in future write ups.\nThe result is a refinement of the process from my previous Firecracker articles.\nDockerfile The root file system is built from an Alpine 3.13 Docker image.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  FROMalpine:3.13RUN apk update \\ \t\u0026\u0026 apk add openrc openssh sudo util-linux \\ \t\u0026\u0026 ssh-keygen -A \\ \t\u0026\u0026 mkdir -p /home/alpine/.ssh \\ \t\u0026\u0026 addgroup -S alpine \u0026\u0026 adduser -S alpine -G alpine -h /home/alpine -s /bin/sh \\ \t\u0026\u0026 echo \"alpine:$(cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 32 | head -n1)\" | chpasswd \\ \t\u0026\u0026 echo '%alpine ALL=(ALL) NOPASSWD: ALL' \u003e /etc/sudoers.d/alpine \\ \t\u0026\u0026 ln -s agetty /etc/init.d/agetty.ttyS0 \\ \t\u0026\u0026 echo ttyS0 \u003e /etc/securetty \\ \t\u0026\u0026 rc-update add agetty.ttyS0 default \\ \t\u0026\u0026 rc-update add devfs boot \\ \t\u0026\u0026 rc-update add procfs boot \\ \t\u0026\u0026 rc-update add sysfs boot \\ \t\u0026\u0026 rc-update add local defaultCOPY ./key.pub /home/alpine/.ssh/authorized_keysRUN chown -R alpine:alpine /home/alpine \\ \t\u0026\u0026 chmod 0740 /home/alpine \\ \t\u0026\u0026 chmod 0700 /home/alpine/.ssh \\ \t\u0026\u0026 chmod 0400 /home/alpine/.ssh/authorized_keys \\ \t\u0026\u0026 mkdir -p /run/openrc \\ \t\u0026\u0026 touch /run/openrc/softlevel \\ \t\u0026\u0026 rc-update add sshd  Plenty but rather straightforward, let’s break it down:\n update the source packages and install required packages:   openrc because an init system in required openssh and other other packages so there is a minimalistic system that can be accessed and used after launch  1 2  apk update \\ \t\u0026\u0026 apk add openrc openssh sudo util-linux \\   generate host keys:  1  \u0026\u0026 ssh-keygen -A \\   create the home directory structure for the alpine user:  1  \u0026\u0026 mkdir -p /home/alpine/.ssh \\   create the alpine group and the user, assign home directory, init shell and a random password; without the password the user account stays disabled and it’s not possible to SSH as that user:  1 2  \u0026\u0026 addgroup -S alpine \u0026\u0026 adduser -S alpine -G alpine -h /home/alpine -s /bin/sh \\ \t\u0026\u0026 echo \"alpine:$(cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 32 | head -n1)\" | chpasswd \\   make the user a password-less sudoer:  1  \u0026\u0026 echo '%alpine ALL=(ALL) NOPASSWD: ALL' \u003e /etc/sudoers.d/alpine \\   mount special file systems on boot and enable local services:  1 2 3 4 5 6 7  \u0026\u0026 ln -s agetty /etc/init.d/agetty.ttyS0 \\ \t\u0026\u0026 echo ttyS0 \u003e /etc/securetty \\ \t\u0026\u0026 rc-update add agetty.ttyS0 default \\ \t\u0026\u0026 rc-update add devfs boot \\ \t\u0026\u0026 rc-update add procfs boot \\ \t\u0026\u0026 rc-update add sysfs boot \\ \t\u0026\u0026 rc-update add local default   copy the generated public key to authorized keys, there’s a single key so add directly to authorized_keys:  1  COPY ./key.pub /home/alpine/.ssh/authorized_keys  finally, apply settings required to access the system via SSH:   OpenSSH is picky about home and .ssh directory permissions so I make sure these are correct: 0740 for home, 0700 for $HOME/.ssh and 0400 for the keys file enable OpenSSH and make sure it starts when the system starts  1 2 3 4 5 6 7  RUN chown -R alpine:alpine /home/alpine \\ \t\u0026\u0026 chmod 0740 /home/alpine \\ \t\u0026\u0026 chmod 0700 /home/alpine/.ssh \\ \t\u0026\u0026 chmod 0400 /home/alpine/.ssh/authorized_keys \\ \t\u0026\u0026 mkdir -p /run/openrc \\ \t\u0026\u0026 touch /run/openrc/softlevel \\ \t\u0026\u0026 rc-update add sshd  I have a /firecracker directory structure which I described in Taking Firecracker for a spin2. The Dockerfile is saved in /firecracker/docker/alpine-3.13/Dockerfile.\nFile system Now I put together the program to start the container and extract the file system. The program is saved as /firecracker/docker/create-alpine-3.13.sh and goes like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13  #!/bin/bash set -eu build_dir=\"/tmp/alpine-build\" dockerfile=\"/firecracker/docker/alpine-3.13/Dockerfile\" filesystem_target=\"/firecracker/filesystems/alpine-base-root.ext4\" key_file=\"alpine\" image_tag=\"local/alpine-base:latest\" pre_build_dir=$(pwd) echo \"Generating a keypair...\" set +e ssh-keygen -t rsa -b 4096 -C \"alpine@firecracker\" -f \"${HOME}/.ssh/${key_file}\" set -e   First, I’m setting up the build context and generating a key pair. ssh-keygen is smart to check if the key pair already exists and answering no will prevent it from overwriting on every run.\nIn the Dockerfile, I was using a build local key.pub for the image (step 8). Here’s how I make sure it exists:\n1 2 3 4 5  echo \"Creating build directory...\" mkdir -p \"${build_dir}\" \u0026\u0026 cd \"${build_dir}\" echo \"Copying public key to the build directory...\" cp \"${HOME}/.ssh/${key_file}.pub\" \"${build_dir}/key.pub\"   Next, bring the Dockerfile to the build directory and build the Docker image. Tag the image with a known name. If the docker build fails, the program will report that fact and exit.\n1 2 3 4 5 6 7 8 9 10 11  echo \"Building Docker image...\" cp \"${dockerfile}\" \"${build_dir}/Dockerfile\" docker build -t \"${image_tag}\" . retVal=$? cd \"${pre_build_dir}\" rm -r \"${build_dir}\" if [ $retVal -ne 0 ]; then echo \" ==\u003e build failed with status $?\" exit $retVal fi   The next step is to prepare the root file system:\n1 2 3 4 5 6  echo \"Creating file system...\" mkdir -p \"${build_dir}/fsmnt\" dd if=/dev/zero of=\"${build_dir}/rootfs.ext4\" bs=1M count=500 mkfs.ext4 \"${build_dir}/rootfs.ext4\" echo \"Mounting file system...\" sudo mount \"${build_dir}/rootfs.ext4\" \"${build_dir}/fsmnt\"   and start the container:\n1 2  echo \"Starting container from new image ${image_tag}...\" CONTAINER_ID=$(docker run --rm -v ${build_dir}/fsmnt:/export-rootfs -td ${image_tag} /bin/sh)   followed by copying everything out of the container to the file system file. I do it the same way as with the Vault VMM root file system in my previous articles.\nI could combine the first two commands together but I decided to keep them separate to distinguish what belongs to the file system and what’s mine, in this case that’s the /home directory alone:\n1 2 3 4  echo \"Copying Docker file system...\" docker exec ${CONTAINER_ID} /bin/sh -c 'for d in home; do tar c \"/$d\" | tar x -C /export-rootfs; done; exit 0' docker exec ${CONTAINER_ID} /bin/sh -c 'for d in bin dev etc lib root sbin usr; do tar c \"/$d\" | tar x -C /export-rootfs; done; exit 0' docker exec ${CONTAINER_ID} /bin/sh -c 'for dir in proc run sys var; do mkdir /export-rootfs/${dir}; done; exit 0'   When everything is copied, unmount the file system, stop the container and clean up:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  echo \"Unmounting file system...\" sudo umount \"${build_dir}/fsmnt\" echo \"Removing docker container...\" docker stop $CONTAINER_ID echo \"Moving file system...\" mv \"${build_dir}/rootfs.ext4\" \"${filesystem_target}\" echo \"Cleaning up build directory...\" rm -r \"${build_dir}\" echo \"Removing Docker image...\" docker rmi ${image_tag} echo \" \\\\o/ File system written to ${filesystem_target}.\"   To run it simply execute /firecracker/docker/create-alpine-3.13.sh. On my machine, assuming that I already have alpine:3.13 Docker image, the process takes about 20 seconds.\nNetworking The resulting VMM would be useless without access to the outside world. My previous write ups didn’t discuss any of that, none of those VMMs were able reach the internet.\nHere, I’m using the method from Julia’s article - use the docker0 bridge. This is really straightforward. I have the following /firecracker/docker/tap-alpine-3.13.sh program:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  #!/bin/bash set -eu sudo apt-get install bridge-utils -y # create and configure a tap device # to launch firecracker VMM on the docker0 bridge TAP_DEV=alpine-test CONTAINER_IP=172.17.0.42 GATEWAY_IP=172.17.0.1 DOCKER_MASK_LONG=255.255.255.0 sudo ip tuntap add dev \"$TAP_DEV\" mode tap sudo brctl addif docker0 $TAP_DEV sudo ip link set dev \"$TAP_DEV\" up # as Julia Evans, I also need to figure out the meaning of this: sudo sysctl -w net.ipv4.conf.${TAP_DEV}.proxy_arp=1 \u003e /dev/null sudo sysctl -w net.ipv6.conf.${TAP_DEV}.disable_ipv6=1 \u003e /dev/null   The gateway IP and mask come from the docker0 bridge:\n1 2 3 4 5  $ ip addr show docker0 6: docker0: \u003cNO-CARRIER,BROADCAST,MULTICAST,UP\u003e mtu 1500 qdisc noqueue state DOWN group default link/ether 02:42:3c:de:fe:d5 brd ff:ff:ff:ff:ff:ff inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 valid_lft forever preferred_lft forever   The IP address of the VMM is an arbitrary selection.\nRun it with /firecracker/docker/tap-alpine-3.13.sh, the outcome will be similar to:\n1 2 3  $ ip link show alpine-test 13: alpine-test: \u003cNO-CARRIER,BROADCAST,MULTICAST,UP\u003e mtu 1500 qdisc fq_codel master docker0 state DOWN mode DEFAULT group default qlen 1000 link/ether b6:53:d1:78:ee:2d brd ff:ff:ff:ff:ff:ff   Time to configure the VMM.\nVMM configuration file A couple of things to take a note of:\n ip=172.17.0.42::172.17.0.1:255.255.255.0::eth0:off is of the format ip=${VMM_IP}::${GATEWAY_IP}:{DOCKER_MASK_LONG}::${VMM_INTERFACE_ID}:off network-interfaces[0].host_dev_name matches the value of $TAP_DEV  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  cat \u003c\u003cEOF \u003e /firecracker/configs/alpine-config.json { \"boot-source\": { \"kernel_image_path\": \"/firecracker/kernels/vmlinux-v5.8\", \"boot_args\": \"ro console=ttyS0 noapic reboot=k panic=1 pci=off nomodules random.trust_cpu=on ip=172.17.0.42::172.17.0.1:255.255.255.0::eth0:off\" }, \"drives\": [ { \"drive_id\": \"rootfs\", \"path_on_host\": \"/firecracker/filesystems/alpine-base-root.ext4\", \"is_root_device\": true, \"is_read_only\": false } ], \"network-interfaces\": [ { \"iface_id\": \"eth0\", \"guest_mac\": \"02:FC:00:00:00:05\", \"host_dev_name\": \"alpine-test\" } ], \"machine-config\": { \"vcpu_count\": 1, \"mem_size_mib\": 128, \"ht_enabled\": false } } EOF   Run the VMM To start the VMM, simply execute:\n1  sudo firecracker --no-api --config-file /firecracker/configs/alpine-config.json   About two seconds later:\n * Mounting misc binary format filesystem ... [ ok ] * Mounting /sys ... [ ok ] * Mounting security filesystem ... [ ok ] * Mounting debug filesystem ... [ ok ] * Mounting SELinux filesystem ... [ ok ] * Mounting persistent storage (pstore) filesystem ... [ ok ] * Starting local ... [ ok ] Welcome to Alpine Linux 3.13 Kernel 5.8.0 on an x86_64 (ttyS0) 172 login: SSH into the VMM In another terminal:\n1  ssh -i ~/.ssh/alpine alpine@172.17.0.42   The authenticity of host '172.17.0.42 (172.17.0.42)' can't be established. ECDSA key fingerprint is SHA256:gYxEJdQIXM3242/yV/RV9qVQBaGSdLoUtpFSmBKEyHE. Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added '172.17.0.42' (ECDSA) to the list of known hosts. Enter passphrase for key '/home/radek/.ssh/alpine': Welcome to Alpine! The Alpine Wiki contains a large amount of how-to guides and general information about administrating Alpine systems. See \u003chttp://wiki.alpinelinux.org/\u003e. You can setup the system with the command: setup-alpine You may change this message by editing /etc/motd. 1 2  172:~$ sudo sh 172:/home/alpine# apk update   fetch https://dl-cdn.alpinelinux.org/alpine/v3.13/main/x86_64/APKINDEX.tar.gz fetch https://dl-cdn.alpinelinux.org/alpine/v3.13/community/x86_64/APKINDEX.tar.gz v3.13.1-115-gf65775dfbc [https://dl-cdn.alpinelinux.org/alpine/v3.13/main] v3.13.1-117-g6a5e33f63c [https://dl-cdn.alpinelinux.org/alpine/v3.13/community] OK: 13880 distinct packages available 1  172:/home/alpine# ping 1.1.1.1   PING 1.1.1.1 (1.1.1.1): 56 data bytes 64 bytes from 1.1.1.1: seq=0 ttl=58 time=18.995 ms 64 bytes from 1.1.1.1: seq=1 ttl=58 time=15.660 ms 64 bytes from 1.1.1.1: seq=2 ttl=58 time=16.246 ms 64 bytes from 1.1.1.1: seq=3 ttl=58 time=17.889 ms ^C --- 1.1.1.1 ping statistics --- 4 packets transmitted, 4 packets received, 0% packet loss round-trip min/avg/max = 15.660/17.197/18.995 ms 1  172:/home/alpine# ip link   1: lo: \u003cLOOPBACK,UP,LOWER_UP\u003e mtu 65536 qdisc noqueue state UNKNOWN qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 2: eth0: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 02:fc:00:00:00:05 brd ff:ff:ff:ff:ff:ff 1 2 3 4 5 6 7 8 9 10 11 12 13  172:/home/alpine# ip addr 1: lo: \u003cLOOPBACK,UP,LOWER_UP\u003e mtu 65536 qdisc noqueue state UNKNOWN qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: eth0: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 02:fc:00:00:00:05 brd ff:ff:ff:ff:ff:ff inet 172.17.0.42/24 brd 172.17.0.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::fc:ff:fe00:5/64 scope link valid_lft forever preferred_lft forever   Nice. Everything is working as expected. The UX is not fully complete, to ping stuff I do have to sudo. Whatever, if I can ping the BBC, I’m good:\n1  172:/home/alpine# ping bbc.co.uk   PING bbc.co.uk (151.101.64.81): 56 data bytes 64 bytes from 151.101.64.81: seq=0 ttl=58 time=23.371 ms 64 bytes from 151.101.64.81: seq=1 ttl=58 time=20.238 ms 64 bytes from 151.101.64.81: seq=2 ttl=58 time=24.788 ms 64 bytes from 151.101.64.81: seq=3 ttl=58 time=24.047 ms ^C --- bbc.co.uk ping statistics --- 4 packets transmitted, 4 packets received, 0% packet loss round-trip min/avg/max = 20.238/23.111/24.788 ms Next steps Next time I am going to look at setting up the network with IPAM so the IP addresses are assigned from a given range.\nThat’s it for today:\n1  172:/home/alpine# reboot   172:/home/alpine# Connection to 172.17.0.42 closed by remote host. Connection to 172.17.0.42 closed.   Day 41: Trying to understand what a bridge is ↩︎\n Taking Firecracker for a spin ↩︎\n   ","description":"One step back, two steps forward; create a base image to investigate Firecracker further","tags":["firecracker","microvm","alpine","docker"],"title":"Launching Alpine Linux on Firecracker like a boss","uri":"/posts/2021-02-13-launching-alpine-linux-on-firecracker-like-a-boss/"},{"content":"So I’ve been on the fence with the vendor directory.\nOn one hand, it’s great to have the modules in the project because it speeds up the build and serves as a safe storage.\nOn the other hand, it does increase the churn in the repository and creates a lot of duplication on disk because many projects often contain the same dependencies.\nSince I do like holding on to my dependencies and go mod works great for me, I’ve decided to try out The Athens. I’s awesome.\nWhat is it It is a self-hosted golang module mirror service. Unlike the public proxy.golang.org, it can cache private modules so no more GOPRIVATE, yay!\nAthens offers a selection of storage back ends: local disk, S3, Minio, GCS and the like. One instance can be pointed at another with upstream services. The documentation is very approachable.\nHow I run it KISS, Docker service backed with the disk type storage. Basically like this:\n1 2 3 4 5 6 7  mkdir -p /var/lib/gomod docker run --rm \\  -p 15001:3000 \\  -v /var/lib/gomod:/gomod \\  -e ATHENS_STORAGE_TYPE=disk \\  -e ATHENS_DISK_STORAGE_ROOT=/gomod \\  -d gomods/athens:latest   I’ve exported the GOPROXY=http://localhost:15001 in my Bash profile so tools use it automatically. I have a dedicated git repository where I sync my local modules to a couple of time a day.\nLong term, maybe something like the S3 storage would be a better option but I’m not sure how expensive it can get.\n","description":"Historical place with great weather and fantastic cuisine, no wonder…","tags":["golang"],"title":"My golang modules live in Athens","uri":"/posts/2021-02-10-my-golang-modules-live-in-athens/"},{"content":"It’s good to know how to set up Firecracker VM by hand but that’s definitely suboptimal long term. So today I am looking at setting up Firecracker with CNI plugins. Firecracker needs four CNI plugins to operate: ptp, firewall, host-local and tc-redirect-tap. First three come from the CNI plugins1 repository, the last one comes from AWS Labs tc-redirect-tap2 repository.\nGolang CNI plugins and tc-redirect-tap require golang to build. I’m using 1.15.8.\nCNI plugins 1 2 3 4  mkdir ~/cni-plugins cd ~/cni-plugins git clone https://github.com/containernetworking/plugins.git . ./build_linux.sh   After about 30 seconds, the are under the bin/ directory:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  $ tree bin bin ├── bandwidth ├── bridge ├── dhcp ├── firewall ├── flannel ├── host-device ├── host-local ├── ipvlan ├── loopback ├── macvlan ├── portmap ├── ptp ├── sbr ├── static ├── tuning ├── vlan └── vrf   tc-redirect-tap 1 2 3 4  mkdir ~/tc-redirect-tap cd ~/tc-redirect-tap git clone https://github.com/awslabs/tc-redirect-tap.git . make all   The binary can be found in the root of the sources directory.\nInstalling the plugins CNI plugins are sought from the /opt/cni/bin directory. Some tools allow overriding that path but there is no consistency so the default directory is the safest choice. However, to keep everything tidy, I will place by plugins in the /firecracker/cni/bin directory, per the structure from Taking Firecracker for a spin3:\n1 2 3  mkdir -p /firecracker/cni/bin cp ~/cni-plugins/bin/* /firecracker/cni/bin/ cp ~/tc-redirect-tap/tc-redirect-tap /firecracker/cni/bin/tc-redirect-tap   then link them to where they are expected to be:\n1 2  sudo mkdir -p /opt/cni sudo ln -sfn /firecracker/cni/bin /opt/cni/bin   Prepare Nomad and task driver I’ll use HashiCorp Nomad with the firecracker-task.driver. First, get Nomad:\n1 2 3 4 5  cd /tmp wget https://releases.hashicorp.com/nomad/1.0.3/nomad_1.0.3_linux_amd64.zip unzip nomad_1.0.3_linux_amd64.zip sudo mv nomad /usr/bin/nomad rm nomad_1.0.3_linux_amd64.zip   Second, get the task driver sources and build them:\n1 2 3 4  mkdir ~/firecracker-task-driver cd ~/firecracker-task-driver git clone https://github.com/cneira/firecracker-task-driver.git . go build -mod=mod -o ./firecracker-task-driver ./main.go   Default Nomad plugins directory is /opt/nomad/plugins:\n1 2  sudo mkdir -p /opt/nomad/plugins sudo mv firecracker-task-driver /opt/nomad/plugins/firecracker-task-driver   Create the network definition These are to be placed under /etc/cni/conf.d. Again, to keep it tidy and in one place:\n1 2 3  mkdir /firecracker/cni/conf.d sudo mkdir -p /etc/cni sudo ln -sfn /firecracker/cni/conf.d /etc/cni/conf.d   Create the network definition file:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  cat \u003c\u003cEOF \u003e /firecracker/cni/conf.d/vault.conflist { \"name\": \"vault\", \"cniVersion\": \"0.4.0\", \"plugins\": [ { \"type\": \"ptp\", \"ipMasq\": true, \"ipam\": { \"type\": \"host-local\", \"subnet\": \"192.168.127.0/24\", \"resolvConf\": \"/etc/resolv.conf\" } }, { \"type\": \"firewall\" }, { \"type\": \"tc-redirect-tap\" } ] } EOF   Start Nomad in dev mode Create the Nomad configuration directory:\n1  sudo mkdir /etc/nomad   We have to create this minimalistic server configuration to tell Nomad where our plugins are (plugins directory under data_dir):\n1 2 3 4  cat \u003c\u003cEOF | sudo tee -a /etc/nomad/server.conf data_dir = \"/opt/nomad\" bind_addr = \"0.0.0.0\" # the default EOF   And we can start Nomad development agent:\n1  sudo nomad agent -dev -config=/etc/nomad/server.conf   Create the Nomad task Create the directory for Nomad jobs:\n1  sudo mkdir -p /etc/nomad/jobs   And write the job definition. The process of getting the kernel and the root image is described in the previous post3.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  cat \u003c\u003cEOF | sudo tee -a /etc/nomad/jobs/vault.nomad job \"vault-with-cni\" { datacenters = [\"dc1\"] type = \"service\" group \"vault-test\" { restart { attempts = 0 mode = \"fail\" } task \"vault1\" { driver = \"firecracker-task-driver\" config { BootDisk = \"/firecracker/filesystems/vault-root.ext4\" Firecracker = \"/usr/bin/firecracker\" KernelImage = \"/firecracker/kernels/vmlinux-v5.8\" Mem = 128 Network = \"vault\" Vcpus = 1 } } } } EOF   Test the job configuration:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  $ sudo nomad job plan /etc/nomad/jobs/vault.nomad + Job: \"vault-with-cni\" + Task Group: \"vault-test\" (1 create) + Task: \"vault1\" (forces create) Scheduler dry-run: - All tasks successfully allocated. Job Modify Index: 0 To submit the job with version verification run: nomad job run -check-index 0 /etc/nomad/jobs/vault.nomad When running the job with the check-index flag, the job will only be run if the job modify index given matches the server-side version. If the index has changed, another user has modified the job and the plan's results are potentially invalid.   Okay, looks good, let’s run it:\n1 2 3 4 5 6 7 8  $ sudo nomad job run /etc/nomad/jobs/vault.nomad ==\u003e Monitoring evaluation \"2e42b090\" Evaluation triggered by job \"vault-with-cni\" ==\u003e Monitoring evaluation \"2e42b090\" Evaluation within deployment: \"d12624cb\" Allocation \"a57d68ec\" created: node \"10f89343\", group \"vault-test\" Evaluation status changed: \"pending\" -\u003e \"complete\" ==\u003e Evaluation \"2e42b090\" finished with status \"complete\"   Awesome, what does the status say?\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  $ sudo nomad status vault ID = vault-with-cni Name = vault-with-cni Submit Date = 2021-02-07T13:49:20Z Type = service Priority = 50 Datacenters = dc1 Namespace = default Status = running Periodic = false Parameterized = false Summary Task Group Queued Starting Running Failed Complete Lost vault-test 0 0 1 0 0 0 Latest Deployment ID = d12624cb Status = running Description = Deployment is running Deployed Task Group Desired Placed Healthy Unhealthy Progress Deadline vault-test 1 1 0 0 2021-02-07T13:59:20Z Allocations ID Node ID Task Group Version Desired Status Created Modified a57d68ec 10f89343 vault-test 0 run running 7s ago 6s ago   Sweet. Let’s have a look at the veth device:\n1 2 3  $ ip -c link show type veth 7: veth200fa5e4@if3: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc noqueue state UP mode DEFAULT group default link/ether 42:ee:02:f4:98:3a brd ff:ff:ff:ff:ff:ff link-netnsid 0   Can we talk to it? 1  curl http://192.168.127.2:8200/sys/health   1  {\"errors\":[]}   Yep, it running!\nCaveats  Stopping the job does not remove the veth interface so a manual cleanup of the unused interfaces is needed. Subsequent runs give the task the next IP address. If 192.168.127.2 does not work for you, try .1 .3, .4 and so on… Something to look into in detail a little bit later.    CNI plugins GitHub repository ↩︎\n AWS Labs tcp-redirect-tap GitHub repository ↩︎\n Taking Firecracker for a spin ↩︎\n   ","description":"Setting up Vault on Firecracker with CNI network on HashiCorp Nomad","tags":["firecracker","microvm","cni","nomad","vault"],"title":"Vault on Firecracker with CNI plugins and Nomad","uri":"/posts/2021-02-07-vault-on-firecracker-with-cni-plugins-and-nomad/"},{"content":"Firecracker1 is recently making rounds on the internet as this relatively new, awesome technology for running lightweight VMs.\nAs something coming from AWS and powering AWS Lambda, my original perception was that it’s not easy to set up and use. However, this write from Julia Evans2 proved me wrong. So, as I have recently picked up a used Dell R720 with decent amount of RAM and CPUs, it was time to take these two for a spin together.\nSipping the first coffee this gloomy Nürburgring weather morning, the thought of putting Vault on Firecracker seemed somewhat amusing, … and the day was gone.\nI’m going to show you how I’ve done it. Do I like Firecracker? It’s been only one day but yes, it’s pretty neat and easy to use.\nEnvironment Clean HWE Ubuntu 18.04.5 Server installation with the password-less sudoer user.\nIntro A Firecracker VM requires a Linux Linux Kernel and a root file system. Most of the people who talk about Firecracker use example hello-vmlinux and a root file system from AWS. I wanted to learn how to build these myself so I went with building my own 5.8 kernel and a root file system extracted from the default official HashiCorp Vault Docker image. The steps:\n install all the software required to build Linux kernel and install Docker setup directory structure to work with install Firecracker get, configure and build Linux kernel extract Vault file system from a running container run Vault Firecracker microVM  Dependencies To avoid multiple apt-get updates, add Docker repository first:\n1 2 3 4 5  curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo add-apt-repository \\  \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs)\\ stable\"   Next, install all the software required to compile the kernel:\n1 2 3 4 5 6 7 8 9  sudo apt-get update sudo apt-get install \\  bison \\  build-essential \\  flex \\  git \\  libelf-dev \\  libncurses5-dev \\  libssl-dev -y   followed by Docker dependencies:\n1 2 3 4 5 6  sudo apt-get install \\  apt-transport-https \\  ca-certificates \\  curl \\  gnupg-agent \\  software-properties-common -y   and finally, Docker itself:\n1 2 3 4 5 6  sudo apt-get install \\  docker-ce \\  docker-ce-cli \\  containerd.io -y sudo groupadd docker # this may report that the group already exists sudo usermod -aG docker $USER   Setup the directory structure To have everything in one place, create the following directories:\n1 2 3  sudo mkdir -p /firecracker sudo chown -R ${USER}:${USER} /firecracker mkdir -p /firecracker/{configs,filesystems,kernels,linux.git,releases}    configs: this is where we will put the VM config JSON files filesystems: this is where the extracted file systems will reside kernels: this directory contains pre-built Linux kernels linux.git: Linux sources go here releases: Firecracker releases will be installed here  Install Firecracker This is as simple as downloading a pre-built binary release from GitHub and putting it on the PATH. I wanted to have versioning available for ease of upgrading in the future so I’ve built a shell program to manage this for me. You can find the program in this repository on GitHub3. Long story, short:\n download install-firecracker.sh program and put it in /firecracker directory; technically does not matter where but the program assumes the /firecracker/... directory structure from the previous step chmod +x /firecracker/install-firecracker.sh run: sudo /firecracker/install-firecracker.sh  This will download the latest Firecracker release, install the release in /firecracker/releases directory and create /usr/bin/firecracker-\u003cversion\u003e and /usr/bin/jailer-\u003cversion\u003e links. You’ll also get /usr/bin/firecracker and /usr/bin/jailer links pointing to the version links. You can now run Firecracker:\n1  firecracker --help   Get Linux Kernel I’m going to use 5.8 kernel. Do I need one? Not sure but why not.\n1  export KERNEL_VERSION=v5.8   Clone sources from GitHub:\n1 2 3  cd /firecracker/linux.git git clone https://github.com/torvalds/linux.git . git checkout ${KERNEL_VERSION}   Configure the kernel So, I’m not really fluent at this and…\nThis is the first time I’m building the kernel but fortunately one of the the Firecracker getting started documents points to a recommended kernel config. What’s less fortunate, the document talks about kernel v4.20 and the config is for v4.14.174. I’ve downloaded the file and placed it in /firecracker/linux.git/.config anyway but when I tried building the v5.8 kernel, make was insisting on recreating the config and asked me a lot of questions about what I want.\nI don’t know what I want and I don’t know if make took the values from the old .config so I basically held Enter down for a bit. make moved my .config to .config.old and gave me a new .config file for v5.8 kernel. I took the new generated file and compared it with the original v4.14.174 config.\nThese files have few thousand lines so I wrote a program in Golang which loads both versions and compares the values.\nWith a flag, it allows bringing non-existing values from the good config to the new one. You can find this program in the kernel-configs directory of this repository[]. To get the v5.8 config, I basically executed:\n1 2 3 4  go run ./compare-configs.go \\  --good-config=./4.14.174.config \\  --new-config=./5.8.config \\  --bring-old-non-existing   Next, I’ve replaced the /firecracker/linux.git/.config file contents with the result of my compare configs program and re-run kernel build. This time, it did not insist on recreating anything and used my config, definitely. Happy times!\nBuild the kernel You probably want to change the value of -j to something like 4 or 8. The build will take a bit more time.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  time make vmlinux -j32 ... LD vmlinux.o MODPOST vmlinux.symvers MODINFO modules.builtin.modinfo GEN modules.builtin LD .tmp_vmlinux.kallsyms1 KSYM .tmp_vmlinux.kallsyms1.o LD .tmp_vmlinux.kallsyms2 KSYM .tmp_vmlinux.kallsyms2.o LD vmlinux SORTTAB vmlinux SYSMAP System.map real\t0m54.052s user\t23m51.313s sys\t2m35.287s   After the build is complete, copy the vmlinux binary to the /firecracker/kernels directory:\n1  mv ./vmlinux /firecracker/kernels/vmlinux-${KERNEL_VERSION}   Build the file system Fun part starts here. The article from Julia Evans talks about getting an init system installed in the container. I have naively tried simply extracting the file system and running it bluntly without doing any additional configuration but Firecracker was complaining about not having /sbin/openrc available. Makes sense, Docker images generally don’t need an init system. So I had to find a method to get one in.\nFortunately, Vault Docker image is built from Alpine Linux and the Creating a rootfs Image instructions show how to add an init system to a file system of an Alpine based container.\nWe’ve building a file system for Vault:\n1  export FS=vault   Create a file system file and format it as ext4:\n1 2 3  rm /firecracker/filesystems/vault-root.ext4 dd if=/dev/zero of=/firecracker/filesystems/vault-root.ext4 bs=1M count=500 mkfs.ext4 /firecracker/filesystems/vault-root.ext4   I have no idea how much space I needed. 50 megs wasn’t enough so I gave it 500 instead.\nCreate a mount directory and mount the file system file:\n1 2 3  mkdir -p /firecracker/filesystems/mnt-${FS} sudo mount /firecracker/filesystems/${FS}-root.ext4 \\  /firecracker/filesystems/mnt-${FS}   Now, run the Vault container and fetch the container ID, attach file system mount directory to the container:\n1  export CONTAINER_ID=$(docker run -t --rm -v /firecracker/filesystems/mnt-${FS}:/export-rootfs -d vault:latest)   Get the shell in the container:\n1  docker exec -ti ${CONTAINER_ID} /bin/sh   Now, in the container shell, execute these commands:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34  # install the init system and some extra tools: apk add openrc apk add util-linux # set up a login terminal on the serial console (ttyS0): ln -s agetty /etc/init.d/agetty.ttyS0 echo ttyS0 \u003e /etc/securetty rc-update add agetty.ttyS0 default # Make sure special file systems are mounted on boot: rc-update add devfs boot rc-update add procfs boot rc-update add sysfs boot # EXTRA: I had to add these lines: # -------------------------------- # 1. enable local services: # https://wiki.gentoo.org/wiki//etc/local.d rc-update add local default # 2. create a local service to Start Vault dev server on system boot: echo \"#!/bin/sh\" \u003e\u003e /etc/local.d/HashiCorpVault.start echo \"/usr/local/bin/docker-entrypoint.sh server -dev \u0026\u0026 reboot || reboot\" \u003e\u003e /etc/local.d/HashiCorpVault.start # 3. make it executable: chmod +x /etc/local.d/HashiCorpVault.start # 4. For convenience, enable output from local service so I can see errors: echo rc_verbose=yes \u003e /etc/conf.d/local # 5. make sure I also have /home and /vault directories in my exported file system for d in home vault; do tar c \"/$d\" | tar x -C /export-rootfs; done # EXTRA / end # Then, copy the newly configured system to the rootfs image: for d in bin etc lib root sbin usr; do tar c \"/$d\" | tar x -C /export-rootfs; done for dir in dev proc run sys var; do mkdir /export-rootfs/${dir}; done # All done, exit docker shell exit   A few words about the HashiCorpVault.start service file. One method to shut the Firecracker VM gracefully down is to call reboot from inside of the VM. This is because Firecracker exits on CPU reset, more info here. Hence the command:\n1  /usr/local/bin/docker-entrypoint.sh server -dev \u0026\u0026 reboot || reboot   will start the Vault server using the regular docker-entrypoint.sh from the original image.\nThe \u0026\u0026 reboot part will ensure the VM stops automatically after Vault exits gracefully. The || reboot part will stop the VM if Vault does not start for whatever reason.\nThis saves a hassle of doing ps -a and sudo kill \u003cpid\u003e dance when things go south.\nYou can now stop the container and unmount the file system:\n1 2  docker stop ${CONTAINER_ID} sudo umount /firecracker/filesystems/mnt-${FS}   Launch Vault on Firecracker Firecracker VMs can, by design, only use Linux tap devices for networking. There are tools that create ad-hoc devices using CNI plugins but I went with the method from Julia Evans. So, this part is directly lifted from Julia Evans.\nPrepare kernel boot args:\n1 2 3 4 5  export MASK_LONG=\"255.255.255.252\" export FC_IP=\"169.254.0.21\" export TAP_IP=\"169.254.0.22\" export KERNEL_BOOT_ARGS=\"ro console=ttyS0 noapic reboot=k panic=1 pci=off nomodules random.trust_cpu=on\" export KERNEL_BOOT_ARGS=\"${KERNEL_BOOT_ARGS}ip=${FC_IP}::${TAP_IP}:${MASK_LONG}::eth0:off\"   Setup tap network interface:\n1 2 3 4 5 6 7 8 9 10  export TAP_DEV=\"fc-88-tap0\" export MASK_SHORT=\"/30\" export FC_MAC=\"02:FC:00:00:00:05\" sudo ip link del \"$TAP_DEV\" 2\u003e /dev/null || true sudo ip tuntap add dev \"$TAP_DEV\" mode tap sudo sysctl -w net.ipv4.conf.${TAP_DEV}.proxy_arp=1 \u003e /dev/null sudo sysctl -w net.ipv6.conf.${TAP_DEV}.disable_ipv6=1 \u003e /dev/null sudo ip addr add \"${TAP_IP}${MASK_SHORT}\" dev \"$TAP_DEV\" sudo ip link set dev \"$TAP_DEV\" up   Write the VM config:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  cat \u003c\u003cEOF \u003e /firecracker/configs/vault-config.json { \"boot-source\": { \"kernel_image_path\": \"/firecracker/kernels/vmlinux-${KERNEL_VERSION}\", \"boot_args\": \"$KERNEL_BOOT_ARGS\" }, \"drives\": [ { \"drive_id\": \"rootfs\", \"path_on_host\": \"/firecracker/filesystems/${FS}-root.ext4\", \"is_root_device\": true, \"is_read_only\": false } ], \"network-interfaces\": [ { \"iface_id\": \"eth0\", \"guest_mac\": \"${FC_MAC}\", \"host_dev_name\": \"${TAP_DEV}\" } ], \"machine-config\": { \"vcpu_count\": 1, \"mem_size_mib\": 128, \"ht_enabled\": false } } EOF   And launch:\n1  firecracker --no-api --config-file /firecracker/configs/vault-config.json   The result will be similar to:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47  [ 0.000000] Linux version 5.8.0 (radek@r720sas) (gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0, GNU ld (GNU Binutils for Ubuntu) 2.30) #4 SMP Sat Feb 6 18:50:30 UTC 2021 [ 0.000000] Command line: ro console=ttyS0 noapic reboot=k panic=1 pci=off nomodules random.trust_cpu=on ip=169.254.0.21::169.254.0.22:255.255.255.252::eth0:off root=/dev/vda rw virtio_mmio.device=4K@0xd0000000:5 virtio_mmio.device=4K@0xd0001000:6 [ 0.000000] x86/fpu: Supporting XSAVE feature 0x001: 'x87 floating point registers' [ 0.000000] x86/fpu: Supporting XSAVE feature 0x002: 'SSE registers' [ 0.000000] x86/fpu: Supporting XSAVE feature 0x004: 'AVX registers' [ 0.000000] x86/fpu: xstate_offset[2]: 576, xstate_sizes[2]: 256 [ 0.000000] x86/fpu: Enabled xstate features 0x7, context size is 832 bytes, using 'standard' format. [ 0.000000] BIOS-provided physical RAM map: [ 0.000000] BIOS-e820: [mem 0x0000000000000000-0x000000000009fbff] usable [ 0.000000] BIOS-e820: [mem 0x0000000000100000-0x0000000007ffffff] usable ... * Mounting persistent storage (pstore) filesystem ... [ ok ] * Starting local ... * Executing \"/etc/local.d/HashiCorpVault.start\" ...==\u003e Vault server configuration: Api Address: http://0.0.0.0:8200 Cgo: disabled Cluster Address: https://0.0.0.0:8201 Go Version: go1.15.7 Listener 1: tcp (addr: \"0.0.0.0:8200\", cluster address: \"0.0.0.0:8201\", max_request_duration: \"1m30s\", max_request_size: \"33554432\", tls: \"disabled\") Log Level: info Mlock: supported: true, enabled: false Recovery Mode: false Storage: inmem Version: Vault v1.6.2 Version Sha: be65a227ef2e80f8588b3b13584b5c0d9238c1d7 ==\u003e Vault server started! Log data will stream in below: 2021-02-06T21:13:22.006Z [INFO] proxy environment: http_proxy= https_proxy= no_proxy= 2021-02-06T21:13:22.007Z [WARN] no `api_addr` value specified in config or in VAULT_API_ADDR; falling back to detection if possible, but this value should be manually set 2021-02-06T21:13:22.020Z [INFO] core: security barrier not initialized 2021-02-06T21:13:22.020Z [INFO] core: security barrier initialized: stored=1 shares=1 threshold=1 ... and starts unsealed with a single unseal key. The root token is already authenticated to the CLI, so you can immediately begin using Vault. You may need to set the following environment variable: $ export VAULT_ADDR='http://0.0.0.0:8200' The unseal key and root token are displayed below in case you want to seal/unseal the Vault or re-authenticate. Unseal Key: YqKuQlzPiMUQXphehp0M7DAvsqImqNJrvJqAn/R0nyc= Root Token: s.F4erraVTvHnx3oU4Ac8zwaCP Development mode should NOT be used in production installations!   Try it out by opening another terminal on the same machine and running:\n1 2  curl http://169.254.0.21:8200/sys/health {\"errors\":[]}   It’s alive.\nYou can stop the VM by simply pressing CTRL+C in the terminal window where Vault is running. The VM will shut gracefully down:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  ^C==\u003e Vault shutdown triggered 2021-02-06T21:13:33.863Z [INFO] core: marked as sealed 2021-02-06T21:13:33.866Z [INFO] core: pre-seal teardown starting 2021-02-06T21:13:33.869Z [INFO] rollback: stopping rollback manager 2021-02-06T21:13:33.872Z [INFO] core: pre-seal teardown complete 2021-02-06T21:13:33.873Z [INFO] core: stopping cluster listeners 2021-02-06T21:13:33.874Z [INFO] core.cluster-listener: forwarding rpc listeners stopped 2021-02-06T21:13:34.076Z [INFO] core.cluster-listener: rpc listeners successfully shut down 2021-02-06T21:13:34.082Z [INFO] core: cluster listeners successfully shut down 2021-02-06T21:13:34.085Z [INFO] core: vault is sealed [ ok ] [ ok ] * Stopping local ... [ ok ] The system is going down NOW! Sent SIGTERM to all processes Sent SIGKILL to all processes Requesting system reboot [ 15.877742] Unregister pv shared memory for cpu 0 [ 15.880260] reboot: Restarting system [ 15.881627] reboot: machine restart radek@r720:/firecracker$   That’s all for today.\n  Firecracker on GitHub ↩︎\n Firecracker: start a VM in less than a second ↩︎\n Complementary GitHub repository ↩︎\n   ","description":"Doing semi-meaningful things with Firecracker","tags":["firecracker","microvm","vault"],"title":"Taking Firecracker for a spin","uri":"/posts/2021-02-06-taking-firecracker-for-a-spin/"},{"content":"This is a clarification to the previous write up about Keycloak Authorization Services1. The documentation of the response_mode documents the two values which can be used: decision and permissions. In the first Keycloak article2, I have wrongly assumed that no response_mode in the grant_type=urn:ietf:params:oauth:grant-type:uma-ticket call implies the value of permissions.\nMmm, that was a wrong assumption.\nNow, looking at the documentation and trying it out, the distinction seems pretty obvious. It turns out there are three types of responses for this grant_type.\n Asking for RPT (requesting party token - an access token with permissions): without response_mode parameter  1 2 3 4 5 6  curl --silent -X POST \\  ${KEYCLOAK_TOKEN_URL} \\  -H \"Authorization: Bearer ${access_token}\" \\  --data \"grant_type=urn:ietf:params:oauth:grant-type:uma-ticket\" \\  --data \"audience=customers\" \\  --data \"permission=CustomerB#customer-b\" | jq '.'   where the ${access_token} is the outcome of:\n1 2 3 4  export access_token=`curl --silent -u customers:${KEYCLOAK_CLIENT_SECRET} \\  -k -d \"grant_type=password\u0026username=member@service-team\u0026password=${USER_PASSWORD}\u0026scope=email profile\" \\  -H \"Content-Type:application/x-www-form-urlencoded\" \\  ${KEYCLOAK_TOKEN_URL} | jq '.access_token' -r`   the response looks like this, the access_token is the RPT:\n1 2 3 4 5 6 7 8 9  { \"upgraded\": false, \"access_token\": \"eyJhbGciOiJSUzI1NiIsI...n8AC51T1AMwDtoqfCEXrdwcrQ\", \"expires_in\": 300, \"refresh_expires_in\": 1800, \"refresh_token\": \"eyJhbGciOiJIUz...RG3zFus\", \"token_type\": \"Bearer\", \"not-before-policy\": 0 }   Asking for permissions - decision only: response_mode=decision  By adding:\n--data \"response_mode=decision\" the full call is:\n1 2 3 4 5 6 7  curl --silent -X POST \\  ${KEYCLOAK_TOKEN_URL} \\  -H \"Authorization: Bearer ${access_token}\" \\  --data \"grant_type=urn:ietf:params:oauth:grant-type:uma-ticket\" \\  --data \"audience=customers\" \\  --data \"response_mode=decision\" \\  --data \"permission=CustomerB#customer-b\" | jq '.'   and returns only a decision. If user has access to the resources, the response is:\n1 2 3  { \"result\": true }   otherwise, the response is:\n1 2 3 4  { \"error\": \"access_denied\", \"error_description\": \"not_authorized\" }   Asking for permissions: response_mode=permissions  By specifying\n--data \"response_mode=permissions\" full call being:\n1 2 3 4 5 6 7  curl --silent -X POST \\  ${KEYCLOAK_TOKEN_URL} \\  -H \"Authorization: Bearer ${access_token}\" \\  --data \"grant_type=urn:ietf:params:oauth:grant-type:uma-ticket\" \\  --data \"audience=customers\" \\  --data \"response_mode=permissions\" \\  --data \"permission=CustomerB#customer-b\" | jq '.'   Keycloak answers:\n1 2 3 4 5 6 7 8 9  [ { \"scopes\": [ \"customer-b\" ], \"rsid\": \"00f34b81-c45b-4e28-b267-45fad4e48b4d\", \"rsname\": \"CustomerB\" } ]   However, what is more interesting, is the call without specific permissions listed:\n1 2 3 4 5 6  curl --silent -X POST \\  ${KEYCLOAK_TOKEN_URL} \\  -H \"Authorization: Bearer ${access_token}\" \\  --data \"grant_type=urn:ietf:params:oauth:grant-type:uma-ticket\" \\  --data \"audience=customers\" \\  --data \"response_mode=permissions\" | jq '.'   which returns all available permissions for the original access token:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  [ { \"scopes\": [ \"customer-a\" ], \"rsid\": \"715f6cc5-8ca7-44e4-a8ce-924493db76b1\", \"rsname\": \"CustomerA\" }, { \"scopes\": [ \"customer-b\" ], \"rsid\": \"00f34b81-c45b-4e28-b267-45fad4e48b4d\", \"rsname\": \"CustomerB\" } ]   Listing permissions without the name Optionally, we can ask Keycloak to not return resource names, only IDs. This is achieved by using response_include_resource_name=false, an example:\n1 2 3 4 5 6 7  curl --silent -X POST \\  ${KEYCLOAK_TOKEN_URL} \\  -H \"Authorization: Bearer ${access_token}\" \\  --data \"grant_type=urn:ietf:params:oauth:grant-type:uma-ticket\" \\  --data \"audience=customers\" \\  --data \"response_mode=permissions\" \\  --data \"response_include_resource_name=false\" | jq '.'   gives:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  [ { \"scopes\": [ \"customer-a\" ], \"rsid\": \"715f6cc5-8ca7-44e4-a8ce-924493db76b1\" }, { \"scopes\": [ \"customer-b\" ], \"rsid\": \"00f34b81-c45b-4e28-b267-45fad4e48b4d\" } ]     Keycloak Authorization Services - retrieving the decision only ↩︎\n Introduction to Keycloak Authorization Services ↩︎\n   ","description":"Keycloak Authorization Services - RPT, permissions or a decision only","tags":["keycloak","iam","uma"],"title":"Keycloak Authorization Services - RPT, permissions or a decision only","uri":"/posts/2020-09-16-keycloak-authorization-services-rpt-permissions-or-a-decision-only/"},{"content":"This always gets me, npm publish fails to authenticate:\nnpm ERR! code E401 npm ERR! Unable to authenticate, need: Basic realm=\"Artifactory Realm\" npm ERR! A complete log of this run can be found in: npm ERR! /Users/rad/.npm/_logs/...Z-debug.log The solution is:\n Sign-in to JFrog. Find Edit profile under the Welcome, ... menu. Put JFrog password in and unlock. Copy the encrypted password. Issue a curl request like this:  1  curl -u ${JFROG_USER}:${JFROG_ENCRYPTED_PASSWORD} https://${JFROG_ORG}.jfrog.io/${JFROG_ORG}/api/npm/auth   Copy the output and put it in ~/.npmrc. The file should be like:  1 2 3 4  _auth=\"cm...M=\" always-auth=true email=email@address registry=https://${JFROG_ORG}.jfrog.io/${JFROG_ORG}/api/npm/${JFROG_REPO}/   ","description":"Authenticate to private JFrog npm registry","tags":["npm","jfrog"],"title":"Authenticate to private JFrog npm registry","uri":"/posts/2020-09-09-authenticate-to-private-jfrog-npm-registry/"},{"content":"In the previous article1, I have investigated modern PKI software alternatives. One of the options on the list was HashiCorp Vault. The natural next step is to set up a Vault PKI.\nThis article documents setting up an imaginary multi-tenant Vault PKI with custom PEM bundles generated with OpenSSL. The steps the following:\n create a root CA with OpenSSL create intermediate CAs for imaginary clients with OpenSSL using HashiCorp Vault in development mode:  import custom bundle with root and intermediate certificates configure Vault roles issue a certificate    The method for generating the root and intermediate CAs comes from OpenSSL Certificate Authority guide written by Jamie Nguyen2. I’m including the scripts and the configuration in this article for reference.\nThe result As an outcome of this article, the reader will be able to create a new root CA and add new intermediate CAs to existing root CAs with a single command. This command will also prepare the Vault PEM bundle. With the Vault bundle and four more shell commands, the user will have a ready to use certificate with a CA chain. The process will be:\n run init-intermediate.sh \u003cca-name\u003e \u003cclient-id\u003e to have an intermediate CA enable PKI for the new client with vault shell command import a PEM bundle with vault shell command add a permission with vault shell command issue a certificate with vault shell command  Create a root CA I’m going to start by creating an environment file which will contain a location for the CA and distinguished name settings. As the root of the CA, I’m going to use ~/.ca directory.\n1  mkdir -p ~/.ca   Now, I will create the environment file, ~/.ca/.article-ca:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  tee ~/.ca/.article-ca \u003e/dev/null \u003c\u003cEOF # DN defaults: export CA_DN_DEFAULT_COUNTRY_CODE=DE export CA_DN_DEFAULT_STATE_OR_PROVINCE=\"NRW\" export CA_DN_DEFAULT_LOCALITY=\"Herzogenrath\" export CA_DN_DEFAULT_ORG=\"klarrio.com\" export CA_DN_DEFAULT_ORG_UNIT=gmbh export CA_DN_DEFAULT_EMAIL_ADDRESS=\"radek.gruchalski@klarrio.com\" export CA_DN_DEFAULT_COMMON_NAME=\"${CA_DN_DEFAULT_ORG_UNIT}.${CA_DN_DEFAULT_ORG}\" # Certificate settings: export DEFAULT_BITS=2048 export DEFAULT_CA_DAYS=365 export DEFAULT_DAYS=90 export NS_CLIENT_CERT_COMMENT=\"OpenSSL Generated Server Certificate\" export NS_SERVER_CERT_COMMENT=\"OpenSSL Generated Server Certificate\" EOF   The next step is to create a shell program responsible for writing the CA root configuration. This, and the following intermediate, are by far the two longest bits of code in this article.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165  #!/usr/bin/env bash set -eu CA_NAME=${1} if [ -f ${HOME}/.ca/.${CA_NAME} ]; then source ${HOME}/.ca/.${CA_NAME} else echo ${HOME}/.ca/.${CA_NAME} not found exit 1 fi CADIR=${HOME}/.ca/${CA_NAME} mkdir -p ${CADIR} \u0026\u0026 cd ${CADIR} # init subdirectories, index.txt and serial files: mkdir certs crl newcerts private chmod 700 private touch index.txt echo 1000 \u003e serial # write the root CA config: tee openssl.cnf \u003e/dev/null \u003c\u003cEOF # OpenSSL root CA configuration file. [ ca ] # man ca default_ca = CA_default [ CA_default ] # Directory and file locations. dir = ${CADIR} certs = \\$dir/certs crl_dir = \\$dir/crl new_certs_dir = \\$dir/newcerts database = \\$dir/index.txt serial = \\$dir/serial RANDFILE = \\$dir/private/.rand # The root key and root certificate. private_key = \\$dir/private/ca.key.pem certificate = \\$dir/certs/ca.cert.pem # For certificate revocation lists. crlnumber = \\$dir/crlnumber crl = \\$dir/crl/ca.crl.pem crl_extensions = crl_ext default_crl_days = 30 # SHA-1 is deprecated, so use SHA-2 instead. default_md = sha256 name_opt = ca_default cert_opt = ca_default default_days = ${DEFAULT_DAYS} preserve = no policy = policy_strict [ policy_strict ] # The root CA should only sign intermediate certificates that match. # See the POLICY FORMAT section of 'man ca'. countryName = match stateOrProvinceName = match organizationName = match organizationalUnitName = optional commonName = supplied emailAddress = optional [ policy_loose ] # Allow the intermediate CA to sign a more diverse range of certificates. # See the POLICY FORMAT section of the 'ca' man page. countryName = optional stateOrProvinceName = optional localityName = optional organizationName = optional organizationalUnitName = optional commonName = supplied emailAddress = optional [ req ] # Options for the 'req' tool ('man req'). default_bits = ${DEFAULT_BITS} distinguished_name = req_distinguished_name string_mask = utf8only # SHA-1 is deprecated, so use SHA-2 instead. default_md = sha256 # Extension to add when the -x509 option is used. x509_extensions = v3_ca [ req_distinguished_name ] # See \u003chttps://en.wikipedia.org/wiki/Certificate_signing_request\u003e. countryName = Country Name (2 letter code) stateOrProvinceName = State or Province Name localityName = Locality Name 0.organizationName = Organization Name organizationalUnitName = Organizational Unit Name commonName = Common Name emailAddress = Email Address # Optionally, specify some defaults. countryName_default = ${CA_DN_DEFAULT_COUNTRY_CODE} stateOrProvinceName_default = \"${CA_DN_DEFAULT_STATE_OR_PROVINCE}\" localityName_default = \"${CA_DN_DEFAULT_LOCALITY}\" 0.organizationName_default = \"${CA_DN_DEFAULT_ORG}\" organizationalUnitName_default = \"${CA_DN_DEFAULT_ORG_UNIT}\" emailAddress_default = \"${CA_DN_DEFAULT_EMAIL_ADDRESS}\" [ v3_ca ] # Extensions for a typical CA ('man x509v3_config'). subjectKeyIdentifier = hash authorityKeyIdentifier = keyid:always,issuer basicConstraints = critical, CA:true keyUsage = critical, digitalSignature, cRLSign, keyCertSign [ v3_intermediate_ca ] # Extensions for a typical intermediate CA ('man x509v3_config'). subjectKeyIdentifier = hash authorityKeyIdentifier = keyid:always,issuer basicConstraints = critical, CA:true, pathlen:0 keyUsage = critical, digitalSignature, cRLSign, keyCertSign [ usr_cert ] # Extensions for client certificates ('man x509v3_config'). basicConstraints = CA:FALSE nsCertType = client, email nsComment = \"${NS_CLIENT_CERT_COMMENT}\" subjectKeyIdentifier = hash authorityKeyIdentifier = keyid,issuer keyUsage = critical, nonRepudiation, digitalSignature, keyEncipherment extendedKeyUsage = clientAuth, emailProtection [ server_cert ] # Extensions for server certificates ('man x509v3_config'). basicConstraints = CA:FALSE nsCertType = server nsComment = \"${NS_SERVER_CERT_COMMENT}\" subjectKeyIdentifier = hash authorityKeyIdentifier = keyid,issuer:always keyUsage = critical, digitalSignature, keyEncipherment extendedKeyUsage = serverAuth [ crl_ext ] # Extension for CRLs ('man x509v3_config'). authorityKeyIdentifier=keyid:always [ ocsp ] # Extension for OCSP signing certificates ('man ocsp'). basicConstraints = CA:FALSE subjectKeyIdentifier = hash authorityKeyIdentifier = keyid,issuer keyUsage = critical, digitalSignature extendedKeyUsage = critical, OCSPSignings EOF # generate root CA: openssl genrsa -aes256 -out ${CADIR}/private/ca.key.pem 4096 chmod 400 ${CADIR}/private/ca.key.pem openssl req -config ${CADIR}/openssl.cnf \\  -key ${CADIR}/private/ca.key.pem \\  -new -x509 -days ${DEFAULT_CA_DAYS} -sha256 -extensions v3_ca \\  -out ${CADIR}/certs/ca.cert.pem chmod 444 ${CADIR}/certs/ca.cert.pem openssl x509 -noout -text -in ${CADIR}/certs/ca.cert.pem   After saving the program as ~/.ca/init-ca.sh, making it executable with chmod +x ~/.ca/init-ca.sh and running as ~/.ca/init-ca.sh article-ca, the new CA has been created in ~/.ca/article-ca. Using this method and changing the CA name given to the program as the first argument, more root CAs can be created.\nThe program will ask three times for the CA private key passphrase: to create the key, to verify and to use the key. Next, it will ask to confirm default values for the distinguished name. When all values are confirmed and correct, the CA root certificate will be generated.\nThe output will be similar to:\nGenerating RSA private key, 4096 bit long modulus ..........................++ ...........................................................................................................................................................................................++ e is 65537 (0x10001) Enter pass phrase for /Users/rad/.ca/article-ca/private/ca.key.pem: Verifying - Enter pass phrase for /Users/rad/.ca/article-ca/private/ca.key.pem: Enter pass phrase for /Users/rad/.ca/article-ca/private/ca.key.pem: You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter '.', the field will be left blank. ----- Country Name (2 letter code) [DE]: State or Province Name [NRW]: Locality Name [Herzogenrath]: Organization Name [klarrio.com]: Organizational Unit Name [gmbh]: Common Name []: Email Address [radek.gruchalski@klarrio.com]: Certificate: Data: Version: 3 (0x2) Serial Number: 14292483172117400839 (0xc65919b8604e4d07) Signature Algorithm: sha256WithRSAEncryption Issuer: C=DE, ST=NRW, L=Herzogenrath, O=klarrio.com, OU=gmbh/emailAddress=radek.gruchalski@klarrio.com Validity Not Before: Sep 8 20:53:48 2020 GMT Not After : Sep 8 20:53:48 2021 GMT Subject: C=DE, ST=NRW, L=Herzogenrath, O=klarrio.com, OU=gmbh/emailAddress=radek.gruchalski@klarrio.com Subject Public Key Info: Public Key Algorithm: rsaEncryption Public-Key: (4096 bit) Modulus: 00:ab:bf:02:ef:72:b3:ce:ac:4b:37:01:1b:57:fc: ... 0d:84:73:28:32:e8:f1:99:64:ee:f5:b2:6f:21:7f: 9e:08:21 Exponent: 65537 (0x10001) X509v3 extensions: X509v3 Subject Key Identifier: 06:DE:59:CD:9A:21:9F:A0:B2:32:EE:B3:E6:89:11:10:9C:6E:83:0F X509v3 Authority Key Identifier: keyid:06:DE:59:CD:9A:21:9F:A0:B2:32:EE:B3:E6:89:11:10:9C:6E:83:0F X509v3 Basic Constraints: critical CA:TRUE X509v3 Key Usage: critical Digital Signature, Certificate Sign, CRL Sign Signature Algorithm: sha256WithRSAEncryption 35:30:29:2c:39:1b:57:81:d8:95:9a:26:1f:5c:44:62:65:ca: ... fe:28:53:b1:76:63:22:cd on disk, there should be:\n1 2 3 4 5 6 7 8 9 10 11 12 13  [rad] ~ $ tree ~/.ca/article-ca/ /Users/rad/.ca/article-ca/ ├── certs │ └── ca.cert.pem ├── crl ├── index.txt ├── newcerts ├── openssl.cnf ├── private │ └── ca.key.pem └── serial 4 directories, 5 files   Create an intermediate CA The intermediate CA reuses the environment file of the root CA. The program to create it is very similar to the init-ca.sh.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195  #!/usr/bin/env bash set -eu CA_NAME=${1} INTERMEDIATE_NAME=${2} if [ -f ${HOME}/.ca/.${CA_NAME} ]; then source ${HOME}/.ca/.${CA_NAME} else echo ${HOME}/.ca/.${CA_NAME} not found exit 1 fi CADIR=${HOME}/.ca/${CA_NAME} INTERMEDIATEDIR=${CADIR}/intermediate/${INTERMEDIATE_NAME} mkdir -p ${INTERMEDIATEDIR} \u0026\u0026 cd ${INTERMEDIATEDIR} # init subdirectories, index.txt and serial files: mkdir certs chain crl csr newcerts private vault-bundle chmod 700 private touch index.txt echo 1000 \u003e serial echo 1000 \u003e crlnumber # write the intermediate config: tee openssl.cnf \u003e/dev/null \u003c\u003cEOF # OpenSSL intermediate CA configuration file. [ ca ] # 'man ca' default_ca = CA_default [ CA_default ] # Directory and file locations. dir = ${INTERMEDIATEDIR} certs = \\$dir/certs crl_dir = \\$dir/crl new_certs_dir = \\$dir/newcerts database = \\$dir/index.txt serial = \\$dir/serial RANDFILE = \\$dir/private/.rand # The root key and root certificate. private_key = \\$dir/private/intermediate.key.pem certificate = \\$dir/certs/intermediate.cert.pem # For certificate revocation lists. crlnumber = \\$dir/crlnumber crl = \\$dir/crl/intermediate.crl.pem crl_extensions = crl_ext default_crl_days = 30 # SHA-1 is deprecated, so use SHA-2 instead. default_md = sha256 name_opt = ca_default cert_opt = ca_default default_days = ${DEFAULT_DAYS} preserve = no policy = policy_loose [ policy_strict ] # The root CA should only sign intermediate certificates that match. # See the POLICY FORMAT section of 'man ca'. countryName = match stateOrProvinceName = match organizationName = match organizationalUnitName = optional commonName = supplied emailAddress = optional [ policy_loose ] # Allow the intermediate CA to sign a more diverse range of certificates. # See the POLICY FORMAT section of the 'ca' man page. countryName = optional stateOrProvinceName = optional localityName = optional organizationName = optional organizationalUnitName = optional commonName = supplied emailAddress = optional [ req ] # Options for the 'req' tool ('man req'). default_bits = ${DEFAULT_BITS} distinguished_name = req_distinguished_name string_mask = utf8only # SHA-1 is deprecated, so use SHA-2 instead. default_md = sha256 # Extension to add when the -x509 option is used. x509_extensions = v3_ca [ req_distinguished_name ] # See \u003chttps://en.wikipedia.org/wiki/Certificate_signing_request\u003e. countryName = Country Name (2 letter code) stateOrProvinceName = State or Province Name localityName = Locality Name 0.organizationName = Organization Name organizationalUnitName = Organizational Unit Name commonName = Common Name emailAddress = Email Address # Optionally, specify some defaults. countryName_default = ${CA_DN_DEFAULT_COUNTRY_CODE} stateOrProvinceName_default = \"${CA_DN_DEFAULT_STATE_OR_PROVINCE}\" localityName_default = \"${CA_DN_DEFAULT_LOCALITY}\" 0.organizationName_default = \"${CA_DN_DEFAULT_ORG}\" organizationalUnitName_default = \"${CA_DN_DEFAULT_ORG_UNIT}\" commonName_default = \"${INTERMEDIATE_NAME}.${CA_DN_DEFAULT_COMMON_NAME}\" emailAddress_default = \"${CA_DN_DEFAULT_EMAIL_ADDRESS}\" [ v3_ca ] # Extensions for a typical CA ('man x509v3_config'). subjectKeyIdentifier = hash authorityKeyIdentifier = keyid:always,issuer basicConstraints = critical, CA:true keyUsage = critical, digitalSignature, cRLSign, keyCertSign [ v3_intermediate_ca ] # Extensions for a typical intermediate CA ('man x509v3_config'). subjectKeyIdentifier = hash authorityKeyIdentifier = keyid:always,issuer basicConstraints = critical, CA:true, pathlen:0 keyUsage = critical, digitalSignature, cRLSign, keyCertSign [ usr_cert ] # Extensions for client certificates ('man x509v3_config'). basicConstraints = CA:FALSE nsCertType = client, email nsComment = \"${NS_CLIENT_CERT_COMMENT}\" subjectKeyIdentifier = hash authorityKeyIdentifier = keyid,issuer keyUsage = critical, nonRepudiation, digitalSignature, keyEncipherment extendedKeyUsage = clientAuth, emailProtection [ server_cert ] # Extensions for server certificates ('man x509v3_config'). basicConstraints = CA:FALSE nsCertType = server nsComment = \"${NS_SERVER_CERT_COMMENT}\" subjectKeyIdentifier = hash authorityKeyIdentifier = keyid,issuer:always keyUsage = critical, digitalSignature, keyEncipherment extendedKeyUsage = serverAuth [ crl_ext ] # Extension for CRLs ('man x509v3_config'). authorityKeyIdentifier=keyid:always [ ocsp ] # Extension for OCSP signing certificates ('man ocsp'). basicConstraints = CA:FALSE subjectKeyIdentifier = hash authorityKeyIdentifier = keyid,issuer keyUsage = critical, digitalSignature extendedKeyUsage = critical, OCSPSigning EOF # generate intermediate CA certificate for ${CA_NAME}/${INTERMEDIATE_NAME} in ${INTERMEDIATEDIR}...\" openssl genrsa -aes256 \\  -out ${INTERMEDIATEDIR}/private/intermediate.key.pem 4096 chmod 400 ${INTERMEDIATEDIR}/private/intermediate.key.pem openssl req -config ${INTERMEDIATEDIR}/openssl.cnf -new -sha256 \\  -key ${INTERMEDIATEDIR}/private/intermediate.key.pem \\  -out ${INTERMEDIATEDIR}/csr/intermediate.csr.pem openssl ca -config ${CADIR}/openssl.cnf -extensions v3_intermediate_ca \\  -days ${DEFAULT_CA_DAYS} -notext -md sha256 \\  -in ${INTERMEDIATEDIR}/csr/intermediate.csr.pem \\  -out ${INTERMEDIATEDIR}/certs/intermediate.cert.pem chmod 444 ${INTERMEDIATEDIR}/certs/intermediate.cert.pem openssl x509 -noout -text \\  -in ${INTERMEDIATEDIR}/certs/intermediate.cert.pem openssl verify -CAfile ${CADIR}/certs/ca.cert.pem \\  ${INTERMEDIATEDIR}/certs/intermediate.cert.pem # generate the chain: cat ${INTERMEDIATEDIR}/certs/intermediate.cert.pem \\  ${CADIR}/certs/ca.cert.pem \u003e ${INTERMEDIATEDIR}/chain/ca-chain.cert.pem chmod 444 ${INTERMEDIATEDIR}/certs/ca-chain.cert.pem # convert intermediate CA key to PKCS1 format required by Vault bundle openssl rsa -in ${INTERMEDIATEDIR}/private/intermediate.key.pem \\  -out ${INTERMEDIATEDIR}/vault-bundle/intermediate-pkcs1.key.pem \\  -outform pem # generate the actual Vault bundle: cat ${INTERMEDIATEDIR}/chain/ca-chain.cert.pem \\  ${INTERMEDIATEDIR}/vault-bundle/intermediate-pkcs1.key.pem \u003e \\  ${INTERMEDIATEDIR}/vault-bundle/bundle.pem # remove the pkcs1 file: rm ${INTERMEDIATEDIR}/vault-bundle/intermediate-pkcs1.key.pem   After saving as ~/.ca/init-intermediate.sh, making executable with chmod +x ~/.ca/init-intermediate.sh, it can be executed. I’m going to create two intermediate CAs immediately. In my case, mirroring the imaginary use case for setting up Keycloak with multiple clients3, I have:\n ClientA ~/.ca/init-intermediate.sh article-ca client-a ClientB ~/.ca/init-intermediate.sh article-ca client-b  In each case, the program will ask the following:\n the intermediate CA key password, three times: to create the key, verify the password and use the key verify to confirm distinguished name defaults, additionally a common name root CA key password during intermediate signing a couple of confirmations once again a passphrase of the intermediate CA key for pkcs1 conversion  After these two commands are finished, the files on disk look similar to:\n[rad] ~ $ tree ~/.ca/article-ca/ /Users/rad/.ca/article-ca/ ├── certs │ └── ca.cert.pem ├── crl ├── index.txt ├── index.txt.attr ├── index.txt.attr.old ├── index.txt.old ├── intermediate │ ├── client-a │ │ ├── certs │ │ │ └── intermediate.cert.pem │ │ ├── chain │ │ │ └── ca-chain.cert.pem │ │ ├── crl │ │ ├── crlnumber │ │ ├── csr │ │ │ └── intermediate.csr.pem │ │ ├── index.txt │ │ ├── newcerts │ │ ├── openssl.cnf │ │ ├── private │ │ │ └── intermediate.key.pem │ │ ├── serial │ │ └── vault-bundle │ │ └── bundle.pem │ └── client-b │ ├── certs │ │ └── intermediate.cert.pem │ ├── chain │ │ └── ca-chain.cert.pem │ ├── crl │ ├── crlnumber │ ├── csr │ │ └── intermediate.csr.pem │ ├── index.txt │ ├── newcerts │ ├── openssl.cnf │ ├── private │ │ └── intermediate.key.pem │ ├── serial │ └── vault-bundle │ └── bundle.pem ├── newcerts │ ├── 1000.pem │ └── 1001.pem ├── openssl.cnf ├── private │ └── ca.key.pem ├── serial └── serial.old 21 directories, 29 files The output of the intermediate CA init, for one of the executions, is similar to:\nGenerating RSA private key, 4096 bit long modulus ...........................................++ .....................................................................................................................................................................................................................................................................................++ e is 65537 (0x10001) Enter pass phrase for /Users/rad/.ca/article-ca/intermediate/client-a/private/intermediate.key.pem: Verifying - Enter pass phrase for /Users/rad/.ca/article-ca/intermediate/client-a/private/intermediate.key.pem: Enter pass phrase for /Users/rad/.ca/article-ca/intermediate/client-a/private/intermediate.key.pem: You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter '.', the field will be left blank. ----- Country Name (2 letter code) [DE]: State or Province Name [NRW]: Locality Name [Herzogenrath]: Organization Name [klarrio.com]: Organizational Unit Name [gmbh]: Common Name [client-a.gmbh.klarrio.com]: Email Address [radek.gruchalski@klarrio.com]: Using configuration from /Users/rad/.ca/article-ca/openssl.cnf Enter pass phrase for /Users/rad/.ca/article-ca/private/ca.key.pem: Check that the request matches the signature Signature ok Certificate Details: Serial Number: 4096 (0x1000) Validity Not Before: Sep 8 21:05:08 2020 GMT Not After : Sep 8 21:05:08 2021 GMT Subject: countryName = DE stateOrProvinceName = NRW organizationName = klarrio.com organizationalUnitName = gmbh commonName = client-a.gmbh.klarrio.com emailAddress = radek.gruchalski@klarrio.com X509v3 extensions: X509v3 Subject Key Identifier: 47:7D:8A:9E:DC:23:03:7A:AA:E4:79:A8:98:EE:40:54:01:84:1C:E8 X509v3 Authority Key Identifier: keyid:06:DE:59:CD:9A:21:9F:A0:B2:32:EE:B3:E6:89:11:10:9C:6E:83:0F X509v3 Basic Constraints: critical CA:TRUE, pathlen:0 X509v3 Key Usage: critical Digital Signature, Certificate Sign, CRL Sign Certificate is to be certified until Sep 8 21:05:08 2021 GMT (365 days) Sign the certificate? [y/n]:y 1 out of 1 certificate requests certified, commit? [y/n]y Write out database with 1 new entries Data Base Updated Certificate: Data: Version: 3 (0x2) Serial Number: 4096 (0x1000) Signature Algorithm: sha256WithRSAEncryption Issuer: C=DE, ST=NRW, L=Herzogenrath, O=klarrio.com, OU=gmbh/emailAddress=radek.gruchalski@klarrio.com Validity Not Before: Sep 9 21:05:08 2020 GMT Not After : Sep 9 21:05:08 2021 GMT Subject: C=DE, ST=NRW, O=klarrio.com, OU=gmbh, CN=client-a.gmbh.klarrio.com/emailAddress=radek.gruchalski@klarrio.com Subject Public Key Info: Public Key Algorithm: rsaEncryption Public-Key: (4096 bit) Modulus: 00:aa:c3:eb:7e:c4:2a:79:2e:bc:7b:6f:c2:61:f9: ... 80:27:da:c6:b8:ff:20:3b:7f:9b:fd:ff:15:d0:2c: e7:2b:03 Exponent: 65537 (0x10001) X509v3 extensions: X509v3 Subject Key Identifier: 47:7D:8A:9E:DC:23:03:7A:AA:E4:79:A8:98:EE:40:54:01:84:1C:E8 X509v3 Authority Key Identifier: keyid:06:DE:59:CD:9A:21:9F:A0:B2:32:EE:B3:E6:89:11:10:9C:6E:83:0F X509v3 Basic Constraints: critical CA:TRUE, pathlen:0 X509v3 Key Usage: critical Digital Signature, Certificate Sign, CRL Sign Signature Algorithm: sha256WithRSAEncryption 22:b5:e3:6e:a5:d6:0d:6b:30:65:a3:d9:68:27:38:2b:ea:64: ... 39:ac:c3:66:88:8c:f0:eb /Users/rad/.ca/article-ca/intermediate/client-a/certs/intermediate.cert.pem: OK Enter pass phrase for /Users/rad/.ca/article-ca/intermediate/client-a/private/intermediate.key.pem: writing RSA key At this stage, I have a root CA and two intermediate CA for respective common names:\n ClientA: client-a.gmbh.klarrio.com ClientB: client-b.gmbh.klarrio.com  For every intermediate, I have a PEM bundle ready to be imported to HashiCorp Vault.\nStart Vault in development mode I’m going to use Vault development mode, with VAULT_DEV_ROOT_TOKEN_ID hard coded and default port 8200 exposed so it can be reached from localhost.\n1 2 3 4 5 6 7 8 9 10 11 12 13  docker run --rm \\  -p 8200:8200 \\  -ti \\  -e 'VAULT_DEV_ROOT_TOKEN_ID=dev-token' \\  -e 'VAULT_DEV_LISTEN_ADDRESS=0.0.0.0:8200' \\  --cap-add=IPC_LOCK \\  vault ... 2020-09-08T21:08:07.855Z [INFO] expiration: revoked lease: lease_id=auth/token/root/h128b471d9329710fbbd34dc5a4a98b4d0c7fe104799e818447a013762aed1e66 2020-09-08T21:08:07.859Z [INFO] core: successful mount: namespace= path=secret/ type=kv 2020-09-08T21:08:07.869Z [INFO] secrets.kv.kv_9795fd99: collecting keys to upgrade 2020-09-08T21:08:07.869Z [INFO] secrets.kv.kv_9795fd99: done collecting keys: num_keys=1 2020-09-08T21:08:07.869Z [INFO] secrets.kv.kv_9795fd99: upgrading keys finished   Now, in another terminal window, I am going to export the token and Vault address environment variables:\n1 2  export VAULT_ADDR=http://127.0.0.1:8200 export VAULT_TOKEN=dev-token   Finally, I can enable PKIs:\n1 2  vault secrets enable -path=client-a pki vault secrets enable -path=client-b pki   I can see Vault confirming:\n2020-09-08T21:11:32.133Z [INFO] core: successful mount: namespace= path=client-a/ type=pki 2020-09-08T21:11:32.914Z [INFO] core: successful mount: namespace= path=client-b/ type=pki Importing the bundles The next step is to import the PEM bundles, they were prepared during the intermediate CA creation:\n1 2 3 4  [rad] ~ $ vault write client-a/config/ca pem_bundle=@${HOME}/.ca/article-ca/intermediate/client-a/vault-bundle/bundle.pem Success! Data written to: client-a/config/ca [rad] ~ $ vault write client-b/config/ca pem_bundle=@${HOME}/.ca/article-ca/intermediate/client-b/vault-bundle/bundle.pem Success! Data written to: client-b/config/ca   1 2 3 4 5 6 7 8 9 10 11 12 13 14  [rad] ~ $ vault write client-a/roles/system-of-client-a-com \\  allow_localhost=true \\  allow_bare_domains=true \\  allowed_domains=\"localhost,client-a.gmbh.klarrio.com\" \\  allow_subdomains=true \\  max_ttl=\"720h\" Success! Data written to: client-a/roles/system-of-client-a-com [rad] ~ $ vault write client-b/roles/system-of-client-b-com \\  allow_localhost=true \\  allow_bare_domains=true \\  allowed_domains=\"localhost,client-b.gmbh.klarrio.com\" \\  allow_subdomains=true \\  max_ttl=\"720h\" Success! Data written to: client-b/roles/system-of-client-b-com   Notes to the examples above  By setting allow_bare_domains to true, I allow the user to generate a certificate for any literal domain specified in allowed_domains, so the user can issue a certificate for client-[X].gmbh.klarrio.com. By setting allow_localhost to true, I allow issuing a certificate for localhost, useful for testing. By setting allow_subdomains to true, I give the user the ability to issue certificates with common names that are subdomains of allowed_domains.  All the interesting options are documented in the Vault documentation4.\nRequesting certificates To request a certificate, simply issue the following command:\n1 2 3 4  vault write client-a/issue/system-of-client-a-com \\  common_name=\"become.client-a.gmbh.klarrio.com\" \\  ttl=\"240h\" \\  format=pem   To which the output is … pretty verbose …:\nKey Value --- ----- ca_chain [-----BEGIN CERTIFICATE----- MIIF+zCCA+OgAwIBAgICEAAwDQYJKoZIhvcNAQELBQAwgYQxCzAJBgNVBAYTAkRF MQwwCgYDVQQIDANOUlcxFTATBgNVBAcMDEhlcnpvZ2VucmF0aDEUMBIGA1UECgwL ... KhlaTNBfyYaIYXeTQgCa+ar7OcZQhKMvPv1dTOFtZQ8Rjk5+8Y0yOazDZoiM8Os= -----END CERTIFICATE----- -----BEGIN CERTIFICATE----- MIIF8DCCA9igAwIBAgIJAMZZGbhgTk0HMA0GCSqGSIb3DQEBCwUAMIGEMQswCQYD ... kY3o7PdJs57rbyjVo3UWLZwQbqBkMpH3zxjKLco8lE3UscEU7Up/ZxqQF9i3Wxj1 bahjbSfvI/V5gKaVkfcp/Pe1wavMFSI0GueEzP4oU7F2YyLN -----END CERTIFICATE-----] certificate -----BEGIN CERTIFICATE----- MIIE8TCCAtmgAwIBAgIUZWaMXolFNt/ufzgUOnvu79ZCVjEwDQYJKoZIhvcNAQEL BQAwgZMxCzAJBgNVBAYTAkRFMQwwCgYDVQQIDANOUlcxFDASBgNVBAoMC2tsYXJy ... nJOv5KyaSz8OCKdX+JlVmU9Qoapj4EyXOZQ+LS8RBsFXrbjpjqxdd3kpiEhpcQwN 7UXijzXX25gZKwFPTof+VdAKa5M1/uU9G15KwL4S/vJwYGwL/zo799qrGdd/+plV cOd4GA883CW0DdC8QuU2+w3HIRS6 -----END CERTIFICATE----- expiration 1600616181 issuing_ca -----BEGIN CERTIFICATE----- MIIF+zCCA+OgAwIBAgICEAAwDQYJKoZIhvcNAQELBQAwgYQxCzAJBgNVBAYTAkRF MQwwCgYDVQQIDANOUlcxFTATBgNVBAcMDEhlcnpvZ2VucmF0aDEUMBIGA1UECgwL a2xhcnJpby5jb20xDTALBgNVBAsMBGdtYmgxKzApBgkqhkiG9w0BCQEWHHJhZGVr ... pIjI/wXZwURNn920ODhsA0v467Llw4gMTTjxOfDIFrdZ46936IMzHNOXI5b87dfU KhlaTNBfyYaIYXeTQgCa+ar7OcZQhKMvPv1dTOFtZQ8Rjk5+8Y0yOazDZoiM8Os= -----END CERTIFICATE----- private_key -----BEGIN RSA PRIVATE KEY----- MIIEpAIBAAKCAQEA57Ww9OC6UzHaLAALdbotEoTf5c2qw4BufVlOB7zZh+GbX2hR ... zcDV96GoXPEnuXHdVsfePFIdPS7IRmAEn72UW6u39mVqGJuX1/5tk76ay6cPHP/B xtX2UAplk9bD046wNh1/PxudBnqavOFf4F5uDizYhyihbmmhS/mcxA== -----END RSA PRIVATE KEY----- private_key_type rsa serial_number 65:66:8c:5e:89:45:36:df:ee:7f:38:14:3a:7b:ee:ef:d6:42:56:31 Cool, we have the certificate, a private key and a certificate chain. The CA chain can now be copied to chain.pem file.\nPay attention to the -----END CERTIFICATE----- -----BEGIN CERTIFICATE----- bit. It needs to be saved as:\n... -----END CERTIFICATE----- -----BEGIN CERTIFICATE----- ... Copy the certificate to certificate.pem:\nAnd the private to key.pem:\nA server with TLS transport can now be created using the chain.pem, certificate.pem and key.pem files.\nWhy is this cool  It’s not possible to request certificates for domain names not in allowed_domains:  1 2 3 4 5 6 7 8 9 10  [rad] ~ $ vault write client-a/issue/system-of-client-a-com \\  common_name=\"become.client-b.gmbh.klarrio.com\" \\  ttl=\"240h\" \\  format=pem Error writing data to client-a/issue/system-of-client-a-com: Error making API request. URL: PUT http://127.0.0.1:8200/v1/client-a/issue/system-of-client-a-com Code: 400. Errors: * common name become.client-b.gmbh.klarrio.com not allowed by this role   It’s possible to provide alt names:  1 2 3 4 5  vault write client-b/issue/system-of-client-b-com \\  common_name=\"get-in-touch.client-b.gmbh.klarrio.com\" \\  alt_names=\"you.client-b.gmbh.klarrio.com,or-you.client-b.gmbh.klarrio.com\" \\  ttl=\"720h\" \\  format=pem   Certificates generated for ClientA do not validate for ClientB and vice versa - different intermediate It is very easy to define a policy with different criteria. For example: to allow a user of ClientB to generate a certificate for specific domain only:  1 2 3 4  [rad] ~ $ vault write client-a/roles/get-in-touch \\  allow_bare_domains=true \\  allowed_domains=\"get-in-touch.client-a.gmbh.klarrio.com\" \\  max_ttl=\"1h\"   so that it’s possible to:\n1 2 3 4  vault write client-a/issue/get-in-touch \\  common_name=\"get-in-touch.client-a.gmbh.klarrio.com\" \\  ttl=\"1h\" \\  format=pem   but not:\n1 2 3 4  vault write client-a/issue/get-in-touch \\  common_name=\"maybe.get-in-touch.client-a.gmbh.klarrio.com\" \\  ttl=\"1h\" \\  format=pem   Conclusion Once the initial setting up of the program to create the CAs is finished, the rest of the process is straightforward. Adding new clients (intermediates) is fairly easy. Configuring Vault is not a big job. Vault PKI is a solid choice for a multi-tenant PKI solution.\nOf course, there are still some challenges left on the table. Mainly storing the root and intermediate certificates safely and properly ensuring who can access them.\n  Certificate Authority is not Voodoo ↩︎\n OpenSSL Certificate Authority ↩︎\n Introduction to Keycloak Authorization Services ↩︎\n Vault PKI Create/Update Role Parameters ↩︎\n   ","description":"Configuring multi-tenant Vault PKI with OpenSSL root and intermediate CAs","tags":["pki","ca","tls","openssl","vault","multi-tenant"],"title":"Multi-tenant Vault PKI with custom root PEM bundles","uri":"/posts/2020-09-09-multi-tenant-vault-pki-with-custom-root-pem-bundle/"},{"content":"Modern applications tend to get fairly complex pretty quick. A usual stack will consist of many moving parts. Starting from a cloud environment, maybe abstracted behind Kubernetes or Mesos, through multitude of web servers, GRPC services, to monitoring systems like Grafana, Jaeger, Prometheus, all fronted with load balancers or proxies like Traefik. Many of these components have fairly complex dependencies, ETCD or Zookeeper come to mind. All these power a highly dynamic environment where containers and virtual machines iterate and get replaced often. Some businesses operate multiple copies of stacks for development, staging and production environments.\nWith so many gizmos and the security breaches happening on a nearly daily basis, it’s important to ensure that the traffic between the individual systems can be traced, what is probably even more important: authenticated and encrypted.\nIn cloud environments, vendor provided tools help to a certain extent. For example, a common pattern in AWS is to use security groups or / and ACLs. As efficient as they are, one only gets so far. Security groups will not help with containerized workflows. An individual host can run containers for different workloads, maybe different customers. Maybe Kubernetes has some answers for some with its networking capabilities. For more advanced requirements, Project Calico 1 may come handy. But even then, certain workflows can’t be covered with any of that.\nThis is where many people will turn to TLS. Today, it is very difficult to find software that would not support TLS out of the box. With HTTP / public facing applications, things are pretty easy. Thanks to Let’s Encrypt2, anybody can get TLS certificates for public facing systems for free. That is awesome. But what about internal infrastructure? Let’s Encrypt, even though the limits are generous, does not really work very well. Fair percentage of software integrates well with Let’s Encrypt. For example nginx, Traefik. But not everything. And what about the systems identified by IP addresses? What about internal DNS names?\nOrganizations end up shoehorning a custom TLS solution, very often disregarding the most reasonable answer: running a custom Certificate Authority.\nTo be honest, even a few years ago, running a custom PKI, Certificate Authority, beeing a part of it, was a hassle. Searching for ready recipes for how to set things up in order to get a root certificate, many tutorials glossing over the importance of intermediate certificates, certificate signing requests, … Ins and outs of configuring and running a CA seemed to be some secret, insider’s knowledge.\nMy personal perception is that the raise of Golang, with its unprecedented ease of crypto library use, broke this trend and brought TLS to the masses. There are many complete code examples in Golang tests. Even today, not taking away from the authors of BouncyCastle (which is really, really, really awesome when one knows the library), trying to do more complex things with TLS in Java can be mind boggling. Maybe the impulse came in 2013, with Ivan Ristić publishing the Bulletproof SSL and TLS 3 book, which is a immense resource for anybody trying to master SSL and TLS, I don’t know. Sorry, slight deviation, personal perception…\nA software developer who is looking to implement TLS at some point is going to search for\n how to generate an ssl certificate\n Answers will turn up. Majority of these will show how to get a self-signed certificate. A self-signed certificate is generally okay, for example, in integration testing there is no real need for a CA. In tests, it does not matter who issued a certificate, what matters is that TLS works.\nHowever, the problem with a self-signed certificate is that it is not signed by a Certificate Authority. So, yes, it does provide security in the sense that the traffic is TLS encrypted, but it does not tell anything about who generated and deployed the certificate. Anybody can issue a self-signed certificate for anything and it’s going to take some effort to figure out where it came from. A Certificate Authority with the correct set of intermediates is going to to provide a verification chain required to properly ensure the authenticity of a certificate.\nIt’s fair to say that a production system should not see a self-signed certificate in operation.\nWhat are the options Below is a very brief overview of a few modern Certificate Authority options.\nOpenSSL Certificate Authority Starting from the most widely available tool for creating a Certificate Authority: OpenSSL. The one and only resource I can recommend for anybody who would like to setup and operate a CA with OpenSSL is the OpenSSL Certificate Authority guide created by Jamie Nguyen. 4 The reader of the guide will find a very well documented step-by-step guide, where every step is easy to follow and well explained. An appendix to the the guide contains ready to use recipes.\nAll documents available on that website are licensed under Creative Commons Attribution-ShareAlike 4.0 International License.\nFor actual operating, a knowledge of OpenSSL will be required.\nCFSSL A good alternative is CloudFlare’s PKI/TLS toolkit: CFSSL. 5 Written in Golang, available on GitHub. CFSSL offers a command line and a HTTP API for signing, verifying and bundling TLS certificates. With CFSSL, one can stand multiple Certificate Authorities within minutes. The readme offers a pretty good overview of the capabilities.\nAvailable under BSD-2-Clause License.\nSquare certstrap Another tool written in Golang, this time from Square, the company providing financial and merchant services. Square certstrap is also available on GitHub. 6 This tool offers a very easy to use command line API for creating new Certificate Authorities, identities, certificate signing requests and generating and signing certificates. As with the previous solution, the readme gives a great overview or what’s possible.\nAvailable under Apache 2.0 License.\nHashiCorp Vault HashiCorp is a very prominent player in the modern infrastructure software. Known for tools like Vagrant, Packer, Terraform, Consul, and Vault. One of the features of Vault is the PKI. From all the tools mentioned here, HashiCorp Vault PKI 7 is by far most complete and the easiest to integrate into an existing infrastructure.\nIt comes packaged as a binary, there are Docker images available. Vault itself can be configured for high availability. It offers a command line API and a HTTP API. Vault can host multiple PKIs in a single installation. Access to each PKI can be controlled using Vault policies and the access is protected by tokens with configurable leases. It’s really good. By default, Vault PKI is configured with a root certificate generated by Vault. Alternatively, a root bundle can be imported. There is a very good guide on setting up a fresh PKI with Vault: Build Your Own Certificate Authority (CA).\nConclusion: lower bar of entry No matter which tool is the tool of choice, there are still challenges in operating a Certificate Authority. Root and intermediate certificates have to be stored secure, they need to be rotated. Preparing an underlying, bulletproof infrastructure for reliably relaunching a Certificate Authority is not a walk in the park. Certificate Revocation Lists are still difficult. In certain environments, certificate distribution is a challenge.\nOn a positive note: the bar of running a CA is today much lower.\n  Project Calico ↩︎\n Let’s Encrypt ↩︎\n Understanding and deploying SSL/TLS and PKI to secure servers and web applications, by Ivan Ristić ↩︎\n OpenSSL Certificate Authority ↩︎\n CloudFlare’s PKI/TLS toolkit ↩︎\n Square certstrap ↩︎\n HashiCorp Vault PKI ↩︎\n   ","description":"Investigation of available options for running and operating a Certificate Authority for modern applications","tags":["pki","ca","tls","ssl","infrastructure"],"title":"Certificate Authority is not Voodoo","uri":"/posts/2020-09-07-certificate-authority-is-not-voodoo/"},{"content":"In Introduction to Keycloak Authorization Services 1, I have described how to use the Authorization Services to find out if the user has access to certain resources.\nI have done so by asking Keycloak to issue an access token with a special grant_type with the value of urn:ietf:params:oauth:grant-type:uma-ticket which returned a list of permissions the has access to.\nThe request looked like this:\n1 2 3 4 5 6  curl --silent -X POST \\  ${KEYCLOAK_TOKEN_URL} \\  -H \"Authorization: Bearer ${access_token}\" \\  --data \"grant_type=urn:ietf:params:oauth:grant-type:uma-ticket\" \\  --data \"audience=customers\" \\  --data \"permission=CustomerB#customer-b\" | jq '.'   where the ${access_token} was the result of\n1 2 3 4  export access_token=`curl --silent -u customers:${KEYCLOAK_CLIENT_SECRET} \\  -k -d \"grant_type=password\u0026username=member@service-team\u0026password=${USER_PASSWORD}\u0026scope=email profile\" \\  -H \"Content-Type:application/x-www-form-urlencoded\" \\  ${KEYCLOAK_TOKEN_URL} | jq '.access_token' -r`   The response I was looking for looked like:\n1 2 3 4 5 6 7 8 9  { \"upgraded\": false, \"access_token\": \"eyJhbGciOiJSUzI1NiIsI...n8AC51T1AMwDtoqfCEXrdwcrQ\", \"expires_in\": 300, \"refresh_expires_in\": 1800, \"refresh_token\": \"eyJhbGciOiJIUz...RG3zFus\", \"token_type\": \"Bearer\", \"not-before-policy\": 0 }   However, since I was only interested in the confirmation if the user has access to certain resources, I could have used an additional parameter to the grant_type=urn:ietf:params:oauth:grant-type:uma-ticket call:\n--data \"response_mode=decision\" With this parameter, the call to retrieve the decision would look like:\n1 2 3 4 5 6 7  curl --silent -X POST \\  ${KEYCLOAK_TOKEN_URL} \\  -H \"Authorization: Bearer ${access_token}\" \\  --data \"grant_type=urn:ietf:params:oauth:grant-type:uma-ticket\" \\  --data \"audience=customers\" \\  --data \"response_mode=decision\" \\  --data \"permission=CustomerB#customer-b\" | jq '.'   In case of the user having access, Keycloak would return:\n1 2 3  { \"result\": true }   Otherwise, the response would be:\n1 2 3 4  { \"error\": \"access_denied\", \"error_description\": \"not_authorized\" }     Introduction to Keycloak Authorization Services ↩︎\n   ","description":"Retrieving just the access decision from Key Authorization Services","tags":["keycloak","iam","uma"],"title":"Keycloak Authorization Services - retrieving the decision only","uri":"/posts/2020-09-06-keycloak-authorization-services-decision-only/"},{"content":"As the number of applications and websites in the organization grows, the developer will inevitably receive a request to implement Single Sign-On. Single Sign-On (SSO for short) is an authentication scheme allowing the user to log in with a single set of credentials and share the session across multiple, independent, potentially unrelated systems.\nThe savvy developer will roll out Keycloak, enable Standard Flow client, maybe enable some of the social login options, like GitHub, Google or Facebook and call it a day. The users will be happy. When they go to any of the internet properties requiring signing in, if they are not signed in, they will be redirected to Keycloak login page. Once they log in, they receive a token, with which they can use to access any other property requiring login.\nAs a bonus, the SSO usually introduces a Single Log-Out (SLO). By invalidating the access token, the user is logged out of all the properties relying on that token.\nSetting up the scene But what if the complexity of the system goes one step further? For example, the user of the properties is a Member of the Support Team and the property in question is a support system where, for example, the Support Team member can view and manage some data on behalf of a Customer. The company has many Customers and many Support Team Members. Maybe some of the Support Team Members are dedicated to certain Customers? When they sign in to the Support System, they should only see and be able to act, on behalf of only those selected, dedicated Customers.\nThe first thought of any seasoned developer would most likely be to create a new database and store a mapping between the Support Team Member user and the Customer. The Support Application would then query the new database and only display the Customers for which the mappings exist. And that’s fine, there is nothing wrong with approach. However, that’s another database to maintain. Someone has to create the rules of which Member supports which Customer. This knowledge has to be stored somewhere and someone has to build an application to ensure the data in the new database is always up to date and relevant.\nAn alternative approach would be to use Keycloak for storing, managing and retrieval of all of this knowledge. If we consider Keycloak to be a single source of truth across the organization, we remove quite a lot of complexity.\nSo, further in this article, I am showing a proof of concept of Keycloak as a mechanism to allow Support Team Members to access selected Customers only, without any other database. This will also present how to use Keycloak Authorization Services in real-world scenario and give the reader a glimpse into User Managed Access (UMA).\nI have described how to start a local development version of Keycloak. Examples here will build on top of the previous write up. 1\nThe goal The outcome of this article is to have a Keycloak realm with an OpenID client configured so that a program can be created to query Keycloak for users' entitlements and discover all available entitlements of a given type by leveraging Keycloak token and resource set endpoints.\nWe will configure a realm with required roles and set up Authorization Services resources, policies, scopes and permissions for two different access levels: a regular user, Service Team Member, and a supervisor, the user who is entitled to see all available resources, regardless of the role membership.\nWithout any further due, let’s start!\nAdd a realm Open the browser, go to http://localhost:28080/auth/admin/master/console/, sign in as admin:admin.\nBy default, we are signed in to the Master realm. So the first thing to do, is to create a new realm. In the top left corner, under the Keycloak logo, hover over Master or Select realm text. A menu will appear, there is the Add realm button. Click the button and on the form that shows up, type multi-customer in the Name field.\nThe realm is our disposable proving ground. All the users, roles and everything we will do further, resides inside. Deleting a realm, deletes all users and settings.\nConfigure roles We will represent our imaginary Customers as roles. We have to add a role for every Customer. In the left menu, find and click Roles. Create a role for every customer, for the sake of this article, I’ll go with:\n CustomerA CustomerB  The OpenID Client Now, we have to create an OpenID Client. Client is what allows the users of our application securely exchanging client id and secret for an access token. Click Clients in the left menu. On the page that opens, find the Create button near the top of the right corner of the page, click it.\nType customers as a Client ID. Leave Client Protocol as openid-connect and put http://localhost:28080 as Root URL. Click Save. The page will reload and a bunch of other settings will become available.\nSettings tab Set them as follows:\n Access Type: confidential Standard Flow Enabled: off Implicit Flow enabled: off Direct Grants Enabled: on Authorization Enabled: on  this will enable Service Accounts   URLs are pre-populated and good for what we need to do Click Save  the Authorization tab will appear    Scope tab Great, now go to Scope tab.\n Full Scope Allowed: off  Realm Roles will appear. Select all Available Roles and click Add selected button.\nAuthorization tab This is where things get a little bit involved. A bunch of new tabs have appeared.\nAuthorization / Settings  Policy Enforcement Mode: Enforcing Decision Strategy: Unanimous Remote Resource Management: off  Click Save.\nAuthorization / Policies  delete Default Policy For each Customer, create a Role based policy (dropdown on the right side of the page):  CustomerA:  Name: Policy-CustomerA Realm Roles: type and select: CustomerA (click Required Logic: Positive   CustomerB:  Name: Policy-CustomerB Realm Roles: type and select: CustomerB (click Required) Logic: Positive      Authorization / Authorization Scopes Create a scope for each customer:\n for CustomerA: customer-a for CustomerB: customer-b  Authorization / Resources  delete Default Resource  Create a resource for each customer:\n CustomerA:  Name: CustomerA Display Name: Customer A Resource Type: urn:customers:resources:customer URI: /customers/CustomerA (irrelevant for our use case) Scope: customer-a User Managed Access: off   CustomerB:  Name: CustomerB Display Name: Customer B Resource Type: urn:customers:resources:customer URI: /customers/CustomerB (irrelevant for our use case) Scope: customer-b User Managed Access: off    Authorization / Permissions Finally, create permissions. One Scope-Based permission per customer. As with policies, the option to add is on the right side of the screen, a dropdown.\n CustomerA:  Name: CustomerA Permission Resource: CustomerA Scope: customer-a Apply policy: Select Existing Policy: Policy-CustomerA Decision strategy: Unanimous   CustomerB:  Name: CustomerB Permission Resource: CustomerB Scope: customer-b Apply policy: Select Existing Policy: Policy-CustomerB Decision strategy: Unanimous    We have finished setting up Authorization services.\nBefore we can play around, we will add our Support Team Member user to Keycloak.\nCreate a user In the left Keycloak menu, click Users. On the right hand side of the Lookup header, there is an Add user button. Click it. Populate the fields as follows:\n Username: member@service-team Email: member@service-team First name: Member Last name: ServiceTeam User enabled: on Email verified: on  Click Save.\nUser credentials We are going to be interacting with Keycloak via command line only and the purpose of this exercise is to validate specific user’s resource access. Hence, we need to set the password for the user because we will use Resource owner credentials grant Section 4.3.\n Set password: password123 Temporary: off  Click Set password.\nFetch the OpenID Client credentials Once again, in the left Keycloak menu, click Clients. Find customers client and click on it. Go to Credentials tab. Your client id is the client name—customers. The secret is displayed. Copy it and in the terminal, export as:\n1  export KEYCLOAK_CLIENT_SECRET=...   Also, export the username and password.\nThis is obviously done only for the sake of this tutorial. Don’t export passwords or secrets like this in a production system. Really, never. Use something like Ansible Vault or HashiCorp Vault to store secrets.\nEven storing the password in the file and using:\n1  $(cat /path/to/the/password/file)   would be better than what we do below. But now…\n1 2  export USER_NAME=member@service-team export USER_PASSWORD=password123   Play time!  If you receive an Invalid bearer token error at any step further, you need to obtain a new access token. It simply means the token has expired.\n Whoa, that was a lot of stuff to set up! The good news, all that can be easily automated. But it was important to execute this once manually to see what goes where. As we have done it, we are ready for some real action!\nIn the same terminal window where we exported the secret and user password, let’s export the token URL so the examples below are a little bit more concise.\n1  export KEYCLOAK_TOKEN_URL=http://127.0.0.1:28080/auth/realms/multi-customer/protocol/openid-connect/token    You can introspect your realm by going to\nhttp://localhost:28080/auth/realms/multi-customer/.well-known/openid-configuration/.\n Keep in mind, we haven’t assigned any roles to member@service-team user yet, other than what’s the default for Keycloak: offline_access and uma_authorization.\nLet’s see if we can obtain an access token:\n1 2 3 4  curl --silent -u customers:${KEYCLOAK_CLIENT_SECRET} \\  -k -d \"grant_type=password\u0026username=${USER_NAME}\u0026password=${USER_PASSWORD}\u0026scope=email profile\" \\  -H \"Content-Type:application/x-www-form-urlencoded\" \\  ${KEYCLOAK_TOKEN_URL} | jq '.' -r   The outcome should be similar to this:\n1 2 3 4 5 6 7 8 9 10  { \"access_token\": \"eyJhbGciOiJSUzI1NiIsIn...iWNlvIOVA\", \"expires_in\": 300, \"refresh_expires_in\": 1800, \"refresh_token\": \"eyJhbGciOiJIUzI1N...K6O5QoSDl_t2JA\", \"token_type\": \"bearer\", \"not-before-policy\": 0, \"session_state\": \"c0536fd1-35f3-4563-b1a4-006f59da7465\", \"scope\": \"profile email\" }   We will always need the value of the access token so further, we will export the access token as an environment variables directly from curl output using jq.\nOkay, let’s get another token then…\n1 2 3 4  export access_token=`curl --silent -u customers:${KEYCLOAK_CLIENT_SECRET} \\  -k -d \"grant_type=password\u0026username=${USER_NAME}\u0026password=${USER_PASSWORD}\u0026scope=email profile\" \\  -H \"Content-Type:application/x-www-form-urlencoded\" \\  ${KEYCLOAK_TOKEN_URL} | jq '.access_token' -r`   UMA tickets  From Wikipedia:\nUMA stands for User Managed Access and is an OAuth based access management protocol standard.\nIt enables a resource owner to control the authorization of data sharing and other protected-resource access made between online services on the owner’s behalf or with the owner’s authorization by an autonomous requesting party. 2\n We can now ask Keycloak to tell us which resources the user has access to.\nIn order to do so, we have to ask for an UMA token by sending a post request to the realm token endpoint with our existing access token and a special grant type: urn:ietf:params:oauth:grant-type:uma-ticket.\nThe audience parameter is required.\n1 2 3 4 5  curl --silent -X POST \\  ${KEYCLOAK_TOKEN_URL} \\  -H \"Authorization: Bearer ${access_token}\" \\  --data \"grant_type=urn:ietf:params:oauth:grant-type:uma-ticket\" \\  --data \"audience=customers\"   We receive the response:\n1  {\"error\":\"access_denied\",\"error_description\":\"not_authorized\"}   I know it does not look like it but this is a Great News!\nThe reason why we have received this answer is because we have removed the Default Resource and not assigned any customer roles to our user. Let’s change that.\nGo to the user roles (Manage / Users (left menu in Keycloak) / member@service-team / Role Mappings) and assign CustomerA role.\nObtain another access token:\n1 2 3 4  export access_token=`curl --silent -u customers:${KEYCLOAK_CLIENT_SECRET} \\  -k -d \"grant_type=password\u0026username=${USER_NAME}\u0026password=${USER_PASSWORD}\u0026scope=email profile\" \\  -H \"Content-Type:application/x-www-form-urlencoded\" \\  ${KEYCLOAK_TOKEN_URL} | jq '.access_token' -r`   and rerun the previous command:\n1 2 3 4 5  curl --silent -X POST \\  ${KEYCLOAK_TOKEN_URL} \\  -H \"Authorization: Bearer ${access_token}\" \\  --data \"grant_type=urn:ietf:params:oauth:grant-type:uma-ticket\" \\  --data \"audience=customers\" | jq '.'   The response is different! Better! We can look at what we are interested in:\n1 2 3 4 5 6 7 8 9  { \"upgraded\": false, \"access_token\": \"eyJhbGci...7I_pA\", \"expires_in\": 300, \"refresh_expires_in\": 1800, \"refresh_token\": \"eyJhbGc...QYko\", \"token_type\": \"Bearer\", \"not-before-policy\": 0 }   Copy the access_token from this response and decode it in jwt.io (or any other tool, it’s just three different base64 encoded strings concatenated with a dot). Look at realm_access and authorization claims. They are like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  \"realm_access\": { \"roles\": [ \"CustomerA\", \"offline_access\", \"uma_authorization\" ] }, \"authorization\": { \"permissions\": [ { \"scopes\": [ \"customer-a\" ], \"rsid\": \"715f6cc5-8ca7-44e4-a8ce-924493db76b1\", \"rsname\": \"CustomerA\" } ] },   This response tells us that the user behind the Bearer token is allowed access to CustomerA using customer-a scope. The realm_access.roles claim contains the CustomerA role but we get that info in a regular access token already.\nHowever, there is no CustomerB on this list.\nFair enough, let’s ask th server Keycloak directly if this user has access to CustomerB:\n1 2 3 4 5 6  curl --silent -X POST \\  ${KEYCLOAK_TOKEN_URL} \\  -H \"Authorization: Bearer ${access_token}\" \\  --data \"grant_type=urn:ietf:params:oauth:grant-type:uma-ticket\" \\  --data \"audience=customers\" \\  --data \"permission=CustomerB#customer-b\" | jq '.'   The response is:\n1 2 3 4  { \"error\": \"access_denied\", \"error_description\": \"not_authorized\" }   Correct! The user does not have access to the CustomerB. What if we don’t specify a scope?\n1 2 3 4 5 6  curl --silent -X POST \\  ${KEYCLOAK_TOKEN_URL} \\  -H \"Authorization: Bearer ${access_token}\" \\  --data \"grant_type=urn:ietf:params:oauth:grant-type:uma-ticket\" \\  --data \"audience=customers\" \\  --data \"permission=CustomerB\" | jq '.'   The response is:\n1 2 3 4  { \"error\": \"access_denied\", \"error_description\": \"not_authorized\" }   Correct again! But let’s verify that this indeed works for CustomerA, the user should have access:\n1 2 3 4 5 6  curl --silent -X POST \\  ${KEYCLOAK_TOKEN_URL} \\  -H \"Authorization: Bearer ${access_token}\" \\  --data \"grant_type=urn:ietf:params:oauth:grant-type:uma-ticket\" \\  --data \"audience=customers\" \\  --data \"permission=CustomerA\" | jq '.'   We receive:\n1 2 3 4 5 6 7 8 9  { \"upgraded\": false, \"access_token\": \"eyJhbGciOiJSUzI...7dlw\", \"expires_in\": 300, \"refresh_expires_in\": 1800, \"refresh_token\": \"eyJhbGciOi...QaM2ZA9nY1M\", \"token_type\": \"Bearer\", \"not-before-policy\": 0 }   Which is correct. Can the user request CustomerA resource with incorrect scope?\n1 2 3 4 5 6  curl --silent -X POST \\  ${KEYCLOAK_TOKEN_URL} \\  -H \"Authorization: Bearer ${access_token}\" \\  --data \"grant_type=urn:ietf:params:oauth:grant-type:uma-ticket\" \\  --data \"audience=customers\" \\  --data \"permission=CustomerA#customer-b\" | jq '.'   1 2 3 4  { \"error\": \"invalid_resource\", \"error_description\": \"Resource with id [CustomerA] does not exist.\" }   Correct, the resource does not exist with this scope! So let’s use the correct scope again, just to make sure everything is fine:\n1 2 3 4 5 6  curl --silent -X POST \\  ${KEYCLOAK_TOKEN_URL} \\  -H \"Authorization: Bearer ${access_token}\" \\  --data \"grant_type=urn:ietf:params:oauth:grant-type:uma-ticket\" \\  --data \"audience=customers\" \\  --data \"permission=CustomerA#customer-a\" | jq '.'   Once again, we get:\n1 2 3 4 5 6 7 8 9  { \"upgraded\": false, \"access_token\": \"eyJhbGciOiJSUzI1N...p3tFd4cjH1UAGOlY0g_U3b5Lj19zH4I3wkzA\", \"expires_in\": 300, \"refresh_expires_in\": 1800, \"refresh_token\": \"eyJhbGciOiJIUzI1N...FeEV8qFCVLM\", \"token_type\": \"Bearer\", \"not-before-policy\": 0 }   Phew. So far so good. Now, go to the user Role Mappings and assign CustomerB role. The user now has offline_access, uma_authorization, CustomerA and CustomerB roles assigned.\nWe require a new token:\n1 2 3 4  export access_token=`curl --silent -u customers:${KEYCLOAK_CLIENT_SECRET} \\  -k -d \"grant_type=password\u0026username=member@service-team\u0026password=${USER_PASSWORD}\u0026scope=email profile\" \\  -H \"Content-Type:application/x-www-form-urlencoded\" \\  ${KEYCLOAK_TOKEN_URL} | jq '.access_token' -r`   Do we now have the permission to access CustomerB? Well, let’s find out:\n1 2 3 4 5 6  curl --silent -X POST \\  ${KEYCLOAK_TOKEN_URL} \\  -H \"Authorization: Bearer ${access_token}\" \\  --data \"grant_type=urn:ietf:params:oauth:grant-type:uma-ticket\" \\  --data \"audience=customers\" \\  --data \"permission=CustomerB\" | jq '.'   Gives us:\n1 2 3 4 5 6 7 8 9  { \"upgraded\": false, \"access_token\": \"eyJhbGciOiJSUzI1NiIsI...n8AC51T1AMwDtoqfCEXrdwcrQ\", \"expires_in\": 300, \"refresh_expires_in\": 1800, \"refresh_token\": \"eyJhbGciOiJIUz...RG3zFus\", \"token_type\": \"Bearer\", \"not-before-policy\": 0 }   And with the scope?\n1 2 3 4 5 6  curl --silent -X POST \\  ${KEYCLOAK_TOKEN_URL} \\  -H \"Authorization: Bearer ${access_token}\" \\  --data \"grant_type=urn:ietf:params:oauth:grant-type:uma-ticket\" \\  --data \"audience=customers\" \\  --data \"permission=CustomerB#customer-b\" | jq '.'   1 2 3 4 5 6 7 8 9  { \"upgraded\": false, \"access_token\": \"eyJhbGciOiJSUzI1NiIsInR5cCIgOiAiSldUIiwia2...KxKyysheoaDgwRaVkyl158flOD5CtyHbjKFkWhA\", \"expires_in\": 300, \"refresh_expires_in\": 1800, \"refresh_token\": \"eyJhbGciOi...1tBGk6vebI\", \"token_type\": \"Bearer\", \"not-before-policy\": 0 }   Cool! Everything works. Our regular user can now see all customers he has been given access to.\nTop secret customer Eventually, we may decide that we should be able to discover all customer resources available in our Keycloak resource server. What would be unfortunate though, if our regular user could see customers who they should never be aware of.\nFrankly speaking, we could have a TopSecretCustomer in the system and nobody, ever, except of the TopSecretCustomer (and us) should be aware of their existence.\nKeycloak offers something called a resource set. Resource set allows us to introspect resources available on our resource server.\nIn order to access the resource set, the user must have the uma_protection role of the client assigned. Which is great. This implies we can just create a dedicated user with access to the resource set. Separation of concern at work! …\nLet’s do so.\nListing available customers Go to Manage / Users and click Add user once again. Set the following:\n Username: supervisor@company Email: supervisor@company First name: Supervisor Last name: Company User enabled and Email verified: on  Click Save.\nSet the password. Go to Credentials tab and set the password to password123!, Temporary: off. Click Set password. Now, go to Role Mappings and in the Client Roles, select customers client. Select uma_protection from Available roles and click Add selected.\nOn the command line, we can now list our customers. First, more environment variables to export.\n1 2 3  export ADMIN_USER_NAME=supervisor@company export ADMIN_USER_PASSWORD='password123!' export KEYCLOAK_RESOURCE_SET_URL=http://127.0.0.1:28080/auth/realms/multi-customer/authz/protection/resource_set    As with well known OpenID configuration, you can introspect well known UMA configuration by going to\nhttp://localhost:28080/auth/realms/multi-customer/.well-known/uma2-configuration/\n We need an access token for the supervisor user:\n1 2 3 4  export supervisor_access_token=`curl --silent -u customers:${KEYCLOAK_CLIENT_SECRET} \\  -k -d \"grant_type=password\u0026username=${ADMIN_USER_NAME}\u0026password=${ADMIN_USER_PASSWORD}\u0026scope=email profile\" \\  -H \"Content-Type:application/x-www-form-urlencoded\" \\  ${KEYCLOAK_TOKEN_URL} | jq '.access_token' -r`   The token received above is technically a protection API token (PAT). PAT is a special OAuth2 access token with a scope defined as uma_protection. 3\nWe can query for available customers like this:\n1 2 3  curl --silent \\  -H \"Authorization: Bearer ${supervisor_access_token}\" \\  ${KEYCLOAK_RESOURCE_SET_URL}?type=urn:customers:resources:customer | jq '.'   1 2 3 4  [ \"715f6cc5-8ca7-44e4-a8ce-924493db76b1\", \"00f34b81-c45b-4e28-b267-45fad4e48b4d\" ]   The above request queried the resource set with the type filter set to urn:customers:resources:customer.\n More about querying the resource set in Keycloak Authorization Services Guide, Managing Resources.\n Let’s check if our filter is working:\n1 2 3  curl --silent \\  -H \"Authorization: Bearer ${supervisor_access_token}\" \\  ${KEYCLOAK_RESOURCE_SET_URL}?type=urn:customers:resources:kitten | jq '.'   1  []   It is working! Let’s focus on the first response:\n1 2 3 4  [ \"715f6cc5-8ca7-44e4-a8ce-924493db76b1\", \"00f34b81-c45b-4e28-b267-45fad4e48b4d\" ]   We can query each individual resource:\n1 2 3  curl --silent \\  -H \"Authorization: Bearer ${supervisor_access_token}\" \\  ${KEYCLOAK_RESOURCE_SET_URL}/715f6cc5-8ca7-44e4-a8ce-924493db76b1 | jq '.'   Gives us:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  { \"name\": \"CustomerA\", \"type\": \"urn:customers:resources:customer\", \"owner\": { \"id\": \"95027cdf-4044-4622-bfdb-19ba8f7db65c\" }, \"ownerManagedAccess\": false, \"displayName\": \"Customer A Resource\", \"attributes\": {}, \"_id\": \"715f6cc5-8ca7-44e4-a8ce-924493db76b1\", \"uris\": [ \"/customers/CustomerA\" ], \"resource_scopes\": [ { \"name\": \"customer-a\" } ], \"scopes\": [ { \"name\": \"customer-a\" } ] }   and:\n1 2 3  curl --silent \\  -H \"Authorization: Bearer ${supervisor_access_token}\" \\  ${KEYCLOAK_RESOURCE_SET_URL}/00f34b81-c45b-4e28-b267-45fad4e48b4d | jq '.'   results in:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  { \"name\": \"CustomerB\", \"type\": \"urn:customers:resources:customer\", \"owner\": { \"id\": \"95027cdf-4044-4622-bfdb-19ba8f7db65c\" }, \"ownerManagedAccess\": false, \"displayName\": \"Customer B Resource\", \"attributes\": {}, \"_id\": \"00f34b81-c45b-4e28-b267-45fad4e48b4d\", \"uris\": [ \"/customers/CustomerB\" ], \"resource_scopes\": [ { \"name\": \"customer-b\" } ], \"scopes\": [ { \"name\": \"customer-b\" } ] }   We could now easily create an application to find and return all available Customers. All with Keycloak and without querying any database directly.\nConclusion Keycloak is a very versatile tool and can be easily used as a single source of truth for authentication, single sign-on and authorization within an organization. This article only touches a tip of an iceberg but it presents to the reader a real-world, useful scenario of using Keycloak as a driver for multi-tenant single sign-on.\nFurther reading  Authorization Services Guide  User Managed Access Managing Resources   User-Managed Access (UMA) 2.0 Grant for OAuth 2.0 Authorization    Keycloak with Docker Compose ↩︎\n https://en.wikipedia.org/wiki/User-Managed_Access ↩︎\n What is a PAT and how to obtain it ↩︎\n   ","description":"Investigating Keycloak Authorization Services using a real-world back office application scenario","tags":["keycloak","iam","multi-tenant","sso","uma"],"title":"Introduction to Keycloak Authorization Services","uri":"/posts/2020-09-05-introduction-to-keycloak-authorization-services/"},{"content":"managing director / software engineer @ Klarrio GmbH Currently employed at at Klarrio GmbH.\nSoftware engineer by trade. With over twenty years of experience specializing in distributed and back end systems: R\u0026D, development and operations. No stranger to the DevOps and CI/CD world. Author of open source tools.\nComputer polyglot who delivered production systems in multiple technologies, including: Scala, Erlang, golang, Java, Ruby, Python and JavaScript deployed in Amazon Web Services, Google Cloud Platform, OpenStack and SoftLayer. Advocate of functional programming. Combines technology and business oriented skills. Striving to deliver high quality work and automate as much as possible. Working with fully remote and on-site teams across different time zones.\npersonal ventures GR2 Systems UG Hermesstraße 39\n52156 Monschau\nDeutschland\nSteuer-Nr.: 202/5782/1478\nUSt-IdNr.: DE320574286\nAmtsgericht Aachen, HRB 22429\nSelf employment / Freiberuf Steuer-Nr.: 202/5134/2825\nUSt-IdNr.: DE306381795\n","description":"","tags":null,"title":"About Radek Gruchalski","uri":"/about/"},{"content":"Updated on 15th of May 2021 for Keycloak 13.0.0 with Postgres 13.2.\n6th of June 2021: Follow up: setting up Keycloak with TLS for local development.\nKeycloak is an open source Identity and Access Management System developed as a JBoss community project under the stewardship of Red Hat. Keycloak makes it is easy to secure apps and services written in many technologies using a large number client libraries.\nOut of the box, we get things like Single Sign-On, Identity Brokering and Social Login, User Federation and Authorization Services.\nWith little to no code, we can give users of our apps the ability to sign in with Identity Providers like GitHub, Twitter or Google. Well, anything that’s capable of talking OpenID or SAML. On the other hand, we can easily connect to existing LDAP or Active Directory servers to integrate with corporate services of this world.\nHere, I’m going to show how can we launch and configure a local Keycloak server to play with. The only dependency is docker with docker-compose.\nDocker Compose Docker Hub contains prebuilt Keycloak images. The version deployed here is 13.0.0.\nLet’s start with the compose.yml file.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  version:'3.9'services:postgres:image:postgres:13.2restart:unless-stoppedenvironment:POSTGRES_DB:${POSTGRESQL_DB}POSTGRES_USER:${POSTGRESQL_USER}POSTGRES_PASSWORD:${POSTGRESQL_PASS}networks:- local-keycloakkeycloak:depends_on:- postgrescontainer_name:local_keycloakenvironment:DB_VENDOR:postgresDB_ADDR:postgresDB_DATABASE:${POSTGRESQL_DB}DB_USER:${POSTGRESQL_USER}DB_PASSWORD:${POSTGRESQL_PASS}image:jboss/keycloak:${KEYCLOAK_VERSION}ports:- \"28080:8080\"restart:unless-stoppednetworks:- local-keycloaknetworks:local-keycloak:  Next to the compose.yml file, we need the .env file.\nKEYCLOAK_VERSION=13.0.0 PORT_KEYCLOAK=8080 POSTGRESQL_USER=keycloak POSTGRESQL_PASS=keycloak POSTGRESQL_DB=keycloak That’s it, we can now start our Keycloak:\ndocker-compose -f compose.yml up After a short moment, we can go to Keycloak landing page using the local address http://localhost:28080. Keycloak will welcome us with this page:\nIntermission Before we move on, let’s quickly figure out what has happened so far.\n With Docker Compose, we have started a Keycloak server with PostgreSQL 13.2 as a database. PostgreSQL protocol is at a stable version 3 since PostgeSQL 8 days so it does not really matter which newer version is used. Using the .env file, we have specified that we want Keycloak 13.0.0 and our Keycloak shall connect to Postgres using keycloak username and keycloak password as the credential. The database used by Keycloak is also called keycloak. The same variables are used for the postgres service. The Postgres container will automagically create a user identified by POSTGRES_USER variable, authenticated by the value of POSTGRES_PASSWORD. We have specified POSTGRES_DB so the container created the database and configured access for our new user. Both containers run in the same bridge network called local-keycloak. Actually, in Docker it’s called something else:  [rad] dev-keycloak $ docker network ls | grep keycloak 920dd184892c dev-keycloak_local-keycloak bridge local The name of the network is essentially:\nbasename $(pwd) + \u003cname-from-compose.yml\u003e We have exposed the 28080 port to the host so we can reach Keycloak from the browser. Finally, we named the Keycloak container as local_keycloak. We will use this name shortly.  Administrator account Okay, so Keycloak is running but we can’t do anything with it because we need to create an Administrator account. That we can also do with Docker.\nWhile the compose setup is running, run this in your terminal:\ndocker exec local_keycloak \\ /opt/jboss/keycloak/bin/add-user-keycloak.sh \\ -u admin \\ -p admin \\ \u0026\u0026 docker restart local_keycloak Once this command finishes, you will see that the compose local_keycloak is going to restart. Give it a short moment and reload the Keycloak landing page.\nClick Administration Console link and sign in with admin as the username and admin as the password. Welcome to Keycloak.\nFurther reading  Keycloak Documentation Janua Technical Blog  Keycloak source code can be found on GitHub.\n","description":"Start and configure Keycloak with Docker Compose","tags":["keycloak","iam","idp","sso","docker"],"title":"Keycloak With Docker Compose","uri":"/posts/2020-09-03-keycloak-with-docker-compose/"},{"content":"First, create an account on the Sonatype JIRA, unless you have one. For the new group ID, create a ticket using the form under this URI. Once requested, wait for the ticket to go into Resolved state. When this happens, you can publish your project to Sonatype.\nTo do so, configure your SBT project. Add the following lines to your project/plugins.sbt:\n1 2  addSbtPlugin(\"org.xerial.sbt\" % \"sbt-sonatype\" % \"1.1\") addSbtPlugin(\"com.jsuereth\" % \"sbt-pgp\" % \"1.0.0\")   Configure Sonatype credentials for sbt-sonatype, create the ~/.sbt/0.13/sonatype.sbt file with the contents like:\n1 2 3 4  credentials += Credentials(\"Sonatype Nexus Repository Manager\", \"oss.sonatype.org\", \"your-username\", \"your-sonatype-password\")   Add the following to your build.sbt:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  organization := \"uk.co.appministry\" // set to whatever you requested as an organization in the form publishMavenStyle := true pomIncludeRepository := { _ =\u003e false } pomExtra := ( \u003curl\u003ehttps://github.com/AppMinistry/scathon\u003c/url\u003e  \u003clicenses\u003e \u003clicense\u003e \u003cname\u003eApache 2\u003c/name\u003e \u003curl\u003ehttp://www.apache.org/licenses/LICENSE-2.0.txt\u003c/url\u003e  \u003c/license\u003e \u003c/licenses\u003e \u003cscm\u003e \u003cconnection\u003escm:git:git://github.com/AppMinistry/scathon.git\u003c/connection\u003e \u003cdeveloperConnection\u003escm:git:git://github.com/AppMinistry/scathon.git\u003c/developerConnection\u003e \u003curl\u003ehttps://github.com/AppMinistry/scathon/tree/master\u003c/url\u003e  \u003c/scm\u003e \u003cdevelopers\u003e \u003cdeveloper\u003e \u003cid\u003eradekg\u003c/id\u003e \u003cname\u003eRadoslaw Gruchalski\u003c/name\u003e \u003curl\u003ehttp://gruchalski.com\u003c/url\u003e  \u003c/developer\u003e \u003c/developers\u003e )   Obviously, make sure you provide the details correct to your project: URL, license details, SCM details, developers data and so on. For the comprehensive explanation, look here.\nIn order to publish to Sonatype, we need gnupg installed. This is for macOS:\n$ brew install gnupg gnupg2  Generate a key pair, use the defaults, set the expiry to some same value - the recommended value is less than 2 years. Also, consider setting a passphrase!\n$ gpg --gen-key  Check the keyid of the newly generated key:\n$ gpg --list-keys /Users/rad/.gnupg/pubring.gpg ----------------------------- pub 2048R/6664767F 2017-01-26 [expires: 2019-01-26] uid [ultimate] Radoslaw Gruchalski \u003cradek@gruchalski.com\u003e sub 2048R/C0C1DF05 2017-01-26 [expires: 2019-01-26]  The keyid in the output above is 6664767F. Send this key to the key server used by Sonatype:\n$ gpg --keyserver hkp://pgp.mit.edu --send-keys 6664767F  Give it a few minutes to propagate your key across different machines. It’ll get a little bit confusing otherwise…\nFinally, deploy the artifacts to the staging repository:\n$ sbt publishSigned  A valid output should be look like this:\n[rad] scathon (master) $ sbt publishSigned [info] Loading global plugins from /Users/rad/.sbt/0.13/plugins [info] Updating {file:/Users/rad/.sbt/0.13/plugins/}global-plugins... [info] Resolving org.fusesource.jansi#jansi;1.4 ... [info] Done updating. [info] Loading project definition from /Users/rad/dev/my/scathon/project [info] Set current project to scathon (in build file:/Users/rad/dev/my/scathon/) [info] Packaging /Users/rad/dev/my/scathon/scathon-client/target/scala-2.11/scathon-client_2.11-0.2.1-sources.jar ... [info] Packaging /Users/rad/dev/my/scathon/scathon-models/target/scala-2.11/scathon-models_2.11-0.2.1-sources.jar ... [info] Packaging /Users/rad/dev/my/scathon/scathon-testServer/target/scala-2.11/scathon-testserver_2.11-0.2.1-sources.jar ... [info] Done packaging. [info] Done packaging. [info] Done packaging. [info] Wrote /Users/rad/dev/my/scathon/scathon-models/target/scala-2.11/scathon-models_2.11-0.2.1.pom [info] :: delivering :: uk.co.appministry#scathon_2.11;0.2.1 :: 0.2.1 :: release :: Mon Jan 30 23:34:00 GMT 2017 [info] delivering ivy file to /Users/rad/dev/my/scathon/target/scala-2.11/ivy-0.2.1.xml [info] :: delivering :: uk.co.appministry#scathon-models_2.11;0.2.1 :: 0.2.1 :: release :: Mon Jan 30 23:34:00 GMT 2017 [info] delivering ivy file to /Users/rad/dev/my/scathon/scathon-models/target/scala-2.11/ivy-0.2.1.xml [info] Wrote /Users/rad/dev/my/scathon/scathon-testServer/target/scala-2.11/scathon-testserver_2.11-0.2.1.pom [info] Wrote /Users/rad/dev/my/scathon/scathon-client/target/scala-2.11/scathon-client_2.11-0.2.1.pom [info] Main Scala API documentation to /Users/rad/dev/my/scathon/scathon-models/target/scala-2.11/api... [info] :: delivering :: uk.co.appministry#scathon-testserver_2.11;0.2.1 :: 0.2.1 :: release :: Mon Jan 30 23:34:00 GMT 2017 [info] delivering ivy file to /Users/rad/dev/my/scathon/scathon-testServer/target/scala-2.11/ivy-0.2.1.xml [info] :: delivering :: uk.co.appministry#scathon-client_2.11;0.2.1 :: 0.2.1 :: release :: Mon Jan 30 23:34:00 GMT 2017 [info] delivering ivy file to /Users/rad/dev/my/scathon/scathon-client/target/scala-2.11/ivy-0.2.1.xml [info] Main Scala API documentation to /Users/rad/dev/my/scathon/scathon-testServer/target/scala-2.11/api... [info] Packaging /Users/rad/dev/my/scathon/scathon-models/target/scala-2.11/scathon-models_2.11-0.2.1.jar ... [info] Done packaging. [info] Packaging /Users/rad/dev/my/scathon/scathon-testServer/target/scala-2.11/scathon-testserver_2.11-0.2.1.jar ... [info] Main Scala API documentation to /Users/rad/dev/my/scathon/scathon-client/target/scala-2.11/api... [info] Done packaging. [info] Packaging /Users/rad/dev/my/scathon/scathon-client/target/scala-2.11/scathon-client_2.11-0.2.1.jar ... [info] Done packaging. model contains 16 documentable templates [warn] there was one feature warning; re-run with -feature for details model contains 17 documentable templates [info] Main Scala API documentation successful. [info] Packaging /Users/rad/dev/my/scathon/scathon-client/target/scala-2.11/scathon-client_2.11-0.2.1-javadoc.jar ... [info] Done packaging. Please enter PGP passphrase (or ENTER to abort): *************************** [warn] there was one feature warning; re-run with -feature for details [warn] one warning found [info] Main Scala API documentation successful. [info] Packaging /Users/rad/dev/my/scathon/scathon-testServer/target/scala-2.11/scathon-testserver_2.11-0.2.1-javadoc.jar ... [info] Done packaging. model contains 157 documentable templates [warn] one warning found [info] Main Scala API documentation successful. [info] Packaging /Users/rad/dev/my/scathon/scathon-models/target/scala-2.11/scathon-models_2.11-0.2.1-javadoc.jar ... [info] Done packaging. [info] published scathon-client_2.11 to https://oss.sonatype.org/service/local/staging/deploy/maven2/uk/co/appministry/scathon-client_2.11/0.2.1/scathon-client_2.11-0.2.1-javadoc.jar [info] published scathon-client_2.11 to https://oss.sonatype.org/service/local/staging/deploy/maven2/uk/co/appministry/scathon-client_2.11/0.2.1/scathon-client_2.11-0.2.1-sources.jar.asc [info] published scathon-client_2.11 to https://oss.sonatype.org/service/local/staging/deploy/maven2/uk/co/appministry/scathon-client_2.11/0.2.1/scathon-client_2.11-0.2.1.jar.asc [info] published scathon-client_2.11 to https://oss.sonatype.org/service/local/staging/deploy/maven2/uk/co/appministry/scathon-client_2.11/0.2.1/scathon-client_2.11-0.2.1-javadoc.jar.asc [info] published scathon-client_2.11 to https://oss.sonatype.org/service/local/staging/deploy/maven2/uk/co/appministry/scathon-client_2.11/0.2.1/scathon-client_2.11-0.2.1-sources.jar [info] published scathon-client_2.11 to https://oss.sonatype.org/service/local/staging/deploy/maven2/uk/co/appministry/scathon-client_2.11/0.2.1/scathon-client_2.11-0.2.1.pom [info] published scathon-client_2.11 to https://oss.sonatype.org/service/local/staging/deploy/maven2/uk/co/appministry/scathon-client_2.11/0.2.1/scathon-client_2.11-0.2.1.pom.asc [info] published scathon-client_2.11 to https://oss.sonatype.org/service/local/staging/deploy/maven2/uk/co/appministry/scathon-client_2.11/0.2.1/scathon-client_2.11-0.2.1.jar [info] published scathon-models_2.11 to https://oss.sonatype.org/service/local/staging/deploy/maven2/uk/co/appministry/scathon-models_2.11/0.2.1/scathon-models_2.11-0.2.1.pom.asc [info] published scathon-models_2.11 to https://oss.sonatype.org/service/local/staging/deploy/maven2/uk/co/appministry/scathon-models_2.11/0.2.1/scathon-models_2.11-0.2.1-sources.jar.asc [info] published scathon-models_2.11 to https://oss.sonatype.org/service/local/staging/deploy/maven2/uk/co/appministry/scathon-models_2.11/0.2.1/scathon-models_2.11-0.2.1-javadoc.jar.asc [info] published scathon-models_2.11 to https://oss.sonatype.org/service/local/staging/deploy/maven2/uk/co/appministry/scathon-models_2.11/0.2.1/scathon-models_2.11-0.2.1.jar.asc [info] published scathon-models_2.11 to https://oss.sonatype.org/service/local/staging/deploy/maven2/uk/co/appministry/scathon-models_2.11/0.2.1/scathon-models_2.11-0.2.1-javadoc.jar [info] published scathon-models_2.11 to https://oss.sonatype.org/service/local/staging/deploy/maven2/uk/co/appministry/scathon-models_2.11/0.2.1/scathon-models_2.11-0.2.1.jar [info] published scathon-models_2.11 to https://oss.sonatype.org/service/local/staging/deploy/maven2/uk/co/appministry/scathon-models_2.11/0.2.1/scathon-models_2.11-0.2.1.pom [info] published scathon-models_2.11 to https://oss.sonatype.org/service/local/staging/deploy/maven2/uk/co/appministry/scathon-models_2.11/0.2.1/scathon-models_2.11-0.2.1-sources.jar [info] published scathon-testserver_2.11 to https://oss.sonatype.org/service/local/staging/deploy/maven2/uk/co/appministry/scathon-testserver_2.11/0.2.1/scathon-testserver_2.11-0.2.1-javadoc.jar [info] published scathon-testserver_2.11 to https://oss.sonatype.org/service/local/staging/deploy/maven2/uk/co/appministry/scathon-testserver_2.11/0.2.1/scathon-testserver_2.11-0.2.1-sources.jar [info] published scathon-testserver_2.11 to https://oss.sonatype.org/service/local/staging/deploy/maven2/uk/co/appministry/scathon-testserver_2.11/0.2.1/scathon-testserver_2.11-0.2.1.pom [info] published scathon-testserver_2.11 to https://oss.sonatype.org/service/local/staging/deploy/maven2/uk/co/appministry/scathon-testserver_2.11/0.2.1/scathon-testserver_2.11-0.2.1.jar [info] published scathon-testserver_2.11 to https://oss.sonatype.org/service/local/staging/deploy/maven2/uk/co/appministry/scathon-testserver_2.11/0.2.1/scathon-testserver_2.11-0.2.1-sources.jar.asc [info] published scathon-testserver_2.11 to https://oss.sonatype.org/service/local/staging/deploy/maven2/uk/co/appministry/scathon-testserver_2.11/0.2.1/scathon-testserver_2.11-0.2.1.jar.asc [info] published scathon-testserver_2.11 to https://oss.sonatype.org/service/local/staging/deploy/maven2/uk/co/appministry/scathon-testserver_2.11/0.2.1/scathon-testserver_2.11-0.2.1.pom.asc [info] published scathon-testserver_2.11 to https://oss.sonatype.org/service/local/staging/deploy/maven2/uk/co/appministry/scathon-testserver_2.11/0.2.1/scathon-testserver_2.11-0.2.1-javadoc.jar.asc [success] Total time: 35 s, completed Jan 30, 2017 11:34:35 PM  If I was to publish this project for Scala 2.12, I’d do the following:\nSCALA_VERSION=2.12.1 $ sbt ++$SCALA_VERSION -Dscala.version=$SCALA_VERSION publishSigned  And release:\n$ sbt sonatypeRelease  The output should look like:\n$ sbt sonatypeRelease [info] Loading global plugins from /Users/rad/.sbt/0.13/plugins [info] Loading project definition from /Users/rad/dev/my/scathon/project [info] Set current project to scathon (in build file:/Users/rad/dev/my/scathon/) [info] Nexus repository URL: https://oss.sonatype.org/service/local [info] sonatypeProfileName = uk.co.appministry [info] Reading staging repository profiles... [info] Reading staging profiles... [info] Closing staging repository [ukcoappministry-1008] status:open, profile:uk.co.appministry(439d1bb6b3d35e) description: Implicitly created (auto staging). [info] Activity open started:2017-01-30T23:34:14.860Z, stopped:2017-01-30T23:34:18.364Z [info] repositoryCreated: id:ukcoappministry-1008, user:radekg, ip:92.79.39.101 [info] Activity close started:2017-01-30T23:38:12.624Z, stopped: [info] Evaluate: id:5e9e8e6f8d20a3, rule:sources-staging [info] Evaluate: checksum-staging [info] Passed: checksum-staging [info] Evaluate: signature-staging [info] Passed: signature-staging [info] Evaluate: sources-staging [info] Passed: sources-staging [info] Evaluate: javadoc-staging [info] Passed: javadoc-staging [info] Evaluate: pom-staging [info] Passed: pom-staging [info] Passed: id:5e9e8e6f8d20a3 [info] email: to:radek@gruchalski.com [info] repositoryClosed: id:ukcoappministry-1008 [info] Closed successfully [info] Promoting staging repository [ukcoappministry-1008] status:closed, profile:uk.co.appministry(439d1bb6b3d35e) description: Implicitly created (auto staging). [info] Activity release started:2017-01-30T23:38:26.336Z, stopped: [info] Evaluate: id:5e9e8e6f8d20a3, rule:sources-staging [info] Evaluate: sources-staging [info] Passed: sources-staging [info] Evaluate: javadoc-staging [info] Passed: javadoc-staging [info] Evaluate: signature-staging [info] Passed: signature-staging [info] Evaluate: pom-staging [info] Passed: pom-staging [info] Evaluate: checksum-staging [info] Passed: checksum-staging [info] Passed: id:5e9e8e6f8d20a3 [info] Evaluate: id:nx-internal-ruleset, rule:RepositoryWritePolicy [info] Evaluate: RepositoryWritePolicy [info] Passed: RepositoryWritePolicy [info] Passed: id:nx-internal-ruleset [info] copyItems: source:ukcoappministry-1008, target:releases [info] email: to:radek@gruchalski.com [info] repositoryReleased: id:ukcoappministry-1008, target:releases [info] Promoted successfully [info] Dropping staging repository [ukcoappministry-1008] status:released, profile:uk.co.appministry(439d1bb6b3d35e) description: Implicitly created (auto staging). [info] Dropped successfully: ukcoappministry-1008 [info] Set current project to scathon (in build file:/Users/rad/dev/my/scathon/)  If this was a first release with a given organization, go back to your Sonatype JIRA ticket and let them know you’ve released your first version. The Maven Central sync will be enabled for your organization. It might take a couple of hours before you see your changes in Central.\nThat’s about it.\nSome interesting links:\n OSSRH Guide Sonatype Requirements Cake Solutions Publishing SBT projects to Nexus sbt-sonatype on Github: have a look at the readme  ","description":"Publish to Maven Central via Sonatype with SBT","tags":["sbt","maven","central","sonatype"],"title":"Publish to Maven Central via Sonatype with SBT","uri":"/posts/2017-01-31-publish-to-maven-central-via-sonatype-with-sbt/"},{"content":"It is entirely possible that what I am going to describe here is an edge case not many people hit with their Kafka deployments. However, in my experience, when Kafka is used to ingest large volumes of data, it makes perfect sense. Considering that every now and then people ask for a cold storage feature on the Kafka mailing list, I am not the only one who would find this useful.\nApache Kafka According to the Kafka website: Apache Kafka is publish-subscribe messaging rethought as a distributed commit log. It is fast: it can handle hundreds of megabytes of data from thousands of clients. It is durable: Kafka persists the data on disk and provides data replication to ensure no data loss. It is scalable: Kafka systems can be scaled without a hassle.\nTo understand where the need for cold storage comes from, it is important to understand how Kafka system works and what trade-offs a Kafka based system needs to deal with.\nKafka stores all data on disk. The servers running Kafka are limited by the amount of storage they can handle. It is always an arbitrary number defined by the amount of storage available to the single Kafka server. Single Kafka node within the cluster may be responsible for one or more topics. Each topic is stored in files called segments. All segments of a topic have an index file describing which parts of the queue reside in which segment. The segment size is configurable. The total amount of data kept for a topic is either the cumulative size of the topic or the oldest message to be kept. For example, a value of two weeks or 500GB of data; depending on which is reached first, that value will be used to determine how much data to keep. If a segment size for such topic is 1GB, the maximum of 500 of segments will be kept.\nThe process presented above applies to a regular topic. This kind of topic stores all the data in sequence. Kafka also offers compacted topics. These are more like key / value store.\nThe process of cleaning up old segments is called log compaction. In case of regular topics, log compaction will remove all segments falling out of range of data to be kept. In case of compacted topics, only the most recent value for a given key is kept.\nThe cold storage case applies only to regular topics.\nThe problem The problem with regular topics, as they are implemented today in Kafka, is that, it is not possible to move excess segments out from under Kafka management without restarting Kafka broker.\nConsider a data processing system using Kafka ingesting hundreds of megabytes of data every second. Such system will have a certain number of servers, call them terminators, used by millions of devices / applications as a connection point. These servers accept the data, they don’t do any processing, just put the data in Kafka. The volume of data is so large that the system needs a certain trade-off: how much data should be kept in Kafka and what to do with historical data? Historical data is important. It needs to be stored somewhere in a format ready for post-processing in case of, for example, having to introduce a new feature during the life-time of a product.\nToday, most of the implementations solve this problem in the following way: the raw data is ingested with Kafka. Some processes work with that data by reading Kafka topics and crunching for real-time dashboards and whatnot. There is also a process somewhere which exists only for the purpose of reading all ingested data out of Kafka and putting it in the raw format in external storage. S3 or Swift Object Store come to mind.\nThere are two drawbacks of such solution. First: the storage of the raw data is basically another format. It needs naming rules to know how to access the data in the future, compression, transport mechanism, verification, replication. Second: the data is already in Kafka, so why the need for consuming it out for Kafka, putting the load on the system and using additional processing power for applying compression and moving the data out to external storage?\nCold storage Kafka comes with a feature which allows it to build the index file from an arbitrary segment file. What this means is: instead of consuming the topic and uploading raw data to external storage, it is possible to move a segment file as it is persisted on disk. The data in the topic is still stored in its raw format, albeit inside of a segment file. The advantage of storing segments files is such that there is no additional cost of consuming for cold storage purpose and no cost in feeding back to Kafka for reprocessing. One can simply download the segment file and use Kafka libraries—no need to run Kafka cluster at all—to read data out. Such segment files can be processed in parallel with Storm, Spark, Hadoop, or any other sufficient tool.\nImplementation Nothing stops anyone from doing this today. The simplest way is to have a program running on the Kafka machine which would check when the old segments are closed, copy them to external storage and let Kafka simply deal with old segments as it does now. This is, however, another one of those “roll it out yourself” approaches. Kafka could help. There are two possible options I can think of.\nFirst approach: if Kafka provided a notification mechanism and could trigger a program when a segment file is to be discarded, it would become feasible to provide a standard method of moving data to cold storage in reaction to those events. Once the program finishes backing the segments up, it could tell Kafka “it is now safe to delete these segments”.\nThe second option is to provide an additional value for the log.cleanup.policy setting, call it cold-storage. In case of this value, Kafka would move the segment files—which otherwise would be deleted—to another destination on the server. They can be picked up from there and moved to the cold storage.\nBoth of these options ensure that Kafka data can be archived without having to interfere with Kafka process itself. Considering data replication features in Kafka, the former method seems more plausible. It would free the cluster operator from having to track file system changes. Furthermore, it could be implemented in such a way that, if there are any listeners waiting for those events for a given topic, Kafka switches to this mode automatically. It does put a responsibility on the operator to ensure flawless execution of the archive process but it is an opt-in mechanism—the operator is aware of the behavior.\nConclusion A well defined, standardized method for moving Kafka segments to the cold storage would significantly improve the availability and processing of historical data.\nNotes This article has been published on Medium and LinkedIn. 1 2\n  Medium ↩︎\n LinkedIn ↩︎\n   ","description":"Investigation of available options for running and operating a Certificate Authority for modern applications","tags":["apache","kafka"],"title":"The case for Kafka cold storage","uri":"/posts/2016-05-08-the-case-for-kafka-cold-storage/"},{"content":"About two weeks ago, Virdata released a set of patches for Apache Spark enabling Spark to work on Mesos with Docker bridge networking. We are using these in production for our multi tenant Spark environment.\nSPARK-11638: Spark patches All patches for all components described below are available in Spark JIRA. We’ve released patches for all versions of Spark available at the time of creating them - from 1.4.0 to 1.5.2. We have also released patches for Akka 2.3.4 and Akka 2.3.11; these are required to make this solution usable.\nWhat has not been released with the patches is the complete minimum example of how these can be verified. This post serves as a missing part.\nAll instructions on how to build Spark and Akka with patches applied can be found at the end of this post.\nScenario and sources The scenario: setting up Spark Notebook running in Docker container, bridge networking on Mesos. Spark notebook, once a document is opened, starts Apache Spark Master process. Executing Spark code, schedules tasks on the same Mesos the notebook server is running on.\nThe observed outcome: executors communicate back to the master, inside of the Docker container.\nAll programs discussed below are available on github. Let’s go through a short summary of what is happening…\nSetting up the machine For the simplicity, I assume clean Ubuntu 14.04 installation. These instructions should work without any issues on EC2, SoftLayer or any other cloud provider.\nMy VM is a 4 CPUs, 8GB RAM VMWare Fusion machine.\nFirst, the machine has to be set up:\n checkout the repository and put them in ~ on your test machine (cd ~ \u0026\u0026 git clone https://github.com/virdata/mesos-spark-docker-bridge.git .) build relevant Spark version with the patch from Spark JIRA (all examples assume 1.5.1) build relevant Akka version for your Spark version (in case of this example it always 2.3.11, spark-notebook excludes any other Akka and assumes 2.3.11) put the akka-remote_2.10-2.3.11.jar, result of the build, in ~ put spark-core_2.10-1.5.1.jar and spark-repl_2.10-1.5.1.jar, result of the build, in ~ execute setup.sh  The most important things to know:\n OpenJDK 7 will be used Mesosphere apt repo will be added Mesos 0.24.1 with Marathon 0.10.1 is used Mesos agent runs on the same machine as Mesos master latest docker is installed local installation of ZooKeeper is used /etc/mesos.agent.sh file is created, explained below Spark stock release is downloaded and stored on the machine to be used for executors patched Akka and spark JARs required only for the master, executors can happily use the stock release  /etc/mesos.agent.sh The container has to know what is the IP address of the Mesos agent the container’s task is running on. Mesos, nor Marathon, currently does not provide this information. When a task is scheduled, there is no way to know which agent the task is going to end up on until the task is running. This make it impossible to give the agent’s IP at task submission time. /etc/mesos.agent.sh is given to the container as a Docker volume. This file needs to exist on every agent node. In this example, it is not really necessary. The master and agent are the same node. The file is provided for clarity and makes it possible to use multi node setup.\nThis step takes takes about 15 minutes on my VMWare Fusion VM. When finished, Mesos UI and Marathon UI should be reachable:\n Mesos: http://mesos:5050 Marathon: http://mesos:8080  mesos host may have to be added /etc/hosts file.\nSpark notebook Docker image In theory, a snapshot version of Data Fellas notebook should work but, if there are any doubts, build-docker-image.sh program should be used to get an upstream version of the Docker image.\nThe most important things to know:\n git master version of the code is used base image changed to ubuntu:trusty and entrypoint changed so it is possible to run the container with docker -ti ... /bin/bash  This will take about half an hour to build. The andypetrella/spark-notebook:0.6.2-SNAPSHOT-scala-2.10.4-spark-x.x.x-hadoop-2.4.0 image should be on the list after executing docker images command.\nRun ~/run-task.sh This will request a Mesos task with Marathon. There are 6 ports being requested, the order:\n 9000: Spark notebook server user interface 9050: LIBPROCESS_PORT 6666: spark.driver.port 6677: spark.fileserver.port 6688: spark.broadcast.port 6699: spark.replClassServer.port  The command used in the container is $MESOS_SANBOX/run-notebook.sh. What happens inside is the following:\n source /etc/mesos.agent.sh to learn the IP address of the host agent set LIBPROCESS_* properties, the advetise IP is set to the host IP and advertise port is $PORT_9050 (the port assigned by Mesos); Mesos will now correctly return offers to the container SPARK_LOCAL_IP is set to the container’s IP address SPARK_PUBLIC_DNS and SPARK_LOCAL_HOSTNAME are set to the hostname of the agent next, agent’s hostname is added to the container’s /etc/hosts file; this makes it possible for Spark to bind all services to the agent hostname so it can be correctly advetised to the executors; for the spark.driver.port / spark.driver.advertisedPort, the akka patch is required CLASSPATH_OVERRIDES contains 3 JARs: patched akka-remote, patched spark-core and spark-repl; this env variable was added to spark-notebook just to make these patches work; the most important thing to know: these 3 JARs have to be on the class path before any other Spark / Akka jars; if loaded first, the JVM will use them over any other classes from any other JARs side note: Zeppelin has recently added support for ZEPPELIN_CLASSPATH_OVERRIDES environment variable  The final step is starting the notebook server. Worth noticing are all settings ending with advertisedPort. This is the main part of the Spark patch.\nTo verify Create a new notebook and execute the following:\nval rdd = sparkContext.parallelize(Seq(1,2)) rdd.count()  Rather simple but… The executors will be scheduled on the same Mesos cluster the notebook server is running on. The executors will successfully publish the results bask to the master running in the Docker container.\nWith the fine-grained mode, it sometimes happes that the tasks are stuck in STAGING for a long time and the whole job fails. If this is the case during the test, repeat the test with:\nspark.mesos.coarse = true  using notebook metadata. It is important to remember that notebooks saved on one server carry over the Spark configuration in them. As such, it is not safe to import a notebook from another notebook server to execute the test. Please use a new, clean notebook.\nPerfomance-wise, no real numbers to share but we are getting within ~95% of the usual setup with coarse mode.\nComponents Patched Akka AKKA_VERSION=2.3.11 git clone https://github.com/akka/akka.git cd akka git fetch origin git checkout v${AKKA_VERSION} git apply path/to/${AKKA_VERSION}.patch sbt package -Dakka.scaladoc.diagrams=false  Patched Spark SPARK_VERSION=1.5.1 git clone github.com/apache/spark.git cd spark git fetch origin git checkout v{$SPARK_VERSION} # from the tag git apply path/to/${SPARK_VERSION}.patch ./make-distribution ...  Akka versions  2.3.4: Spark 1.4.0 Spark 1.4.1 2.3.11 Spark 1.5.0 Spark 1.5.1 Spark 1.5.2  Comments? Thougths? Thoughts and comments are appreciated! Please use github issues for communication.\nHappy coding!\n","description":"Apache Spark on Mesos with Docker bridge networking","tags":["spark","mesos","marathon","docker"],"title":"Apache Spark on Mesos with Docker bridge networking","uri":"/posts/2015-11-23-apache-spark-on-mesos-with-docker-bridge-networking/"},{"content":"Wow. It’s difficult to believe it’s been almost a week since I gave a talk about gossip protocols at Erlang User Conference in Stockholm. It was a fantastic event, great agenda, great topics, fantastic networking. EUC is one of those events you should attend, you will not regret it. No matter if you are interested in Erlang/Elixir or not. Big “Thank You” to all who made it happen.\nEUC2015 was also a place of first public appearance of Gossiperl. This was the first time I was giving a public talk to such an audience, I’m not the best at this. Judging by the number of people attending my talk, the subject was good. Judging by the number of people who were there when I was wrapping up, I wasn’t terrible. I do hope that people got the value out of my 45 minutes. And finally, I was able to show Gossiperl.\nAt the end of my talk I’ve presented a couple of very simple demos:\n event subscribe / dissemination: events arriving only at interested nodes Gossiperl running over multicast on Raspberry Pi cluster  These are the very basic Gossiperl services covered in the documentation and the core of what Gossiperl is about. The core is ready. So, what are the next steps?\n  Top priority: Raft, in order to make Gossiperl fully data center aware. The idea behind this one is to make the data centers (racks in Gossiperl notion) exchange the data between the physical locations only via seeds. This will ensure that the physical locations communicate via established, dedicated, streamlined channel. Currently, all members from different locations communicate with each other. This isn’t very effective because of the latency. As explained in my talk, it is not desired to have overlay members in, say, Oregon talk to members, say, in Singapore. The latency is expected but it should be controllable. This will be achieved with Raft. Only the seeds of those locations will talk to each other. The members of respective locations will talk only to local seeds.\n  Priority number two: subscriptions. Gossiperl subscription message redelivery can be configured in two different ways. Either the delivery will be attempted selected number of times or indefinitely. In case of a fixed number, if a message can’t be delivered, the sender will never become aware of this fact. This problem will be addressed by solving the following issue.\n  Number three: subscriptions, again. Well, kind of. Ability to attach a luggage to the digest. Such information will be used to build an initial overview of the overlay with application specific data. Similar to membership but domain specific. Subscriptions are available only to members who already participate in an overlay. Any new joiners miss on this data and will never receive it. It’s possible to build a mechanism to resend such information but it is not straightforward. For data which should be available to all new joiners luggage is the answer.\n  Next one is exometer. Exometer is de facto the standard for metrics in Erlang world. Like JMX for JVM. Currently only InfluxDB connector is available out of the box and exometer needs to be supported.\n  After that comes an easy one. The only encryption algorithm available (and hard coded) is AES-256. This will become a configurable option with pluggable behaviours. Gossiperl will ship with configuration for AES-128, AES-192 and AES-256. Other algorithms can still be added and selected on per overlay basis.\n  The final one is: user interface improvements. Gossiperl ships with a REST API. Next to the REST API a web interface will be available. All management operations must be accessible through the browser and statistics will be streamed via web sockets.\n  Here we are. I’m sure there will be more.\nThis post would not be complete without words of appreciation to my managers at Virdata who supported me in this endeavour. You know who you are, thank you.\n","description":"","tags":["gossiperl","euc2015"],"title":"Gossiperl at EUC 2015 and next steps","uri":"/posts/2015-06-17-gossiperl-at-euc-2015-and-next-steps/"},{"content":"A little moan to start with… I owned a mid 2009 MacBook Pro, I never used it for presenting stuff to others but I actually bought a remote control for it. The computer cost me a lot of money, it was top end stuff when it was bought. The remote control cost me the money as well. I also have a copy of Keynote. It also cost me money. A while ago I’ve switched to a new MacBook, the Retina one, top end as well. It cost even more money than the first one.\nAs it turns out, next month I will be speaking at a conference and I thought it would be great if I could use the remote control to switch between the slides. Right? Well, except that the new, very expensive computer, doesn’t have a built in IR receiver. There goes an idea of using the remote. But hold on, there must an application to control Keynote presentation using a phone, surely. Emmm… there was. It was called Keynote remote. But Apple pulled it from Appstore and merged the functionality with Keynote for iOS. Keynote for iOS costs money. Now, I could’ve just bite the bullet and buy the damn thing. But something kicked off inside of me though…\nWhy would I spend another c.a. €8 for a bit of software to flip the slides from my phone?\nDear Apple, I’ve spent so much money on the stuff already:\n 2 MBPs roughly €6k together ~€700 on a phone €20 for Keynote  I mean, come on, surely you’d at least give me a remote for free. Hell no, not paying for it. It’s not like I can’t afford it, but I’m not paying for it…\nSinatra presenter Here it is. The application is called sinatra-presenter and is now available on github. I’ve built it purely to vent off my frustration…\nAll the instructions on how to use it are available in the readme file.\nQuick rundown through the features:\n lists presentation placed in a given directory (RTFM) selecting a presentation from the list opens it and puts it in slideshow mode starting with the first slide go to the next and previous slide using the buttons on the screen if the presnetation is put in the background (say to switch to the terminal to demo something), clicking next/previous buttons will restore the presentation into slideshow and handle slide change correctly if it happens that the browser interface is reloaded, the state will be restored by introspecting Keynote state  Is it free? Yep, it’s free. Available under LGPLv3 license.\nHave fun with it.\n","description":"","tags":["keynote","presenting"],"title":"Control Keynote presentation with your mobile browser","uri":"/posts/2015-05-11-control-keynote-presnetation-with-your-mobile-browser/"},{"content":"Unit testing has evolved a long way over the years, continuous integration services even more so. Few years back, when mentioning CI, Jenkins was the only reasonable choice someone would take. It was kind of easy to set up, depending on what one was planning to do with it. Looking at this space today there are plenty of services offering painless CI for the masses. The most known one - Travis CI. Compared to Jenkins it’s night and day. Simply sign in with Github, go to settings, enable repository to be tested and you are off. The build can be customised by modifying .travis.yml file placed in the root of the repository to be tested. Every push to Github, every pull request and every merge is automatically tested. Results are available on Github together with full history of the project. Easy.\nI love Travis CI, like I would most likely love a lot of similar products, Snap CI comes to mind, there are dozens of those out there in the wild. Travis has changed how people think of unit testing / CI. If you are working on an open source project, there’s no reason not to use Travis or similar solution. For open source projects Travis CI is available for free. But there is one problem with such products. Obviously, people who run them are not charities. This is not to criticise, we all write software and would like to make for living somehow. Travis, like any other platform of such kind, has a paid version. The free version is a driver to bring customers to the paid version. The free version has limits. In case of Travis, there are two most obvious. Free version allows public repositories only and there’s a global resource limit available for all users. As the product grows in popularity, testing becomes slower and slower. To the point where it becomes, by far, the slowest process of all, 10 minute wait is not uncommon these days. Please keep in mind, this is not a complaint, it’s great that these products exist and can be used for free. The problem still exists though. There is a lot of great free tools that don’t get any direct financial support and a paid version is not an option. Any kind of payment may be a stretch.\nLet’s have a look at what the modern open source project cycle is. There is a (distributed) version control system. The members (committers) are most likely distributed around the world, they contribute at different times and different pace. Their work is available to the public via central repository. Most often Github, but also Bitbucket and probably more I never heard of. A central testing / CI mechanism is employed, such as Travis, to enable full visibility to everybody involved in development and everyone who wants to use the product. It’s a central source of truth. Any delay hinders the progress.\nIs there a (better) way to do this? A few weeks ago, while talking to one of my colleagues at work, I mentioned an idea of a system which would enable testing / CI of the open source products on a big scale using resources already available to the developers. The purpose is to fulfil the process described above. How could such system work?\nThere would be a central website displaying build / testing progress, like Travis does. This website would be communicating with systems such as Github or Bitbucket, it would receive notifications about pushes, pull requests and merges. It would display every build status and progress. It would report results back to Github, Bitbucket and such. The problem to solve is the resources' availability to run tests / CI. The website could have a number of virtual machines available for download, say VMWare, VirtualBox, QEMU, KVM and so on. Each of those would be configured to receive a build / test / CI request from the central system, execute it according to the given specification and send the results in real-time back to the central system. People could contribute to the overall processing capacity of such system by simply running these VMs. The more of them out there, the more processing capacity available for everybody. Such VMs could either run Docker images, for fast tests that don’t require root permissions (or sudo as one prefers), or could be used directly to run tests with all the options enabled.\nThere are obvious problems to solve. Security — how to prevent this system being turned into a bot net and ensure that the results are not manipulated with. How to ensure that tests are run in timely fashion, the results are available. How to enable the system to be fault tolerant. Once the build is accepted by a VM worker, it has to be completed. A VM failure could prevent the results from arriving at the central system. The job may have to be replayed somewhere else. The tools are available. It would be in an interest of open source developers to run such VMs, maybe some organisation could donate computing power to run more VMs? There is cost involved, running a “central” system wouldn’t come for free. This would have to be load balanced and replicated across different geo-locations to make it fault tolerant in itself. But it would be significantly cheaper to run than having a fleet of machines on stand-by ready to execute tests for the world.\nJust a thought…\n","description":"","tags":[],"title":"Crowd sourced unit testing / CI","uri":"/posts/2015-04-21-crowd-sourced-unit-testing-ci/"},{"content":"The gossiperl project is growing. I had a great, productive, working Christmas break, implementing a number of client libraries for the message bus. In the last couple of weeks a number of client libraries for gossiperl have been released. The full list now includes:\n Ruby JVM with examples in Clojure and Scala .NET as a mono project Erlang  And the most exciting so far — gossiperl client Chrome extension. This one is not complete yet and it will take a little bit longer until it is fully implemented. However, the perspective of running a gossiperl client in the browser — communicating with applications running locally — opens the door for some fantastic opportunities. While working on the Chrome client I also implemented the Thrift BinaryProtocol for JavaScript library. Currently the code is not included in the main Thrift project due to some issues with Thrift unit tests but there’s an ongoing effort to make this code part of Thrift as soon as possible. Right now the code available as a separate github repository.\nThe next steps for gossiperl are:\n provide unit tests for gossiperl itself and integrate with Travis CI enable unit testing of the client libraries via Travis CI provide end to end tests for gossiperl  Afterwards, there will be more clients for other languages:\n Python Node.JS Haskell C++ examples in F#  There will also examples of useful end to end things appearing on github!\n","description":"","tags":["gossip","thrift"],"title":"State of gossiperl and some JavaScript Thrift goodies","uri":"/posts/2015-01-05-state-of-gossiperl-and-some-javascript-thrift-goodies/"},{"content":"Today I’ve released a project called gossiperl. Gossiperl is a language agnostic gossip middleware written in Erlang. The purpose of the project was purely research on gossip based systems, as well as, learning Erlang.\nMain intent was to create a common communication middleware enabled over gossip using a standard binary transport mechanism. Gossiperl is a result of over 6 months of research, reading, learning, planning and implementation. Gossiperl uses Apache Thrift binary serialisation over UDP. Gossiperl is intended for projects where order of delivery is not important and latency is acceptable. Intended projects require a messaging layer but not necessarily want to implement their own communication stack.\nGossiperl provides distributed subscriptions over gossip via UDP with Thrift digests secured with AES-256.\nGossiperl overlay contains a number of members — could be servers in a cluster. These servers do some common operations and the data has to be shared across the cluster. Every server within the cluster runs a gossiperl daemon. All servers connect over a gossip overlay. Application requiring exchanging the data connect to the local member and subscribe for a certain digest type. Other instances of same application connected to members on other nodes of a cluster (overlay) publish digests of expected type to gossip. The data is being forwarded across the overlay, only to the members that actually require receiving the data. Gossiperl is a presence mechanism and a message bus.\nThe main features of gossiperl:\n multiple overlays — adding / removing overlays at runtime reconfiguration at runtime presence mechanism distributed subscriptions with “at least once delivery” semantics every overlay secured using separate AES-256 symmetric key / iv combination local subscribers secured using subscriber specific secret language agnostic — uses Apache Thrift for all communication REST API with HTTPS and authentication provided statistics (using InfluxDB)  Being a research code and a subject of learning Erlang, gossiperl might not be the highest quality Erlang code in the wild. It is also not intended to be used in production environments. The work on gossiperl is ongoing.\nCurrently only the main codebase is available. Unit tests, end to end tests, client libraries for different languages and deployment / packaging tools will be released soon (it won’t be months). Most of these parts are written but need to be structured for shipping.\nAdditionally, there is development on providing communication across physical locations. This enables gossiperl to operate in a multi datacenter deployment, using RAFT consensus. Every seed of every overlay will elect an overlay leader, the leaders of different racks will exchange the data across locations.\nI hope some find this stuff useful. If you do, please let me know! The code is available on github under MIT license. Some basic documentation / brain dump can be found in the github wiki. The main website of the project is at gossiperl.com. The list of work items, issues and bugs is at github issues.\nIt’s been fun building the basics. There’s more fun ahead.\n","description":"","tags":["erlang","gossip","thrift","influxdb"],"title":"Gossiperl   gossip middleware in Erlang","uri":"/posts/2014-12-09-gossiperl-gossip-middleware-in-erlang/"},{"content":"A few months ago I have started learning Erlang, mostly for fun but it was right about time to jump on the functional bandwagon. The best way to learn a new language is to find an engaging project, in my case its been something what has been on my mind for quite a while: a cloud communication protocol / framework for distributed computing. Some basic principles of what it is about can be found here: CloudDDS.\nWhile learning Erlang, I decided to also learn a number of other technologies, or using other words - apply them in practice. One of those is Apache Thrift. Apache Thrift is a binary cross-language framework for distributed service development. It is a serialization protocol with its own RPC stack.\nThrift RPC stack comes with a number of supported protocols, referred to as transports. Erlang implementation provides HTTP, JSON, file, disk log and framed transports. The project I was working on uses UDP for all communication. Below, I am going to describe how I implemented Thrift over UDP in Erlang. I will walk the reader through building a simple active-active setup of two clients communicating via UDP using Thrift messages.\nPrerequisites Apache Thrift uses .thrift file to describe services and types to used by the software. To generate the language code, Apache Thrift must be installed first. There is a lot of examples available how this can be achieved. I have created a bash script for Ubuntu 12.04 for my own reference, available here: Install Thrift 0.9.1 with all language support.\nDesign I will follow the standard OTP principles while developing the example.\napplication -\u003e supervisor -\u003e UDP messaging worker | -\u003e serialization worker  While discussing the implementation, I will not focus on the details of the messaging code, neither I will dive into the details of the standard OTP components. I will focus only on the Thrift part.\nBasic OTP modules First step is to create a basic Erlang application. All the code can be found below. Additionally, at the bottom of the article I am publishing the link to the sources available on github.\nrebar.config A bare minimum required for installing dependencies.\n1 2 3 4  {deps, [ {thrift, \".*\", {git, \"https://github.com/radekg/thrift-erlang.git\", \"master\"}} ]}.   I am using my own fork of Thrift, it contains updated JSX dependencies.\nsrc/udp_thrift.app.src 1 2 3 4 5 6 7 8 9 10 11 12  {application, udp_thrift, [{description, \"Example of using Thrift with UDP\"}, {vsn, \"1\"}, {modules, [udp_thrift_types, udp_thrift_app, udp_thrift_common, udp_thrift_sup, udp_thrift_messaging, udp_thrift_serialization]}, {registered, udp_thrift_sup}, {applications, [kernel, stdlib, crypto, thrift]}, {mod, {udp_thrift_app, []}}]}.   src/udp_thrift_app.erl 1 2 3 4 5 6 7 8  -module(udp_thrift_app). -behaviour(application). -export([start/2, stop/1]). start(_Type, _Args) -\u003e udp_thrift_sup:start_link(). stop(_State) -\u003e ok.   src/udp_thrift_common.erl 1 2 3 4 5 6 7 8 9 10 11  -module(udp_thrift_common). -export([ get_timestamp/0, get_message_id_as_string/0 ]). get_timestamp() -\u003e {Mega,Sec,Micro} = os:timestamp(), trunc( ((Mega*1000000+Sec)*1000000+Micro) / 1000000 ). get_message_id_as_string() -\u003e lists:flatten( io_lib:format( \"~p\", [ make_ref() ] ) ).   src/udp_thrift_sup.erl 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45  -module(udp_thrift_sup). -behaviour(supervisor). -export([start_link/0, init/1]). start_link() -\u003e supervisor:start_link({local, ?MODULE}, ?MODULE, []). init([]) -\u003e Port = case application:get_env(udp_thrift, port) of { ok, Value } -\u003e Value; undefined -\u003e { error, no_port } end, PeerPort = case application:get_env(udp_thrift, peer_port) of { ok, Value2 } -\u003e Value2; undefined -\u003e { error, no_peer_port } end, case application:get_env(udp_thrift, name) of { ok, Name } -\u003e {ok, { { one_for_all, 10, 10}, [ { udp_thrift_messaging, {udp_thrift_messaging, start_link, [ {127,0,0,1}, Port, Name, PeerPort ]}, permanent, brutal_kill, worker, [] }, { udp_thrift_serialization, {udp_thrift_serialization, start_link, []}, permanent, brutal_kill, worker, [] } ] } }; undefined -\u003e { error, no_name_given } end.   The supervisor expects a number of environment properties to be set. This will be provided later on, in Running and testing section. If one or more of these is not provided, the application will not start or fail at a worker start step.\nsrc/udp_thrift_messaging.erl 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99  -module(udp_thrift_messaging). -behaviour(gen_server). -export([start_link/4, stop/0]). -export([init/1, handle_call/3, handle_cast/2, handle_info/2, code_change/3, terminate/2]). -define(SERVER, ?MODULE). -include(\"udp_thrift_types.hrl\"). start_link(BindAddress, BindPort, Name, PeerPort) -\u003e gen_server:start_link({local, ?MODULE}, ?MODULE, [BindAddress, BindPort, Name, PeerPort], []). stop() -\u003e gen_server:cast(?MODULE, stop). init([BindAddress, BindPort, Name, PeerPort]) -\u003e case gen_udp:open(BindPort, [binary, {ip, BindAddress}]) of {ok, Socket} -\u003e erlang:send_after(2000, self(), { contact_peer, BindPort }), {ok, {messaging, Socket, Name, PeerPort}}; {error, Reason} -\u003e {error, Reason} end. terminate(_Reason, {messaging, Socket, _}) -\u003e gen_udp:close(Socket). handle_cast(stop, LoopData) -\u003e {noreply, LoopData}. %% SENDING  handle_info({ contact_peer, Port }, { messaging, Socket, Name, PeerPort }) -\u003e error_logger:info_msg( \"Sending digest to ~p.\", [ PeerPort ] ), Digest = #digest{ name = Name, port = Port, heartbeat = udp_thrift_common:get_timestamp(), id = udp_thrift_common:get_message_id_as_string() }, udp_thrift_serialization ! { serialize, digest, Digest, self() }, erlang:send_after(2000, self(), { contact_peer, Port }), {noreply, { messaging, Socket, Name, PeerPort }}; handle_info({ message_serialized, { ok, SerializedMessage } }, { messaging, Socket, Name, PeerPort }) -\u003e gen_udp:send( Socket, { 127,0,0,1 }, PeerPort, SerializedMessage ), {noreply, { messaging, Socket, Name, PeerPort } }; %% RECEIVING  handle_info({udp, _ClientSocket, _ClientIp, _ClientPort, Msg}, { messaging, Socket, Name, PeerPort }) -\u003e udp_thrift_serialization ! { deserialize, Msg, self() }, {noreply, { messaging, Socket, Name, PeerPort }}; handle_info({ message_deserialized, { ok, DecodedPayloadType, DecodedPayload } }, { messaging, Socket, Name, PeerPort }) -\u003e self() ! { message, DecodedPayloadType, DecodedPayload }, {noreply, { messaging, Socket, Name, PeerPort }}; handle_info({ message, digest, DecodedPayload }, { messaging, Socket, Name, PeerPort }) -\u003e error_logger:info_msg(\"Received digest from ~p. Replying to ~p\", [ DecodedPayload#digest.name, DecodedPayload#digest.port ]), DigestAck = #digestAck{ name = Name, heartbeat = udp_thrift_common:get_timestamp(), reply_id = DecodedPayload#digest.id }, udp_thrift_serialization ! { serialize, digestAck, DigestAck, self() }, {noreply, { messaging, Socket, Name, PeerPort }}; handle_info({ message, digestAck, DecodedPayload }, { messaging, Socket, Name, PeerPort }) -\u003e error_logger:info_msg(\"Received digestAck to digest ~p.\", [ DecodedPayload#digestAck.reply_id ]), {noreply, { messaging, Socket, Name, PeerPort }}; %% ERROR HANDLING  handle_info({ message_deserialized, {error, Reason} }, { messaging, Socket, Name, PeerPort }) -\u003e error_logger:error_msg(\"Message decode failed. Reason ~p.\", [Reason]), {noreply, { messaging, Socket, Name, PeerPort }}; handle_info(Msg, LoopData) -\u003e error_logger:info_msg(\"Unhandled handle_info ~p\", [Msg]), {noreply, LoopData}. %% REMAINDER OF GEN_SERVER BEHAVIOUR  handle_call({message, _Msg}, _From, LoopData) -\u003e {reply, ok, LoopData}. code_change(_OldVsn, State, _Extra) -\u003e {ok, State}.   private/udp_thrift.thrift The task of this example is for every client, at two seconds interval, send a Digest to the other client. On the other side of the wire, upon receiving the Digest, send a DigestAck message back. Every message is wrapped in a DigestEnvelope container so the program can easily establish what is the digest type sent. The thrift file looks like this:\nnamespace erl udp_thrift struct DigestEnvelope { 1: required string payload_type; 2: required string bin_payload; } struct Digest { 1: required string name; 2: required i32 port; 3: required i64 heartbeat; 4: required string id; } struct DigestAck { 1: required string name; 2: required i64 heartbeat; 3: required string reply_id; } Generate code from Thrift and make it available Once thrift is installed on the system, the code can be generated from the thrift file. To do so:\n1 2 3 4 5 6 7 8  cd \u003cproject root\u003e/private/ thrift --gen erl udp_thrift.thrift mkdir -p ../include # make the generated code available for the program: mv gen-erl/udp_thrift_types.erl ../src/udp_thrift_types.erl mv gen-erl/udp_thrift_constants.hrl ../include/udp_thrift_constants.hrl mv gen-erl/udp_thrift_types.hrl ../include/udp_thrift_types.hrl rm -Rf gen-erl   The source code on github contains a Vagrant box which can be used to generate the code:\n1  vagrant up thrift   Thrift serialisation and deserialisation The module responsible for these tasks has been already started in the supervisor. Let’s go through the code and discuss the parts in detail.\n1 2 3 4 5 6 7 8 9 10 11 12  -module(udp_thrift_serialization). -behaviour(gen_server). -export([start_link/0, stop/0]). -export([init/1, handle_call/3, handle_cast/2, handle_info/2, code_change/3, terminate/2]). -include(\"udp_thrift_types.hrl\").   First, these records are required. As the serialisation happens within this single module, I include them directly by copying from Erlang Thrift sources.\n1 2 3 4 5  -record(binary_protocol, {transport, strict_read=true, strict_write=true }). -record(memory_buffer, {buffer}).   Following is the code for the module setup. For serialisation, a single protocol instance may be used for all serialized messages.\n1 2 3 4 5 6 7 8 9  start_link() -\u003e gen_server:start_link({local, ?MODULE}, ?MODULE, [], []). init([]) -\u003e {ok, OutThriftTransport} = thrift_memory_buffer:new(), {ok, OutThriftProtocol} = thrift_binary_protocol:new(OutThriftTransport), {ok, { serialization, OutThriftProtocol }}. stop() -\u003e gen_server:cast(?MODULE, stop).   Serialization 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  handle_info({ serialize, DigestType, Digest, CallerPid }, LoopData) -\u003e self() ! { serialize, DigestType, Digest, udp_thrift_types:struct_info(DigestType), CallerPid }, {noreply, LoopData}; handle_info({ serialize, DigestType, Digest, StructInfo, CallerPid }, { serialization, OutThriftProtocol }) -\u003e CallerPid ! { message_serialized, { ok, digest_to_binary( #digestEnvelope{ payload_type = atom_to_list(DigestType), bin_payload = digest_to_binary(Digest, StructInfo, OutThriftProtocol) }, udp_thrift_types:struct_info(digestEnvelope), OutThriftProtocol ) } }, {noreply, { serialization, OutThriftProtocol } };   First handle_info receives the data to serialize, it uses udp_thrift_types:struct_info provided by the thrift generated modules to get the thrift type definition. This is forwarded to the the second handle_info. digest_to_binary is where all the action happens (presented shortly). The inner digest_to_binary call serializes the digest received from messaging component. This is then wrapped inside of the digestEnvelope digest which has to be converted to binary data for UDP delivery.\nDeserialization 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  handle_info({ deserialize, BinaryDigest, CallerPid }, LoopData) -\u003e try case digest_from_binary(digestEnvelope, BinaryDigest) of {ok, DecodedResult} -\u003e case payload_type_as_known_atom(DecodedResult#digestEnvelope.payload_type) of { ok, PayloadTypeAtom } -\u003e case digest_from_binary( PayloadTypeAtom, DecodedResult#digestEnvelope.bin_payload) of { ok, DecodedResult2 } -\u003e CallerPid ! { message_deserialized, { ok, PayloadTypeAtom, DecodedResult2 } }; _ -\u003e error_logger:error_msg(\"Message could not be decoded.\"), CallerPid ! { message_deserialized, { error, decode_binary_content_failed } } end; { error, UnsupportedPayloadType } -\u003e error_logger:error_msg(\"Unsupprted message ~p.\", [UnsupportedPayloadType]), CallerPid ! { message_deserialized, {error, unsuppoted_payload_type} } end; _ -\u003e error_logger:error_msg(\"Could not open digestEnvelope.\"), CallerPid ! { message_deserialized, {error, digest_envelope_open_failed} } end catch _Error:Reason -\u003e gossiper_log:err(\"Error while reading digest: ~p.\", [Reason]), CallerPid ! { message_deserialized, { error, digest_read } } end, {noreply, LoopData}; handle_info(_Info, LoopData) -\u003e {noreply, LoopData}.   Upon received a deserialization request, the first thing that happens is deserialize_from_binary. This function assumes that the data received is Thrift and the data is of type digestEnvelope. The details will be presented shortly. Once the payload carried by the envelope has been read, the program detects if the payload is of a supported type. If so, it attempts deserializing the content. If all of the above succeeds, the result is passed back to messaging (caller of the serialization). Majority of that function is simple error handling.\ndigest_to_binary, digest_from_binary and payload_type_as_known_atom 1 2 3 4 5 6  digest_to_binary(Digest, StructInfo, OutThriftProtocol) -\u003e {PacketThrift, ok} = thrift_protocol:write(OutThriftProtocol, { {struct, element(2, StructInfo)}, Digest}), {protocol, _, OutProtocol} = PacketThrift, {transport, _, OutTransport} = OutProtocol#binary_protocol.transport, iolist_to_binary(OutTransport#memory_buffer.buffer).   digest_to_binary serializes the payload. It uses the copy of the protocol defined in init to write the thrift binary representation of data to be sent. All what has to be done afterwards is converting iolist contained in the resulting OutTransport to binary data. This is returned to CallerPid and sent via UDP to the destination.\n1 2 3 4 5 6 7 8 9 10 11 12  digest_from_binary(DigestType, BinaryDigest) -\u003e {ok, InTransport} = thrift_memory_buffer:new(BinaryDigest), {ok, InProtocol} = thrift_binary_protocol:new(InTransport), case thrift_protocol:read( InProtocol, {struct, element(2, udp_thrift_types:struct_info(DigestType))}, DigestType) of {_, {ok, DecodedResult}} -\u003e {ok, DecodedResult}; _ -\u003e {error, not_thrift} end.   digest_from_binary receives an atom of an expected digest type and a binary digest as an input. First, a memory_buffer transport is constructed from the binary data. It is then converted into a binary protocol and an attempt to read the data from thrift is made using a structure provided by udp_thrift_types:struct_info.\n1 2 3 4 5 6 7 8  payload_type_as_known_atom(DigestTypeBin) -\u003e KnownDigestTypes = [ { \u003c\u003c\"digest\"\u003e\u003e, digest }, { \u003c\u003c\"digestAck\"\u003e\u003e, digestAck } ], case lists:keyfind( DigestTypeBin, 1, KnownDigestTypes ) of false -\u003e { error, DigestTypeBin }; { _Bin, Atom } -\u003e { ok, Atom } end.   The above function is rather self explanatory. The deserialization will be attempted if an atom can be found in the KnownDigestTypes for given digest type. Otherwise an error is returned.\nThe module ends with the rest of required gen_server behaviour.\n1 2 3 4 5 6 7 8 9 10 11  handle_call(_Msg, _From, LoopData) -\u003e {reply, ok, LoopData}. handle_cast(stop, LoopData) -\u003e {noreply, LoopData}. terminate(_Reason, _LoopData) -\u003e {ok}. code_change(_OldVsn, State, _Extra) -\u003e {ok, State}.   Running and testing Preparation:\n1 2 3  cd \u003cproject directory\u003e ./rebar get-deps ./rebar compile   To run, two terminal windows are required. In the first window, start Erlang with this command:\n1  erl -pa ebin/ -pa deps/*/ebin   Run the application:\n1 2 3 4 5 6  application:set_env(udp_thrift, name, \u003c\u003c\"memberA\"\u003e\u003e). application:set_env(udp_thrift, port, 6666). application:set_env(udp_thrift, peer_port, 6667). application:start(crypto). application:start(thrift). application:start(udp_thrift).   In the second terminal window, start Erlang:\n1  erl -pa ebin/ -pa deps/*/ebin   Run the application:\n1 2 3 4 5 6  application:set_env(udp_thrift, name, \u003c\u003c\"memberB\"\u003e\u003e). application:set_env(udp_thrift, port, 6667). application:set_env(udp_thrift, peer_port, 6666). application:start(crypto). application:start(thrift). application:start(udp_thrift).   Both windows should be showing output similar to this:\n… Sending digest to 6667. =INFO REPORT==== 12-Oct-2014::21:43:43 === Sending digest to 6667. =INFO REPORT==== 12-Oct-2014::21:43:43 === Received digestAck to digest \u003c\u003c\"#Ref\u003c0.0.0.95\u003e\"\u003e\u003e. =INFO REPORT==== 12-Oct-2014::21:43:45 === Received digest from \u003c\u003c\"memberB\"\u003e\u003e. Replying to 6667 =INFO REPORT==== 12-Oct-2014::21:43:45 === Sending digest to 6667. =INFO REPORT==== 12-Oct-2014::21:43:45 === Received digestAck to digest \u003c\u003c\"#Ref\u003c0.0.0.100\u003e\"\u003e\u003e. =INFO REPORT==== 12-Oct-2014::21:43:47 === Received digest from \u003c\u003c\"memberB\"\u003e\u003e. Replying to 6667 =INFO REPORT==== 12-Oct-2014::21:43:47 === Sending digest to 6667. =INFO REPORT==== 12-Oct-2014::21:43:47 === Received digestAck to digest \u003c\u003c\"#Ref\u003c0.0.0.105\u003e\"\u003e\u003e. … If that is the case, both clients are communicating via UDP with Thrift protocol.\nCode and license All code discussed above can be found on github: Using Apache Thrift with UDP in Erlang. The code is available under MIT license.\n","description":"","tags":["thrift","erlang"],"title":"Apache Thrift via UDP in Erlang","uri":"/posts/2014-10-12-apache-thrift-via-udp-in-erlang/"},{"content":"I’d like to develop for OpenStack The eaiest way to start, is to use a project called devstack. Devstack is:\n A documented shell script to build complete OpenStack development environments.\n It looks promising. Unfortunately, the script is not as simple as what is claimed on the website:\ngit clone https://github.com/openstack-dev/devstack.git # configure, while optional... cd devstack; ./stack.sh  There be Dragons.\nSo? After looking at a number of different search results on the Internets, digging through some Chef cookbooks, Puppet stuff, here’s a Vagrant file to set up Devstack using Vagrant (Ubuntu 12.04):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  Vagrant.configure(\"2\") do |config| config.vm.provider \"virtualbox\" do |vbox, override| override.vm.box = \"precise64\" override.vm.box_url = \"http://files.vagrantup.com/precise64_vmware.box\" vbox.customize [\"modifyvm\", :id, \"--memory\", \"2048\"] end config.vm.define \"devstack\" do |devstack| # redirect horizon, we can't run on port 80 devstack.vm.network :forwarded_port, guest: 80, host: 9000 # redirect the API  devstack.vm.network :forwarded_port, guest: 8774, host: 8774 # redirect the auth API devstack.vm.network :forwarded_port, guest: 35357, host: 35357 # install git devstack.vm.provision :shell, :inline =\u003e \"apt-get install -y git-core\" # clone Devstack devstack.vm.provision :shell, :inline =\u003e \"cd /opt \u0026\u0026 git clone https://github.com/openstack-dev/devstack.git\" # Create the stack user devstack.vm.provision :shell, :inline =\u003e \"/opt/devstack/tools/create-stack-user.sh\" # add user to sudoers devstack.vm.provision :shell, :inline =\u003e \"echo \\\"stack ALL=(ALL) NOPASSWD: ALL\\\"\u003e\u003e /etc/sudoers\" # localrc file, explained below # we are going to copy this file from the host to the guest where we run Devstack devstack.vm.provision :shell, :inline =\u003e \"cat \u003e /opt/devstack/localrc \u003c\u003c'VAGRANTEOP'\\n#{File.read( File.dirname(__FILE__) + \"/setup/devstack-localrc\" )}\\nVAGRANTEOP\" # just make it work, it's dev devstack.vm.provision :shell, :inline =\u003e \"chmod -R 0777 /opt/devstack\" # and stack it all up devstack.vm.provision :shell, :inline =\u003e \"su - stack -c '/opt/devstack/stack.sh'\" end end   About 12.5 minutes Vagrant should display something like this:\n2014-05-20 15:03:49.711 | stack.sh completed in 758 seconds.  Horizon will be accessible via http://localhost:9000, the API is available on http://localhost:8774/v2/…\nWait, localrc! The structure of the project should looks like this:\n-- Vagrantfile -- setup |-- devstack-localrc  The devstack-localrc file contains some critical information, its place is most likely in .gitignore. Here’s the basic content, just put your own data in it:\nDEST=/opt/devstack ENABLED_SERVICES+=,heat ADMIN_PASSWORD=meh MYSQL_PASSWORD=meh RABBIT_PASSWORD=meh SERVICE_PASSWORD=meh SERVICE_TOKEN=meh HOST_IP=0.0.0.0 LOGFILE=stack.sh.log LOGDAYS=1 LOG_COLOR=False SCREEN_LOGDIR=/opt/stack/logs/screen API_RATE_LIMIT=False APT_FAST=True RECLONE=yes  Voilà!\n","description":"","tags":["openstack","devstack","vagrant","ubuntu"],"title":"OpenStack Devstack up and running with Vagrant, in 12.5 minutes","uri":"/posts/2014-05-20-openstack-devstack-up-and-running-with-vagrant-in-125-minutes/"},{"content":"It’s been already a month since I released erflux on github. Erflux is an Erlang client for InfluxDB HTTP protocol.\nInstallation 1 2 3 4  {deps, [ {erflux, \".*\", {git, \"git://github.com/radekg/erflux.git\", {tag, \"version-1\"}}} }]}   and run\n./rebar get-deps  Configuration Erflux allows configuring a number of parameters:\n InfluxDB host, default 127.0.0.1 InfluxDB port, default 8086 username, default: root password, default: root SSL usage, default: false timeout, default: infinity  The simplest way of applying configuration is to use application:set_env, like the example below:\napplication:start(crypto), application:start(asn1), application:start(public_key), application:start(ssl), application:start(idna), application:start(hackney), application:start(jsx), application:load(erflux), application:set_env(erflux, host, \u003c\u003c\"192.168.50.115\"\u003e\u003e), application:set_env(erflux, port, 8086), application:set_env(erflux, username, \u003c\u003c\"root\"\u003e\u003e), application:set_env(erflux, password, \u003c\u003c\"root\"\u003e\u003e), application:set_env(erflux, ssl, false), application:set_env(erflux, timeout, infinity), application:start(erflux), erflux_sup:add_erflux(erflux_http), erflux_http:get_databases().  Writing data To write data with erlfux:\n1 2 3 4 5 6 7  erflux_http:write_series(erfluxtest, [ [ { points, [ [ 1, 2, 3 ] ] }, { name, testseries }, { columns, [ a, b, c ] } ] ]).   or\n1 2 3 4 5  erflux_http:write_point(erfluxtest, testseries, [ { a, 1 }, { b, 2 }, { c, 3 } ]).   Reading data Reading many columns:\nerflux_http:read_point(erfluxtest, [a, b], testseries).  or a single column:\nerflux_http:read_point(erfluxtest, a, testseries).  More complex queries like this can be executed using the q function:\nerflux_http:q(erfluxtest, \u003c\u003c\"select A from testseries limit 1\"\u003e\u003e).  Advanced features In case if it is necessary to open multiple connection to different InfluxDB servers, erflux allows for it by instantiating multiple clients. The application has a choice of using provider supervisor:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  application:start(crypto), application:start(asn1), application:start(public_key), application:start(ssl), application:start(idna), application:start(hackney), application:start(jsx), application:start(erflux), %% This will connect to localhost, if no other settings provided: erflux_sup:add_erflux(erflux_http), erflux_http:get_databases(). %% Start the additional client, connect to a different host: {ok, RemoteHost} = erflux_sup:add_erflux(erflux_custom_host, \u003c\u003c\"root\"\u003e\u003e, \u003c\u003c\"root\"\u003e\u003e, \u003c\u003c\"somehost.influxdb\"\u003e\u003e). %% To list databases of the remote host, do the following: erflux_http:get_databases(RemoteHost). %% To remove the instance: erflux_sup:remove_erflux(erflux_custom_host).   or bypassing the supervisor:\n1 2  { ok, Pid } = erflux_http:start_link( erflux_custom, #erflux_config{} ), erflux_http:get_databases( Pid ).   Atoms and binaries Every function comes in 2 variants:\n accepting atoms and accepting binaries  It’s not possible to mix argument types. For example, this will fail:\n1  erflux_http:create_database_user(database, \u003c\u003c\"username\"\u003e\u003e, password).   Query parameter of erflux_http:q is always binary.\nThe exceptions to the all rule are columns in write_series, write_point and read_series. All these are valid:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  erflux_http:write_series(erfluxtest, [ [ { points, [ [ 1, 2, 3 ] ] }, { name, testseries }, { columns, [ a, \u003c\u003c\"b\"\u003e\u003e, c ] } ] ]). erflux_http:write_point(erfluxtest, testseries, [ { a, 1 }, { \u003c\u003c\"b\"\u003e\u003e, 2 }, { c, 3 } ]). erflux_http:read_point(erfluxtest, [\u003c\u003c\"a\"\u003e\u003e, b], testseries).   The code The code is available on github. It is licensed under the MIT license.\n","description":"","tags":["erlang","influxdb"],"title":"Erflux, InfluxDB client for Erlang","uri":"/posts/2014-10-21-erflux-influxdb-client-for-erlang/"},{"content":"The problem One of my Zookeeper clusters has to be available to everyone. But I want to make sure only specific known hosts are allowed to connect. The problem was, those known hosts are dynamic and I didn’t want any configuration for this component. The servers are running in the cloud, they come and go.\nAt Technicolor, we use Chef to manage all our boxes. Every box is registered on the Chef server, we know every private and public IP address of every server within our setup. Be it EC2, IBM SCE, Rackaspace or any other cloud provider. What I came up with required writing Zookeeper authentication provider connected to the Chef server; only allow the connection if the IP is known at the time of connection.\nIt all sounds pretty straightforward, however, Zookeeper authentication is not very well documented. It took quite a while to connect all the dots and make it work.\nThe code Full code available on Github.\nThe solution First of all, the term authentication. It is a bit confusing at start, Zookeeper uses the term ACL. That’s because permissions are applied per znode and not the server. When talking about authentication one is really thinking access to that particular znode.\nMy authentication implementation can be found here. All the Ruby bits are in the same project.\nAs can be seen, all what has to be done is creating a class which will implement org.apache.zookeeper.server.auth.AuthenticationProvider interface. In case of Chef based authentication, in my environment, I wanted every node to have full post authentication access to Zookeeper. Hence this:\n1 2 3 4 5 6 7 8 9 10 11  public boolean matches(String id, String aclExpr) { return true; } public boolean isAuthenticated() { return true; } public boolean isValid(String id) { return true; }   If the purpose of the authenticartion provider was to lock down separate znodes, the implementation of matches(String id, String aclExpr) would differ. There are quite good examples in Zookeeper sources, these are available here.\nWriting the provider was the easy part. Once the jar is compiled, placed in the lib folder and Zookeeper is restarted, it is available. The poorly documented bit is the following:\n Every node that should not be available to the world should have the ACL applied. ACL is not recursive.\n That basically means:\n Even though the autentication is enabled and working, it can’t be observed because everything is public anyway.\n Example Let’s consider fresh Zookeeper installation. It comes with 3 znodes. These are /, /zookeeper and /zookeeper/quota. Even if the authentication provider is loaded and working, everyone in the world will be able to see and modify these if ACL is not applied. Following code may be used to lock down the / znode:\n1 2 3 4 5 6 7 8  require \"rubygems\" require \"zookeeper\" acl = [ Zookeeper::ACLs::ACL.new( :perms =\u003e 31, :id =\u003e Zookeeper::ACLs::Id.new( :scheme =\u003e \"chef\", :id =\u003e \"\" ) )] z = Zookeeper.new(\"127.0.0.1:2181\") z.set_acl(:path =\u003e \"/\", :acl =\u003e acl )   From now on the world would not be allowed to browse nor modify anything under /. However, /zookeeper and /zookeeper/quota are still wide open. Full lock down of a fresh Zookeeper installation is simple:\n1 2 3 4 5 6 7  require \"rubygems\" require \"zookeeper\" acl = [ Zookeeper::ACLs::ACL.new(:perms =\u003e 31, :id =\u003e Zookeeper::ACLs::Id.new( :scheme =\u003e \"chef\", :id =\u003e \"\" ) )] z = Zookeeper.new(\"127.0.0.1:2181\") z.set_acl(:path =\u003e \"/\", :acl =\u003e acl ) z.set_acl(:path =\u003e \"/zookeeper\", :acl =\u003e acl ) z.set_acl(:path =\u003e \"/zookeeper/quota\", :acl =\u003e acl )   From now on the world will still be able to connect to my Zookeeper. But no operations are permitted. Obviously, whatever new znode is created, it should have the ACL applied. Like this:\n... zk_acl = [ Zookeeper::ACLs::ACL.new( :perms =\u003e 31, :id =\u003e Zookeeper::ACLs::Id.new( :scheme =\u003e \"chef\", :id =\u003e \"\" ) )] z.create(:path =\u003e \"/nodes/#{@@ipv4}\", :acl =\u003e zk_acl, :ephemeral =\u003e true) ...  Unless the znode is meant to be wide open, obviously.\n","description":"","tags":["zookeeper","apache","chef","opscode"],"title":"Apache Zookeeper authentication","uri":"/posts/2013-06-24-apache-zookeeper-authentication/"},{"content":" I had to cut out cookbooks/nodejs from the develop into a separate repository and move develop to master.\n The problem is, there is a git repository called XYZ with the following structure:\n cookbooks  nodejs ... mysql ...   other_stuff ...  and following branches:\n refs/heads/master refs/heads/develop  I had to cut out cookbooks/nodejs from the develop into a separate repository and move develop to master. Here’s how I’ve done it:\ngit clone git@github.com:.../XYZ.git . git filter-branch --tag-name-filter cat --prune-empty --subdirectory-filter cookbooks/nodejs develop rm -Rf cookbooks rm -Rf other_stuff # move develop to master git checkout develop git branch -D master git branch master git checkout master git branch -D develop # now we're left with master only, make sure no unwanted tags are carried over: git tag # for each unwanted tag do: git tag -d tagname git remote remove origin git remote add git@.../NEWREPO.git git clone --bare . NEWREPO.git cd NEWREPO.git # create new repo on github and push: git push --mirror git@github.com:.../NEWREPO.git  Job done.\nUseful links  Detach subdirectory into separate Git repository  ","description":"","tags":["git"],"title":"Git: chopping out part of the repo into a separate repo","uri":"/posts/2013-03-22-git-chopping-out-part-of-the-repo-into-a-separate-repo/"},{"content":" I keep getting Fog::Compute::AWS::NotFound: The key pair ... does not exist error!\n How to solve this problem isn’t explained well enough anywhere.\nInstall knife-ec2 first:\nsudo gem install knife-ec2  Then I added this to my knife.rb:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  # key name, as defined in EC2 Key Pairs, # this value has to be exactly the same as the one in the management console: knife[:aws_ssh_key_id] = \"my-ec2-key\" # AWS ID/SECRET: knife[:aws_access_key_id] = \"...\" knife[:aws_secret_access_key] = \"...\" # Some defaults knife[:region] = \"eu-west-1\" knife[:availability_zone] = \"eu-west-1a\" knife[:ssh_user] = \"ubuntu\" knife[:flavor] = \"m1.medium\" # Ubuntu 12.04 LTS 64bit: knife[:image] = \"ami-f2191786\" knife[:use_sudo] = \"true\"   As I was trying to create an instance with this command:\nknife ec2 server create -G \"default\" -r 'recipe[monit]'  I kept getting following error:\nFog::Compute::AWS::NotFound: The key pair 'my-ec2-key' does not exist  It turned out, 2 hours later, in order to fix this I had to run:\nssh-add ~/.ssh/my-ec2-key.pem  It worked.\nUseful links  ServerFault: the only place where I found a mention of ssh-add with this particular issue Knife EC2 Extension – Install and Use  ","description":"I keep getting Fog::Compute::AWS::NotFound: The key pair … does not exist error","tags":["knife","ec2","chef","devops"],"title":"Setting up knife ec2","uri":"/posts/2013-03-07-setting-up-knife-ec2/"},{"content":"{% include collecttags.html %}\n I was asked to build a box with all the monitoring tools required. Chef, Logstash, Ganglia and Monit were selected. Here’s a core dump.\n In this post I’m going to describe how to setup a box containing following elements:\n Opsworks Chef Server Logstash indexer with Redis as an incoming queue Kibana Ganglia collector Monit with the log shipped to logstash M/Monit   I have this nasty habit - this whole thing is running as root. Well, Chef Server isn’t, it creates it’s own user ane group. But everything else is…\n All of those components will sit behind nginx SSL. Components that don’t provide authentication will be secured with HTTP basic auth.\nWe are going to start with Chef Server. The whole process is nicely explained in the Chef documentation. But it is scattered across a number of pages, some sort of condensed knowledge is always nice to have.\nChef Server Start with creating an A record for chef.[your domain name] pointing to your box. Then SSH to the box.\nSet some initial values, the password in the first line doesn’t really matter, it has to be changed upon first login:\nCHEF_WEBUI_ADMIN_PASSWORD=somerandompassword CHEF_RABBITMQ_CONSUMER_PASSWORD=[use-some-strong-password-here] CHEF_WEBUI_HOST=http://chef.[your domain name]:4000 set -e -x export HOME=\"/root\" export DEBIAN_FRONTEND=noninteractive  We have to install debconf-util to be able to provide the settings dutring installation. And git, we will need it later…\napt-get install -q -y debconf-utils git-core  Follow the steps from Chef docs:\necho \"deb http://apt.opscode.com/ `lsb_release -cs`-0.10 main\" | sudo tee /etc/apt/sources.list.d/opscode.list mkdir -p /etc/apt/trusted.gpg.d gpg --keyserver keys.gnupg.net --recv-keys 83EF826A gpg --export packages@opscode.com | sudo tee /etc/apt/trusted.gpg.d/opscode-keyring.gpg \u003e /dev/null apt-get update apt-get install -q -y opscode-keyring apt-get -y upgrade  This is where we add the additional steps. Set those settings to be able to do noninteractive setup:\necho chef-server-webui chef-server-webui/admin_password password $CHEF_WEBUI_ADMIN_PASSWORD | debconf-set-selections echo chef-solr chef-solr/amqp_password password $CHEF_RABBITMQ_CONSUMER_PASSWORD | debconf-set-selections echo chef chef/chef_server_url string $CHEF_WEBUI_HOST | debconf-set-selections apt-get -q -y install chef chef-server  When this is done, and it can take a bit, you can go http://[your server]:4040. Log in as admin using $CHEF_WEBUI_ADMIN_PASSWORD password. You will have to chnge it upon first login.\nnginx I am sure you would like to see Chef running. We may as well set up nginx just now. Why not. If you have ports 4000 and 4040 opened for public access, close them. You won’t need them.\napt-get install -q -y nginx  Now we need a self-signed certificate so we can serve everything via SSL:\nmkdir -p /tmp/certs cd /tmp/certs  You will have to type the password a number of times in this step. Just make sure it is always the same.\nopenssl genrsa -des3 -out myssl.key 1024 openssl req -new -key myssl.key -out myssl.csr cp myssl.key myssl.key.org openssl rsa -in myssl.key.org -out myssl.key openssl x509 -req -days 365 -in myssl.csr -signkey myssl.key -out myssl.crt cp myssl.crt /etc/ssl/certs/ cp myssl.key /etc/ssl/private/  Time to confiure nginx:\ntouch /etc/nginx/sites-available/devops ln -s /etc/nginx/sites-available/devops /etc/nginx/sites-enabled/devops [vi|vim|nano|joe] /etc/nginx/sites-enabled/devops  Paste the following content:\nupstream chef_api_local { server localhost:4000; } upstream chef_webui_local { server localhost:4040; } server { server_name chef.[your domain name]; ssl on; ssl_certificate /etc/ssl/certs/myssl.crt; ssl_certificate_key /etc/ssl/private/myssl.key; listen 443; ssl_session_timeout 5m; ssl_protocols SSLv2 SSLv3 TLSv1; ssl_ciphers ALL:!ADH:!EXPORT56:RC4+RSA:+HIGH:+MEDIUM:+LOW:+SSLv2:+EXP; ssl_prefer_server_ciphers on; root /var/www; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto https; location / { # API request incoming if ( $http_x_ops_timestamp != \"\" ){ proxy_pass http://chef_api_local; break; } #webui request incoming proxy_pass http://chef_webui_local; } }  Save the file and execute:\n/etc/init.d/nginx stop /etc/init.d/nginx start  The reload command gives some incosistent results. It doesn’t like to work every single time.\nNow you can simply go to https://chef.[your domain name].\nKibana and logstash Logstash will be used for logs aggregation. It can be set up in standalone or distributed mode. But standalone isn’t really “aggregation”. No?\nIn the distributed mode we are looking at two components. An indexer and shipper. Indexer is the end point. Somwehere where we see everything. Or where we ship to another indexer. Can be tricky, depending on the setup.\nFor now let’s just assume that we have “some shippers” and “an indexer”. Shipper is really easy, we will focus on the indexer for now. Bear with me.\nCreate an A record for kibana.[your server].\nNext, back on the server:\nmkdir /opt/kibana cd /opt/kibana git clone --branch=kibana-ruby https://github.com/rashidkpc/Kibana.git . gem install bundler bundle install  Please note - Ruby with all additional dependencies is already installed for us by Chef. Now the maual stuff:\n[vi|vim|nano|joe] /opt/kibana/KibanaConfig.rb  and change KibanaHost to 0.0.0.0. Save the file.\nNow logstash dependencies - ElasticSearch comes first:\nmkdir /opt/elasticsearch cd /opt/elasticsearch cd /tmp wget https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-0.20.5.tar.gz tar xvf elasticsearch-0.20.5.tar.gz -C /opt/elasticsearch --strip 1 rm elasticsearch-0.20.5.tar.gz  and Redis:\ncd /tmp wget http://redis.googlecode.com/files/redis-2.6.10.tar.gz tar xvf redis-2.6.10.tar.gz cd redis-2.6.10/ mkdir -p /opt/redis make PREFIX=/opt/redis install cp redis.conf /opt/redis/redis.conf  Edit the /opt/redis/redis.conf file, change port to 10000, or whatever isn’t already in use… (hint: it is at the top of the file). For whatever reason 6379 default port is already used by something.\nIn a moment, when we start Redis, it will become available to everyone. If this is a what the fuck moment for you, read the red header section right below.\nRedis password: depending on where you run this setup If you run on EC2 and you have your security rules based on security groups, I assume you know how to enable the port and you understand the restrictions around that solution. In this case you don’t have to set any passwords for Redis.\nIf you have nothing like this handy, you can simply enable Redis authentication. Or use firewall. If you decide to go for the password: edit redis.conf again. Find the line starting with # requirepass, uncomment, set strong password. Be careful when choosing the password with Redis \u003c= 2.6.10.\nMake sure you also read this - Redis AUTH command.\nUpstart We need 3 upstart services for:\n Kibana ElasticSearch Redis  Ubuntu comes with upstart already installed, we simply need the files in /etc/init.\nelasticsearch.conf description \"ElasticSearch\" start on filesystem and net-device-up IFACE=eth0 stop on shutdown respawn script sudo -u root /opt/elasticsearch/bin/elasticsearch end script  kibana.conf description \"kibana\" start on filesystem and net-device-up IFACE=eth0 stop on shutdown respawn chdir /opt/kibana script exec ruby kibana.rb end script  redis.conf description \"redis\" start on filesystem and net-device-up IFACE=eth0 stop on shutdown respawn script /opt/redis/bin/redis-server /opt/redis/redis.conf end script  You must chmod +x those files and start services  1 :\nchmod +x /etc/init/elasticsearch.conf chmod +x /etc/init/kibana.conf chmod +x /etc/init/redis.conf service elasticsearch start service kibana start service redis start    I don’t know why ElasticSearch must be started manually the first time. Just run:\n /opt/elasticsearch/bin/elasticsearch  Which will run it as a service.\nBy default Kibana runs on port 6501. But we want it to run over SSL. Edit /etc/nginx/sites-enabled/devops. Add the following upstream, at the top of the file:\nupstream kibana_local\t{ server localhost:5601; }  And this at the bottom:\nserver { server_name kibana.[your domain name]; ssl on; ssl_certificate /etc/ssl/certs/myssl.crt; ssl_certificate_key /etc/ssl/private/myssl.key; listen 443; ssl_session_timeout 5m; ssl_protocols SSLv2 SSLv3 TLSv1; ssl_ciphers ALL:!ADH:!EXPORT56:RC4+RSA:+HIGH:+MEDIUM:+LOW:+SSLv2:+EXP; ssl_prefer_server_ciphers on; root /var/www; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto https; location / { auth_basic \"Resricted - Kibana\"; auth_basic_user_file /etc/nginx/htpasswd; proxy_pass http://kibana_local; } }  One thing to note over here is the use of basic authentication. We must create a user:\nhtpasswd -c -d /etc/nginx/htpasswd your-user-name  You will have to provide the password, 8 characters max. We will reuse the same htpasswd for ganglia a bit later.\nTime to restart nginx again.\n/etc/init.d/nginx reload  You should now be able to go to https://kibana.[your domain name].\nLogstash indexer This is really straightforward. Our logstash will read from local Redis. We will store all input/output configuration in a designated folder. By default our indexer will process incoming monit logs.\nmkdir -p /opt/logstash/conf.d cd /opt/logstash wget https://logstash.objects.dreamhost.com/release/logstash-1.1.9-monolithic.jar  Save this file in /opt/logstash/conf.d/monit.conf:\ninput { redis { port =\u003e 10000 type =\u003e \"monit-input\" key =\u003e \"monit_logs\" data_type =\u003e \"list\" format =\u003e \"json_event\" } } output { elasticsearch { host =\u003e \"127.0.0.1\" } }  And create an upstart service for it, save the file in /etc/init/logstash-indexer.conf:\ndescription \"logstash indexer\" start on filesystem and net-device-up IFACE=eth0 stop on shutdown respawn chdir /opt/logstash script java -jar /opt/logstash/logstash-1.1.9-monolithic.jar agent -v -f /opt/logstash/conf.d/ end script  Start the service:\nchmod +x /etc/init/logstash-indexer.conf service logstash-indexer start  Ganglia collector (master) Create an A record for ganglia.[your domain name]. Point it to the server.\nWe need just a few packages more on the box:\napt-get install -q -y ganglia-monitor gmetad ganglia-webfrontend php5-cgi  Create a www-data user and group:\nmkdir -p /home/www-data groupadd -g 3320 www-data useradd -m -d /home/www-data -s /bin/bash -u 3320 -g 3320 www-data chown www-data:www-data /home/www-data chown -R www-data:www-data /usr/share/ganglia-webfrontend  Ganglia is written in PHP. Nginx needs FastCGI to process PHP. We must create a FastCGI service. Save this file in /etc/init.d/nginx-fastcgi:\n#!/bin/bash BIND=127.0.0.1:9000 USER=www-data PHP_FCGI_CHILDREN=15 PHP_FCGI_MAX_REQUESTS=1000 PHP_CGI=/usr/bin/php-cgi PHP_CGI_NAME=`basename $PHP_CGI` PHP_CGI_ARGS=\"- USER=$USER PATH=/usr/bin PHP_FCGI_CHILDREN=$PHP_FCGI_CHILDREN PHP_FCGI_MAX_REQUESTS=$PHP_FCGI_MAX_REQUESTS $PHP_CGI -b $BIND\" RETVAL=0 start() { echo -n \"Starting PHP FastCGI: \" start-stop-daemon --quiet --start --background --chuid \"$USER\" --exec /usr/bin/env -- $PHP_CGI_ARGS RETVAL=$? echo \"$PHP_CGI_NAME.\" } stop() { echo -n \"Stopping PHP FastCGI: \" killall -q -w -u $USER $PHP_CGI RETVAL=$? echo \"$PHP_CGI_NAME.\" } case \"$1\" in start) start ;; stop) stop ;; restart) stop start ;; *) echo \"Usage: php-fastcgi {start|stop|restart}\" exit 1 ;; esac exit $RETVAL  Execute:\nchmod +x /etc/init.d/nginx-fastcgi update-rc.d /etc/init.d/nginx-fastcgi /etc/init.d/nginx-fastcgi start  Final change in the /etc/nginx/sites-enabled/devops file. Add this at the bottom:\nserver { server_name ganglia.[your domain name]; ssl on; ssl_certificate /etc/ssl/certs/myssl.crt; ssl_certificate_key /etc/ssl/private/myssl.key; listen 443; ssl_session_timeout 5m; ssl_protocols SSLv2 SSLv3 TLSv1; ssl_ciphers ALL:!ADH:!EXPORT56:RC4+RSA:+HIGH:+MEDIUM:+LOW:+SSLv2:+EXP; ssl_prefer_server_ciphers on; location / { auth_basic \"Resricted - Ganglia\"; auth_basic_user_file /etc/nginx/htpasswd; } location ~ \\.php$ { include /etc/nginx/fastcgi_params; fastcgi_split_path_info ^(.+\\.php)(/.+)$; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME /usr/share/ganglia-webfrontend$fastcgi_script_name; } }  And restart nginx for the final time.\n/etc/init.d/nginx reload  M/Monit Installing M/Monit is really easy.\nmkidr /opt wget http://mmonit.com/dist/mmonit-2.4-linux-x64.tar.gz tar xvf mmonit-2.4-linux-x64.tar.gz ln -s /opt/mmonit-2.4 /opt/mmonit rm mmonit-2.4-linux-x64.tar.gz  Upstart job, save it as /etc/init/mmonit.conf:\ndescription \"mmonit\" start on filesystem and net-device-up IFACE=eth0 stop on shutdown respawn script /opt/mmonit/bin/mmonit start end script pre-stop script /opt/mmonit/bin/mmonit stop end script  Make it executable and run:\nchmod +x /etc/init/mmonit.conf service mmonit start  Create A record for mmonit.[your domain name] and update nginx config, at the top of the file add:\nupstream mmonit_local { server localhost:8080; }  And then, at the bottom:\nserver { server_name mmonit.[your domain name]; ssl on; ssl_certificate /etc/ssl/certs/myssl.crt; ssl_certificate_key /etc/ssl/private/myssl.key; listen 443; ssl_session_timeout 5m; ssl_protocols SSLv2 SSLv3 TLSv1; ssl_ciphers ALL:!ADH:!EXPORT56:RC4+RSA:+HIGH:+MEDIUM:+LOW:+SSLv2:+EXP; ssl_prefer_server_ciphers on; root /var/www; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto https; location / { proxy_pass http://mmonit_local; } }  Reload nginx config:\n/etc/init.d/nginx reload  Wrap up In this post we have covered setting up a collector box. We now have:\n https://chef.[your domain name], runs thr API and the WebUI, supports it’s own authentication and OpenID https://kibana.[your domain name], web frontend for logstash, protected with basic authentication https://ganglia.[your domain name], web frontend for ganglia, protected with basic authentication, uses the same credentials as kibana https://mmonit.[your domain name], web frontend for mmonit, protected with it’s own authentication  We have also limited a number of ports opened on the firewall / security group. These have to be opened:\n 22 (SSH) 443 (HTTPS) 8080 (HTTP) - M/Monit still redirects to 8080 10000 (Redis)  In the next post I’m going to cover setting up knife so we can start deploying the infrastructure with it. Our infrastructure is going to contain:\n monit recipe ganglia client recipe logstash shipper recipe  We will use those to bootstrap a core setup box.\nUseful links Chef:\n Installing Chef Server on Ubuntu using Packages Chef Resources A Brief Chef Tutorial (From Concentrate)  Logstash:\n Logstash website  Nginx:\n Fcgi Example  ","description":"I was asked to build a box with all the monitoring tools required. Chef, Logstash and Ganglia were suggested. Here’s a brain dump.","tags":["devops","redis","logstash","ganglia","monit"],"title":"DevOps box","uri":"/posts/2013-03-05-devops-box/"}]
