[{"content":"","description":"","tags":null,"title":"gruchalski.com","uri":"/"},{"content":"","description":"","tags":null,"title":"istio","uri":"/tags/istio/"},{"content":"","description":"","tags":null,"title":"Posts","uri":"/posts/"},{"content":"","description":"","tags":null,"title":"Tags","uri":"/tags/"},{"content":"You are upgrading Istio and a message like this pops up:\nWARNING: Istio is being downgraded from 1.20.0 to 1.19.3. Before upgrading, you may wish to use 'istioctl x precheck' to check for upgrade warnings. Not very obvious what happens the first time one comes across this.\nThe reason is simple:\nThe control plane is at version 1.20.0. The istioctl binary is at version 1.19.3. The fix: update istioctl to the matching 1.20.0 version to remove this warning.\n","description":"when one upgrades Istio and sees such a message","tags":["istio"],"title":"Warning: Istio is being downgraded","uri":"/posts/2023-11-20-warning-istio-is-being-downgraded/"},{"content":" Update 2023.11.10: The original post was long and contained a lot of code, right in your face.\nSince I tidied up the accompanying repository, I am making the following changes:\nThe default ingress is turned on. Removing large code blocks, replacing them with invocation and functional description. Splitting the VM installation process into two separate steps: VM create and bootstrap. Update 2023.11.13: I have updated the Istio installation:\nUse an explicit revision during installation revision, adapt components to use the revision. Work documented in the Revisioned Istio pull request. I am going to connect a VM to the Istio mesh running in the Kubernetes cluster.\nI will need:\nA VM. A Kubernetes cluster. Containers. This is not a job for KinD because I need a VM. Since I’m on a macOS, it’s Multipass for me today.\nI am going to run:\nA k3s cluster on Multipass. A VM on Multipass, same network as the k3s cluster. After setting up the k3s cluster I follow the steps from Istio documentation: Virtual Machine Installation1.\nThe outcome is a working Istio mesh with a VM in the mesh supporting bidirectional traffic, and a short dive into network policies.\ntable of contents tools working directory environment configuration setting up the k3s cluster setting up the client verify the cluster install istioctl istio installation eastwest gateway create the vm the workload group workload group ca_addr the vm: arm64 bootstrap caveat the deb package arm64 binaries bootstrap the vm validate the vm validate DNS resolution validating communication create and configure the namespace on amd64 host on arm64 host hello world pods are running checking vm to mesh connectivity routing mesh traffic to the vm workload entry is unhealthy fix it by starting the workload verify connectivity with curl enable strict tls network policies caveat: cannot select a workload entry in a network policy guarding the vm from the source of traffic namespace network boundary for network policies cleaning up summary tools Besides the standard kubectl:\nmultipass: macOS brew install multipass, Linux official instructions2 or follow instructions for your distribution, git: to fetch the additional data, yq: follow an official guide3, docker or podman if you are on an arm64-based host, istioctl: instructions further in the article. working directory 1 2 mkdir -p ~/.project \u0026\u0026 cd ~/.project git clone https://github.com/radekg/istio-vms.git . This repository brings all tools used further in this post: shell programs and additional artifacts used during the rollout of the VM.\nAll commands further in this article assume that you remain in the newly created directory.\nYou can place it wherever you like, name it however you like, but that’s the working directory.\nenvironment configuration A few settings. Mainly:\nwhere’s the data stored the temp directory, some artifacts will be downloaded and extracted to disk where’s the kubeconfig? settings: operating system to use Istio version to use VM resource settings VM-related configuration Settings exist in the run.env file:\n1 cat run.env 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 #!/bin/bash env_base=\"$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\" \u0026\u0026 pwd )\" export BIN_DIR=\"${env_base}/.bin/\" mkdir -p \"${BIN_DIR}\" [[ \":$PATH:\" != *\":${BIN_DIR}:\"* ]] \u0026\u0026 export PATH=\"${BIN_DIR}:${PATH}\" export DATA_DIR=\"${env_base}/.data/\" mkdir -p \"${DATA_DIR}\" export TEMP_DIR=\"${env_base}/.tmp/\" mkdir -p \"${TEMP_DIR}\" export KUBECONFIG=\"${DATA_DIR}/.kubeconfig\" export CONTAINER_TOOL=${CONTAINER_TOOL:-docker} # Settings: export RUN_OS=22.04 export ISTIO_VERSION=${ISTIO_VERSION:-1.19.3} export ISTIO_REVISION=$(echo $ISTIO_VERSION | tr '.' '-') # Resources: export CPU_MASTER=2 export CPU_WORKER=2 export DISK_MASTER=4G export DISK_WORKER=8G export MEM_MASTER=1G export MEM_WORKER=8G # VM-related configuration: export ISTIO_NAMESPACE=${ISTIO_NAMESPACE:-istio-system} export ISTIO_MESH_ID=\"vmmesh\" export WORKLOAD_VM_NAME=vm-istio-external-workload export ISTIO_CLUSTER=test export VM_APP=\"external-app\" export VM_NAMESPACE=\"vmns\" export SERVICE_ACCOUNT=\"vmsa\" export CLUSTER_NETWORK=\"kube-network\" export VM_NETWORK=\"vm-network\" # East-west gateway exposes different ports than the default ingress. # When both gateways need to be in use, and the cluster has one load balancer, # these need to differ from the default ingress agteway. export DEFAULT_PORT_STATUS=15021 export DEFAULT_PORT_TLS_ISTIOD=15012 export DEFAULT_PORT_TLS_WEBHOOK=15017 export EWG_PORT_STATUS=15022 export EWG_PORT_TLS_ISTIOD=15013 export EWG_PORT_TLS_WEBHOOK=15018 # However, if there's a run.env file in pwd, use that one: pwd=`pwd` if [ \"${pwd}\" != \"${env_base}\" ]; then [ -f \"${pwd}/run.env\" ] \u0026\u0026 source \"${pwd}/run.env\" \u0026\u0026 \u003e\u00262 echo \"configured from ${pwd}/run.env\" fi setting up the k3s cluster I am starting a k3s cluster with one control plane and two workers. Traefik is disabled because we use Istio. Also, the default k3s load balancer—Klipper—is on. This program requires multipass.\n1 ./install.k3s.sh Once the cluster is running, the kubeconfig is written to .data/.kubeconfig.\nPlease be mindful of the rather high resource requirement. Adjust as you see fit.\nThe cluster can be deleted with:\n1 ./install.k3s.sh --delete and recreated (removed and created again) with:\n1 ./install.k3s.sh --recreate setting up the client To have your kubectl and and other tools use the correct kubeconfig:\n1 source run.env This will set your KUBECONFIG environment variable and some other various settings.\nverify the cluster 1 kubectl get nodes Something along the lines of:\nNAME STATUS ROLES AGE VERSION k3s-master Ready control-plane,master 10m v1.27.7+k3s1 k3s-worker-1 Ready \u003cnone\u003e 9m26s v1.27.7+k3s1 k3s-worker-2 Ready \u003cnone\u003e 9m18s v1.27.7+k3s1 install istioctl This program downloads istioctl for ISTIO_VERSION and ISTIO_ARCH, and places it in the .bin/ directory.\nISTIO_VERSION: Istio version, default 1.19.3 ISTIO_ARCH: one of \u003c osx-arm64, osx-amd64, linux-armv7, linux-arm64, linux-amd64 \u003e, default osx-arm64 1 ./install.istioctl.sh Verify:\n1 istioctl version no ready Istio pods in \"istio-system\" 1.19.3 1 which istioctl | sed 's!'$(pwd)'/!!' .bin//istioctl Double / is not a mistake.\nistio installation This is where I start to follow the Istio documentation1.\nThis program installs and configures Istio from the IstioOperator resource. Install Istio:\n1 ./install.istio.sh Verify:\n1 kubectl get services -n \"${ISTIO_NAMESPACE}\" NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istiod-1-19-3 ClusterIP 10.43.255.139 \u003cnone\u003e 15010/TCP,15012/TCP,443/TCP,15014/TCP 39s 1 istioctl tag list --istioNamespace=\"${ISTIO_NAMESPACE}\" TAG REVISION NAMESPACES default 1-19-3 eastwest gateway This program installs the eastwest gateway. This program depends on DEFAULT_PORT_* and EWG_PORT_* environment variables.\n1 ./install.eastwest.gateway.sh The outcome should be similar to this:\n1 kubectl get services -n \"${ISTIO_NAMESPACE}\" -w NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istiod-1-19-3 ClusterIP 10.43.255.139 \u003cnone\u003e 15010/TCP,15012/TCP,443/TCP,15014/TCP 17m istio-ingressgateway LoadBalancer 10.43.217.75 192.168.64.60,192.168.64.61,192.168.64.62 15021:31941/TCP,80:30729/TCP,443:32187/TCP 11m istio-eastwestgateway LoadBalancer 10.43.106.169 192.168.64.60,192.168.64.61,192.168.64.62 15022:31036/TCP,15443:32297/TCP,15013:31263/TCP,15018:32660/TCP 15s If your eastwest gateway remains in the pending state, ensure that a DEFAULT_PORT is not equal to the corresponding EWG_PORT.\n1 kubectl get services -n \"${ISTIO_NAMESPACE}\" -w NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istiod-1-19-3 ClusterIP 10.43.255.139 \u003cnone\u003e 15010/TCP,15012/TCP,443/TCP,15014/TCP 17m istio-ingressgateway LoadBalancer 10.43.217.75 192.168.64.60,192.168.64.61,192.168.64.62 15021:31941/TCP,80:30729/TCP,443:32187/TCP 11m istio-eastwestgateway LoadBalancer 10.43.106.169 \u003cpending\u003e 15021:31036/TCP,15443:32297/TCP,15013:31263/TCP,15018:32660/TCP 15s In the example above, the problem is 15021:31036/TCP: 15021 is already used by the default ingress gateway.\ncreate the vm Before a workload group can be created, we need to have the VM because, in this case, the IP address is needed for the workload group. To create the VM:\n1 ./install.vm.create.sh This command supports –delete and –recreate flags. Only the multipass VM is manipulated. Other files remain on disk.\nthe workload group This program finds the VM IP address and uses it to create a WorkloadGroup Kubernetes resource. When ready, the program downloads all files required by the VM to join the mesh (istioctl x workload entry configure). Files are stored in .data/workload-files.\n1 ./install.workload.sh Verify:\n1 cat .data/workload-files/hosts output similar to:\n192.168.64.60 istiod-1-19-3.istio-system.svc If there are no hosts here, your eastwest gateway is most likely not working correctly.\nworkload group ca_addr The CA_ADDR environment variable exported in the cluster.env file points by default to the Istio TLS port, the 15012.\n1 2 ./install.workload.sh --no-fix-ca cat .data/workload-files/cluster.env | grep CA_ADDR CA_ADDR='istiod-1-19-3.istio-system.svc:15012' The service name is correct but the port isn’t. I hoped that the tool would pick up the port from the ingress gateway service but the help for istioctl x workload entry configure says:\n--ingressService string Name of the Service to be used as the ingress gateway, in the format \u003cservice\u003e.\u003cnamespace\u003e. If no namespace is provided, the default istio-system namespace will be used. (default \"istio-eastwestgateway\") Since that’s our gateway name, it obviously doesn’t detect ports. By default install.workload.sh program fixes that by simply appending the CA_ADDR to use to the end of the cluster.env file.\n1 cat .data/workload-files/cluster.env | grep CA_ADDR CA_ADDR='istiod-1-19-3.istio-system.svc:15012' CA_ADDR=istiod-'1-19-3.istio-system.svc:15013' the vm: arm64 bootstrap caveat For example if you are on an M2 mac, like me… Istio documentation instructs to install Istio sidecar on the VM using a downloaded deb package. The problem is, Multipass runs an arm64 build of Ubuntu and the deb package is available only for the amd64 architecture. Eventually, trying to start Istio on the VM, you’d end up with:\n1 sudo dpkg -i istio-sidecar.deb dpkg: error processing archive istio-sidecar.deb (--install): package architecture (amd64) does not match system (arm64) Errors were encountered while processing: istio-sidecar.deb As a workaround, I have to replicate the work done in the deb package but I have to source arm64 binaries.\nthe deb package I decompressed it:\n1 2 3 4 5 source run.env wget \"https://storage.googleapis.com/istio-release/releases/${ISTIO_VERSION}/deb/istio-sidecar.deb\" \\ -O \"${DATA_DIR}/istio-sidecar/istio-sidecar.deb\" tar xf \"${DATA_DIR}/istio-sidecar/istio-sidecar.deb\" -C \"${DATA_DIR}/istio-sidecar/\" tar xf \"${DATA_DIR}/istio-sidecar/data.tar.gz\" -C \"${DATA_DIR}/istio-sidecar/\" and kept the following:\n1 git ls-tree -r HEAD ${DATA_DIR}/istio-sidecar 100644 blob c18ec3ce73f52fafe05585de91cd4cda2cdf3951\t.data/istio-sidecar/lib/systemd/system/istio.service 100755 blob e022fbb08d4375a66276263b70380230e4702dbe\t.data/istio-sidecar/usr/local/bin/istio-start.sh 100644 blob ab4bbffd39a7462db68312b7049828c7b4c1d673\t.data/istio-sidecar/var/lib/istio/envoy/envoy_bootstrap_tmpl.json 100644 blob fc42e5483094378ca0f0b00cd52f81d1827531cb\t.data/istio-sidecar/var/lib/istio/envoy/sidecar.env arm64 binaries Skip if not on an arm64 host.\nTwo binaries have to be replaced with their arm64 versions:\n/usr/local/bin/envoy /usr/local/bin/pilot-agent For me, the easiest way I could come up with was to:\nDownload the linux/arm64 Istio proxyv2 Docker image. Create a container, don’t start it. Copy the files out of the file system. Remove the container. Reference exported filesystem for required arm64 binaries. 1 ./install.arm64.binary.patches.sh The default CONTAINER_TOOL depends on your run.env and equals to docker when not set. To use podman:\n1 CONTAINER_TOOL=podman ./install.arm64.binary.patches.sh bootstrap the vm This program deploys all the files required by the VM to join the mesh, moves them to the right places, and configures to VM to handle the Istio sidecar.\n1 ./install.vm.bootstrap.sh validate the vm Get the shell on the VM:\n1 multipass exec vm-istio-external-workload -- bash On the VM ubuntu@vm-istio-external-workload:~$, regardless of the fact that we set the CA_ADDR, we still have to use the correct value for the PILOT_ADDRESS.\n1 2 cd / \u0026\u0026 sudo PILOT_ADDRESS=istiod-1-19-3.istio-system.svc:15013 istio-start.sh # cd / \u0026\u0026 sudo PILOT_ADDRESS=istiod-${ISTIO_REVISION}.${ISTIO_NAMESPACE}.svc:${EWG_PORT_TLS_ISTIOD} istio-start.sh However, this is also already dealt with in install.workload.sh. We can start the program with:\n1 cd / \u0026\u0026 sudo istio-start.sh 2023-11-01T01:27:02.836001Z\tinfo\tRunning command: iptables -t nat -D PREROUTING -p tcp -j ISTIO_INBOUND 2023-11-01T01:27:02.837879Z\tinfo\tRunning command: iptables -t mangle -D PREROUTING -p tcp -j ISTIO_INBOUND 2023-11-01T01:27:02.839355Z\tinfo\tRunning command: iptables -t nat -D OUTPUT -p tcp -j ISTIO_OUTPUT ... 2023-11-01T01:27:04.000569Z\terror\tcitadelclient\tFailed to load key pair open etc/certs/cert-chain.pem: no such file or directory 2023-11-01T01:27:04.004712Z\tinfo\tcache\tgenerated new workload certificate\tlatency=118.57166ms ttl=23h59m58.995289161s 2023-11-01T01:27:04.004744Z\tinfo\tcache\tRoot cert has changed, start rotating root cert 2023-11-01T01:27:04.004759Z\tinfo\tads\tXDS: Incremental Pushing ConnectedEndpoints:2 Version: 2023-11-01T01:27:04.004885Z\tinfo\tcache\treturned workload certificate from cache\tttl=23h59m58.995116686s 2023-11-01T01:27:04.004954Z\tinfo\tcache\treturned workload trust anchor from cache\tttl=23h59m58.995045987s 2023-11-01T01:27:04.005150Z\tinfo\tcache\treturned workload trust anchor from cache\tttl=23h59m58.994850182s 2023-11-01T01:27:04.006124Z\tinfo\tads\tSDS: PUSH request for node:vm-istio-external-workload.vmns resources:1 size:4.0kB resource:default 2023-11-01T01:27:04.006176Z\tinfo\tads\tSDS: PUSH request for node:vm-istio-external-workload.vmns resources:1 size:1.1kB resource:ROOTCA 2023-11-01T01:27:04.006204Z\tinfo\tcache\treturned workload trust anchor from cache\tttl=23h59m58.99379629s If your istio-start.sh command doesn’t produce any output after iptables output:\n-A OUTPUT -p udp --dport 53 -d 127.0.0.53/32 -j REDIRECT --to-port 15053 COMMIT 2023-11-09T11:21:47.136622Z\tinfo\tRunning command: iptables-restore --noflush (hangs here) CTRL+C, exec to the VM, and rerun last command again.\nvalidate DNS resolution Open another terminal:\n1 multipass exec vm-istio-external-workload -- bash On the VM ubuntu@vm-istio-external-workload:~$:\n1 2 dig istiod-1-19-3.istio-system.svc # dig istiod-${ISTIO_REVISION}.${ISTIO_NAMESPACE}.svc ; \u003c\u003c\u003e\u003e DiG 9.18.12-0ubuntu0.22.04.3-Ubuntu \u003c\u003c\u003e\u003e istiod-1-19-3.istio-system.svc ;; global options: +cmd ;; Got answer: ;; -\u003e\u003eHEADER\u003c\u003c- opcode: QUERY, status: NOERROR, id: 27953 ;; flags: qr aa rd; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 0 ;; WARNING: recursion requested but not available ;; QUESTION SECTION: ;istiod-1-19-3.istio-system.svc.\tIN\tA ;; ANSWER SECTION: istiod-1-19-3.istio-system.svc. 30\tIN\tA\t10.43.26.124 ;; Query time: 0 msec ;; SERVER: 127.0.0.53#53(127.0.0.53) (UDP) ;; WHEN: Wed Nov 01 02:31:59 CET 2023 ;; MSG SIZE rcvd: 80 The IP address should in the answer section be equal to the cluster IP of the service:\n1 kubectl get service \"istiod-${ISTIO_REVISION}\" -n \"${ISTIO_NAMESPACE}\" NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istiod-1-19-3 ClusterIP 10.43.26.124 \u003cnone\u003e 15010/TCP,15012/TCP,443/TCP,15014/TCP 19m It’s 10.43.26.124 in both cases, the DNS resolution is working.\nvalidating communication Deploy a sample application allowing us to validate the connection from the VM to the mesh.\ncreate and configure the namespace 1 2 kubectl create namespace sample kubectl label namespace sample \"istio.io/rev=${ISTIO_REVISION}\" on amd64 host 1 kubectl apply -n sample -f https://raw.githubusercontent.com/istio/istio/release-1.19/samples/helloworld/helloworld.yaml on arm64 host 1 2 3 curl --silent https://raw.githubusercontent.com/istio/istio/release-1.19/samples/helloworld/helloworld.yaml \\ | sed -E 's!istio/examples!radekg/examples!g' \\ | kubectl apply -n sample -f - Again, both example images published by Istio do not exist for the linux/arm64 architecture, I build them from my own Dockerfile for linux/arm64. The source code is here4.\nhello world pods are running 1 kubectl get pods -n sample -w NAME READY STATUS RESTARTS AGE helloworld-v1-cff64bf8c-z5nq5 0/2 PodInitializing 0 8s helloworld-v2-9fdc9f56f-tbmk8 0/2 PodInitializing 0 8s helloworld-v1-cff64bf8c-z5nq5 1/2 Running 0 20s helloworld-v2-9fdc9f56f-tbmk8 1/2 Running 0 21s helloworld-v1-cff64bf8c-z5nq5 2/2 Running 0 21s helloworld-v2-9fdc9f56f-tbmk8 2/2 Running 0 22s checking vm to mesh connectivity In a shell on a VM ubuntu@vm-istio-external-workload:-$:\n1 curl -v helloworld.sample.svc:5000/hello * Trying 10.43.109.101:5000... * Connected to helloworld.sample.svc (10.43.109.101) port 5000 (#0) \u003e GET /hello HTTP/1.1 \u003e Host: helloworld.sample.svc:5000 \u003e User-Agent: curl/7.81.0 \u003e Accept: */* \u003e * Mark bundle as not supporting multiuse \u003c HTTP/1.1 200 OK \u003c server: envoy \u003c date: Wed, 01 Nov 2023 01:59:55 GMT \u003c content-type: text/html; charset=utf-8 \u003c content-length: 59 \u003c x-envoy-upstream-service-time: 88 \u003c Hello version: v2, instance: helloworld-v2-9fdc9f56f-tbmk8 * Connection #0 to host helloworld.sample.svc left intact The service responded, the VM can reach services in the mesh.\nrouting mesh traffic to the vm Create a service pointing at the workload group. This sets up the route to the VM service for a port. The example uses hardcoded values but it would be a simple job to makes those configurable.\n1 ./install.route.to.vm.sh Find the workload entry, this exists only when Istio sidecar is running in the VM.\nworkload entry is unhealthy 1 kubectl get workloadentry -n \"${VM_NAMESPACE}\" NAME AGE ADDRESS external-app-192.168.64.64-vm-network 2m33s 192.168.64.64 Check its status, it will be unhealthy:\n1 kubectl get workloadentry external-app-192.168.64.64-vm-network -n \"${VM_NAMESPACE}\" -o yaml | yq '.status' 1 2 3 4 5 6 conditions: - lastProbeTime: \"2023-11-01T21:04:50.343243250Z\" lastTransitionTime: \"2023-11-01T21:04:50.343245959Z\" message: 'Get \"http://localhost:8000/\": dial tcp 127.0.0.6:0-\u003e127.0.0.1:8000: connect: connection refused' status: \"False\" type: Healthy fix it by starting the workload The reason why it is unhealthy is because the service on the VM isn’t running. Start a simple HTTP server to fix this, on the VM ubuntu@vm-istio-external-workload:-$:\n1 python3 -m http.server Serving HTTP on 0.0.0.0 port 8000 (http://0.0.0.0:8000/) ... 127.0.0.6 - - [01/Nov/2023 22:44:25] \"GET / HTTP/1.1\" 200 - 127.0.0.6 - - [01/Nov/2023 22:44:30] \"GET / HTTP/1.1\" 200 - 127.0.0.6 - - [01/Nov/2023 22:44:35] \"GET / HTTP/1.1\" 200 - ... Almost immediately we see requests arriving. This is the health check. Istio sidecar on the VM logged:\n2023-11-01T21:04:50.337302Z\tinfo\thealthcheck\tfailure threshold hit, marking as unhealthy: Get \"http://localhost:8000/\": dial tcp 127.0.0.6:0-\u003e127.0.0.1:8000: connect: connection refused 2023-11-01T21:32:12.943221Z\tinfo\txdsproxy\tconnected to upstream XDS server: istiod-1-19-3.istio-system.svc:15012 2023-11-01T21:44:25.343463Z\tinfo\thealthcheck\tsuccess threshold hit, marking as healthy The status of the workload entry has changed:\n1 kubectl get workloadentry external-app-192.168.64.64-vm-network -n \"${VM_NAMESPACE}\" -o yaml | yq '.status' 1 2 3 4 5 conditions: - lastProbeTime: \"2023-11-01T21:44:25.339260737Z\" lastTransitionTime: \"2023-11-01T21:44:25.339264070Z\" status: \"True\" type: Healthy verify connectivity with curl Finally, run an actual command to verify:\n1 kubectl run vm-response-test -n sample --image=curlimages/curl:8.4.0 --rm -i --tty -- sh If you don't see a command prompt, try pressing enter. ~ $ Pay attention to the namespace used in the last command above. The curl pod must be launched in an Istio-enabled namespace, and sample already existed.\nExecute the following command in that terminal:\n1 2 curl -v http://external-app.vmns.svc:8000/ # Use the VM_NAMESPACE from run.env * Trying 10.43.122.27:8000... * Connected to external-app.vmns.svc (10.43.122.27) port 8000 \u003e GET / HTTP/1.1 \u003e Host: external-app.vmns.svc:8000 \u003e User-Agent: curl/8.4.0 \u003e Accept: */* \u003e \u003c HTTP/1.1 200 OK \u003c server: envoy \u003c date: Wed, 01 Nov 2023 22:10:20 GMT \u003c content-type: text/html; charset=utf-8 \u003c content-length: 768 \u003c x-envoy-upstream-service-time: 7 \u003c \u003c!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01//EN\" \"http://www.w3.org/TR/html4/strict.dtd\"\u003e \u003chtml\u003e \u003chead\u003e \u003cmeta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\"\u003e \u003ctitle\u003eDirectory listing for /\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e \u003ch1\u003eDirectory listing for /\u003c/h1\u003e \u003chr\u003e \u003cul\u003e \u003cli\u003e\u003ca href=\".bash_history\"\u003e.bash_history\u003c/a\u003e\u003c/li\u003e \u003cli\u003e\u003ca href=\".bash_logout\"\u003e.bash_logout\u003c/a\u003e\u003c/li\u003e \u003cli\u003e\u003ca href=\".bashrc\"\u003e.bashrc\u003c/a\u003e\u003c/li\u003e \u003cli\u003e\u003ca href=\".cache/\"\u003e.cache/\u003c/a\u003e\u003c/li\u003e \u003cli\u003e\u003ca href=\".profile\"\u003e.profile\u003c/a\u003e\u003c/li\u003e \u003cli\u003e\u003ca href=\".ssh/\"\u003e.ssh/\u003c/a\u003e\u003c/li\u003e \u003cli\u003e\u003ca href=\".sudo_as_admin_successful\"\u003e.sudo_as_admin_successful\u003c/a\u003e\u003c/li\u003e \u003cli\u003e\u003ca href=\"lib/\"\u003elib/\u003c/a\u003e\u003c/li\u003e \u003cli\u003e\u003ca href=\"usr/\"\u003eusr/\u003c/a\u003e\u003c/li\u003e \u003cli\u003e\u003ca href=\"var/\"\u003evar/\u003c/a\u003e\u003c/li\u003e \u003cli\u003e\u003ca href=\"workload/\"\u003eworkload/\u003c/a\u003e\u003c/li\u003e \u003c/ul\u003e \u003chr\u003e \u003c/body\u003e \u003c/html\u003e * Connection #0 to host external-app.vmns.svc left intact enable strict tls 1 2 3 4 5 6 7 8 9 10 kubectl apply -f - \u003c\u003cEOP apiVersion: security.istio.io/v1beta1 kind: PeerAuthentication metadata: name: default namespace: ${ISTIO_NAMESPACE} spec: mtls: mode: STRICT EOP There are no observable changes.\nnetwork policies caveat: cannot select a workload entry in a network policy Because a network policy selects pods using .spec.podSelector, and we have no pods—we have a workload entry instead—we are not able to attach network policies to the VM. The following has no effect:\n1 2 3 4 5 6 7 8 9 10 11 12 kubectl apply -f - \u003c\u003cEOF kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: deny-all namespace: ${VM_NAMESPACE} spec: podSelector: matchLabels: app: external-app ingress: [] EOF 1 kubectl run vm-response-test -n sample --image=curlimages/curl:8.4.0 --rm -i --tty -- sh in that shell:\n1 2 curl -v http://external-app.vmns.svc:8000/ # Use the VM_NAMESPACE from run.env * Trying 10.43.122.27:8000... * Connected to external-app.vmns.svc (10.43.122.27) port 8000 \u003e GET / HTTP/1.1 \u003e Host: external-app.vmns.svc:8000 \u003e User-Agent: curl/8.4.0 \u003e Accept: */* \u003e \u003c HTTP/1.1 200 OK \u003c server: envoy ... Cosider future work: is this working when using Istio CNI?\nguarding the vm from the source of traffic namespace 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 kubectl apply -n sample -f - \u003c\u003cEOF kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: deny-egress-tovm spec: podSelector: {} policyTypes: - Egress egress: - to: - namespaceSelector: matchExpressions: - key: namespace operator: NotIn values: [\"${VM_NAMESPACE}\"] EOF 1 kubectl run vm-response-test -n sample --image=curlimages/curl:8.4.0 --rm -i --tty -- sh in that shell:\n1 2 curl http://external-app.vmns.svc:8000/ # Use the VM_NAMESPACE from run.env upstream connect error or disconnect/reset before headers. retried and the latest reset reason: remote connection failure, transport failure reason: delayed connect error: 111~ $ ^C ~ $ exit Session ended, resume using 'kubectl attach vm-response-test -c vm-response-test -i -t' command when the pod is running pod \"vm-response-test\" deleted 1 kubectl delete networkpolicy deny-egress-tovm -n sample 1 kubectl run vm-response-test -n sample --image=curlimages/curl:8.4.0 --rm -i --tty -- sh in that shell:\n1 2 curl http://external-app.vmns.svc:8000/ # Use the VM_NAMESPACE from run.env \u003c!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01//EN\" \"http://www.w3.org/TR/html4/strict.dtd\"\u003e \u003chtml\u003e \u003chead\u003e \u003cmeta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\"\u003e \u003ctitle\u003eDirectory listing for /\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e \u003ch1\u003eDirectory listing for /\u003c/h1\u003e \u003chr\u003e \u003cul\u003e \u003cli\u003e\u003ca href=\".bash_history\"\u003e.bash_history\u003c/a\u003e\u003c/li\u003e \u003cli\u003e\u003ca href=\".bash_logout\"\u003e.bash_logout\u003c/a\u003e\u003c/li\u003e \u003cli\u003e\u003ca href=\".bashrc\"\u003e.bashrc\u003c/a\u003e\u003c/li\u003e \u003cli\u003e\u003ca href=\".cache/\"\u003e.cache/\u003c/a\u003e\u003c/li\u003e \u003cli\u003e\u003ca href=\".profile\"\u003e.profile\u003c/a\u003e\u003c/li\u003e \u003cli\u003e\u003ca href=\".ssh/\"\u003e.ssh/\u003c/a\u003e\u003c/li\u003e \u003cli\u003e\u003ca href=\".sudo_as_admin_successful\"\u003e.sudo_as_admin_successful\u003c/a\u003e\u003c/li\u003e \u003cli\u003e\u003ca href=\"lib/\"\u003elib/\u003c/a\u003e\u003c/li\u003e \u003cli\u003e\u003ca href=\"usr/\"\u003eusr/\u003c/a\u003e\u003c/li\u003e \u003cli\u003e\u003ca href=\"var/\"\u003evar/\u003c/a\u003e\u003c/li\u003e \u003cli\u003e\u003ca href=\"workload/\"\u003eworkload/\u003c/a\u003e\u003c/li\u003e \u003c/ul\u003e \u003chr\u003e \u003c/body\u003e \u003c/html\u003e network boundary for network policies Current situation:\nEgress from source to the VM can be blocked only on a namespace level. On the VM side, network policies aren’t capable selecting workload entries. These only select pods using .spec.podSelector. The natural network boundary is the namespace and explicit deny of egress to the namespace with the VM.\ncleaning up 1 2 3 4 ./install.k3s.sh --delete ./install.vm.create.sh --delete cd ~ rm -rf ~/.project summary Success, a pod in the mesh can communicate to the VM via the service, VM is in the mesh and can communicate back to the mesh. Istio VM workloads are easy way to automate VM-mesh onboarding.\nVirtual Machine Installation ↩︎ ↩︎\nInstalling Multipass on Linux ↩︎\nyq installation instructions ↩︎\nIstio hello world examples for linux/arm64 ↩︎\n","description":"setting up a proof-of-concept connectivity with a VM in an Istio mesh","tags":["istio","kubernetes","k8s","multipass","tls"],"title":"Istio VM workloads","uri":"/posts/2023-11-02-istio-external-workloads/"},{"content":"","description":"","tags":null,"title":"k8s","uri":"/tags/k8s/"},{"content":"","description":"","tags":null,"title":"kubernetes","uri":"/tags/kubernetes/"},{"content":"","description":"","tags":null,"title":"multipass","uri":"/tags/multipass/"},{"content":"","description":"","tags":null,"title":"tls","uri":"/tags/tls/"},{"content":"","description":"","tags":null,"title":"docker","uri":"/tags/docker/"},{"content":"","description":"","tags":null,"title":"postgres","uri":"/tags/postgres/"},{"content":"","description":"","tags":null,"title":"yugabytedb","uri":"/tags/yugabytedb/"},{"content":"A couple of weeks ago I had a pleasure talking to Denis Magda and Franck Pachot from Yugabyte about my past Postgres foreign data wrapper contributions to the awesome YugabyteDB RDBMS. We discussed the whys and the hows, and reflected on my experiences as a contributor. Thanks for having me!\nTowards the end we touched on the build infrastructure I created to support my work on said contributions. I was chuffed to learn that Franck used it internally!\nThe guys got a commitment out of me. I’ll publish a write up on how I build YugabyteDB from sources, and share some light on working with the code in the process of explaining the former.\nSo… after this long and boring introduction. As a prep for the write up I upgraded the build infrastructure to support the latest and greatest YugabyteDB v2.19.0.0.\nThe code is on GitHub1.\nLots of changes in there, mainly:\nAlmaLinux 8.8 for the build infrastructure, because CentOS Red Hat feud … Single Docker image with Clang and GCC. Build infrastructure supports multiple build targets. Comes with preconfigured Ccache. Build infrastructure targets linux/amd64, resulting YugabyteDB also linux/amd64. Tested on Intel and ARM mac, and amd64 Linux, check the compatibility matrix2. Faced some issues3. Watch the recording for more context4.\nYugabyteDB build infrastructure on GitHub ↩︎\nBuild infrastructure: compatibility matrix ↩︎\nBuild infrastructure: Known issues and caveats ↩︎\nYouTube: Contributing to YugabyteDB: Enhancing Multi-Tenancy with Foreign Data Wrappers ↩︎\n","description":"build the latest and greatest 2.19.0.0 in Docker","tags":["yugabytedb","postgres","docker"],"title":"YugabyteDB build infrastructure upgrade","uri":"/posts/2023-07-21-yugabytedb-build-infrastructure-upgrade/"},{"content":"","description":"","tags":null,"title":"productivity","uri":"/tags/productivity/"},{"content":"","description":"","tags":null,"title":"software","uri":"/tags/software/"},{"content":"The jq tool was a game-changer for JSON on the command line. Before jq manipulating JSON data meant invoking programs in third-party languages. jq changed that for the better. But the world has moved on, more and more people have adopted YAML. There was a need for a YAML processing jq-like tool—the yq.\nThere were multiple attempts to solve this problem, often sharing the same name.\nThe yq from Mike Farah is the best one.\nThe MIT-licensed yq source code is available on GitHub1.\ntable of contents what is yq how do I install yq how do I use it yq teasers extracting all helm chart dependencies with versions extracting only non-local helm chart dependencies with versions extracting default images from helm chart values count number of CRDs find latest non-literal-latest ory hydra image tag in docker hub find latest numerical keycloak image tag available in quay.io find unique statefulset images used by the yugabytedb helm chart apply selected resources only closing words what is yq After GitHub repository:\nyq: a lightweight and portable command-line YAML, JSON and XML processor. yq uses jq like syntax but works with yaml files as well as json, xml, properties, csv and tsv. It doesn’t yet support everything jq does - but it does support the most common operations and functions, and more is being added continuously.\nhow do I install yq yq is written in go. If you have go installed, the easiest method is:\n1 go install github.com/mikefarah/yq/v4@v4.34.1 If you don’t have go installed, no worries. There are plenty of other ways to install the tool, simply consult the GitHub repository2.\nhow do I use it The documentation is thorough3. The best way to start is to check examples and simply play with it.\nThere are many easy to find yq tutorials so I’ll spare that here.\nHowever, I’ll share some pretty cool teasers.\nyq teasers extracting all helm chart dependencies with versions 1 2 3 curl --silent -L \\ https://github.com/kubeshop/helm-charts/raw/develop/charts/testkube/Chart.yaml \\ | yq '.dependencies[] | .repository + \"@\" + .version' file://../testkube-operator@1.13.0 https://charts.bitnami.com/bitnami@12.1.31 https://nats-io.github.io/k8s/helm/charts/@0.19.1 file://../testkube-api@1.13.0 file://../testkube-dashboard@1.13.0 file://../global@0.1.1 extracting only non-local helm chart dependencies with versions 1 2 3 curl --silent -L \\ https://github.com/kubeshop/helm-charts/raw/develop/charts/testkube/Chart.yaml \\ | yq '.dependencies | filter(.repository | match(\"^https?\")) | .[] | .repository + \"@\" + .version' https://charts.bitnami.com/bitnami@12.1.31 https://nats-io.github.io/k8s/helm/charts/@0.19.1 extracting default images from helm chart values 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 cat \u003e /tmp/runme.sh \u003c\u003c 'EOF' #!/bin/bash set -eu DEP_NAME=mongodb DEP_VERSION=$(curl --silent -L \\ https://github.com/kubeshop/helm-charts/raw/develop/charts/testkube/Chart.yaml \\ | yq '.dependencies[] | select(.name == \"'${DEP_NAME}'\") | .version' -r) DEP_REPO=$(curl --silent -L \\ https://github.com/kubeshop/helm-charts/raw/develop/charts/testkube/Chart.yaml \\ | yq '.dependencies[] | select(.name == \"'${DEP_NAME}'\") | .repository' -r) HELM_REPO_NAME=${DEP_NAME}-${DEP_VERSION} helm repo add ${HELM_REPO_NAME} ${DEP_REPO} helm repo update ${HELM_REPO_NAME} mkdir -p /tmp/${HELM_REPO_NAME} cd /tmp/${HELM_REPO_NAME} helm pull ${DEP_NAME}-${DEP_VERSION}/${DEP_NAME} --version ${DEP_VERSION} tar xvf ${DEP_NAME}-${DEP_VERSION}.tgz MONGODB_IMAGE=$(cat /tmp/${DEP_NAME}-${DEP_VERSION}/${DEP_NAME}/values.yaml \\ | yq '.image.registry + \"/\" + .image.repository + \":\" + .image.tag') cd - rm -rf /tmp/${DEP_NAME}-${DEP_VERSION} helm repo remove ${DEP_NAME}-${DEP_VERSION} echo MongoDB image is: ${MONGODB_IMAGE} EOF chmod +x /tmp/runme.sh /tmp/runme.sh rm -f /tmp/runme.sh \"mongodb-12.1.31\" has been added to your repositories Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \"mongodb-12.1.31\" chart repository Update Complete. ⎈Happy Helming!⎈ x mongodb/Chart.yaml ... x mongodb/charts/common/README.md /Users/radek/dev \"mongodb-12.1.31\" has been removed from your repositories MongoDB image is: docker.io/bitnami/mongodb:5.0.10-debian-11-r3 count number of CRDs 1 2 3 4 curl --silent -L \\ https://github.com/istio/istio/raw/1.18.0/manifests/charts/base/crds/crd-all.gen.yaml \\ | yq '. | select(.kind == \"CustomResourceDefinition\") | .metadata.name' -r -o json \\ | wc -l 14 find latest non-literal-latest ory hydra image tag in docker hub 1 2 3 curl --silent \\ https://registry.hub.docker.com/v2/repositories/oryd/hydra/tags?page_size=50 \\ | yq '.results | filter(.name | match(\"^v\")) | .[0].name' -r v2.2.0 find latest numerical keycloak image tag available in quay.io 1 2 3 curl --silent -L \\ https://quay.io/api/v1/repository/keycloak/keycloak/tag?limit=50 \\ | yq '.tags | filter(.name | match(\"^\\d\")) | .[0].name' 21.1 find unique statefulset images used by the yugabytedb helm chart 1 2 3 4 5 6 7 8 helm repo add yugabytedb https://charts.yugabyte.com helm repo update yugabytedb helm template yb-demo yugabytedb/yugabyte \\ --version 2.19.0 \\ --set resource.master.requests.cpu=0.5,resource.master.requests.memory=0.5Gi,resource.tserver.requests.cpu=0.5,resource.tserver.requests.memory=0.5Gi \\ --namespace yb-demo \\ | yq '. | select(.kind == \"StatefulSet\") | .spec.template.spec.containers[] | .image' -r -o json \\ | uniq yugabytedb/yugabyte:2.19.0.0-b190 1 helm repo remove yugabytedb apply selected resources only 1 2 3 cat ...yaml \\ | yq '. | select(.kind == \"Deployment\" or .kind == \"StatefulSet\") | . | select(.metadata.name == \"yb-tserver\")' \\ | kubectl apply -f - closing words yq brings new superpowers to the terminal. Processing YAML, JSON, XML, TSV, and CSV with one tool is absolutely outstanding.\nA real must-have tool in modern shell data processing stack.\nyq source code on GitHub ↩︎\nyq installation instructions ↩︎\nyq documentation ↩︎\n","description":"because life is too short to struggle with YAML on the command line","tags":["productivity","software","k8s","kubernetes"],"title":"yq - the yaml power tool","uri":"/posts/2023-07-10-yq-the-yaml-power-tool/"},{"content":"","description":"","tags":null,"title":"terminal","uri":"/tags/terminal/"},{"content":"","description":"","tags":null,"title":"windows","uri":"/tags/windows/"},{"content":"I am a heavy macOS user but my current work requires the use of Windows and its Terminal for WSL2. Working with Windows Terminal was very frustrating until I changed a couple of settings:\nCopy-on-select. Set the word delimiters setting to match the one of iTerm2. table of contents copy-on-select word delimiters bonus: trim whitespace on paste copy-on-select Selecting text automatically copies it to the clipboard. No need to hit CTRL+C to copy. This is a massive time saver. To enable copy-on-select:\nOpen Windows Terminal Settings. Go to the Interaction tab. Enable Automatically copy selection to clipboard. word delimiters There are two main text selection methods in Windows Terminal:\nClick-and-drag to select desired text portion. Double-click on text to select. The double-click with copy-on-select speeds up working in terminal.\nHowever, double-click must select the right thing. The default Windows Terminal word delimiters setting isn’t very well suited to working with Linux-based systems.\nAs a—primarily—macOS user, I work with iTerm2 which comes with the best word delimiter setting out there, in my humble opinion. It correctly selects strings representing complex variable names and paths. Consider the following example:\nhello-world_example=/path/to/a/file.md With the default Windows Terminal setting, double-clicking on:\nhello selects hello only. world selects world_example only. any part of the path selects only that part of the path, for example clicking on file selects file only. To make it better, change the Word delimiters setting in Settings / Interaction to:\n()\"':,;\u003c\u003e~!@#$%^\u0026*|=[]{}?│ There’s a space as the first character. In settings.json, this would be:\n1 \"wordDelimiters\": \" ()\\\"':,;\u003c\u003e~!@#$%^\u0026*|=[]{}?│\", If you’re having trouble updating the setting to the proposed value due to the \", use the Open JSON file button in the bottom left corner to modify the settings.json file directly.\nAfter that change, double-clicking on:\nhello selects hello-world_example. any part of the path selects the complete /path/to/a/file.md. This is a productivity boost when working with:\nTools like find, grep, tree, and so on. Various outputs. For example textual, JSON and YAML kubectl outputs. Editing bash programs or any other files directly in the terminal. Don’t dismiss this advice based on the fact that it originates from macOS. At least give it a try, if you don’t like it, you can always change it back.\nbonus: trim whitespace on paste Two more settings I always make sure are enabled:\nRemove trailing white-space in rectangular selection. Remove trailing white-space when pasting. Both available under the same Interaction section.\n","description":"boost your productivity when working with WSL2 in Windows Terminal","tags":["productivity","windows","terminal"],"title":"Windows Terminal text selection productivity tips","uri":"/posts/2023-07-10-windows-terminal-text-selection-productivity-tips/"},{"content":"","description":"","tags":null,"title":"cert-manager","uri":"/tags/cert-manager/"},{"content":"I’m running Istio 1.16.1 with cert-manager 1.11.0 ClusterIssuer pointed at Let’s Encrypt using HTTP-01 challenge. I have a gateway with a virtual service and I would like to automatically redirect all HTTP traffic to HTTPS. Except of the Let’s Encrypt challenge. Because, after Let’s Encrypt documentation1:\nThe HTTP-01 challenge can only be done on port 80. Allowing clients to specify arbitrary ports would make the challenge less secure, and so it is not allowed by the ACME standard.\nIstio documentation suggests using the ServerTLSSettings2 httpRedirect: true but this has a very nasty side effect. All traffic, even challenge solving is redirected to HTTPS. No certificate will be issued or renewed until httpsRedirect is set to false. This can be observed in cert-manager logs (broken into multiple lines for readability):\nE0708 15:04:16.187682 1 sync.go:190] cert-manager/challenges \"msg\"=\"propagation check failed\" \"error\"=\"failed to perform self check GET request 'http://test.svcs.sh/.well-known/acme-challenge/KVPhulYn7MtBxU9rPnFumBfh8mutovrVi50UGzaEOCA': Get \\\"https://test.svcs.sh/.well-known/acme-challenge/KVPhulYn7MtBxU9rPnFumBfh8mutovrVi50UGzaEOCA\\\": EOF\" \"dnsName\"=\"test.svcs.sh\" \"resource_kind\"=\"Challenge\" \"resource_name\"=\"test-certificate-mfjkm-1639063799-1954195061\" \"resource_namespace\"=\"istio-system\" \"resource_version\"=\"v1\" \"type\"=\"HTTP-01\" The challenge needs to be done over HTTP but due to the httpRedirect=true, it gets redirected to HTTPS, which cannot work. The EOF is intriguing. Someone closes the connection!\nThe fix is relatively simple and if you are interesting in that part only, jump right fixing the problem. The rest of this post is an investigation for a supporting evidence why it is working and why the httpsRedirect: true doesn’t work.\ntable of contents istio configuration fixing the problem what’s the difference? how istio interacts with envoy proxy gateway httpsRedirect in simple terms scheme redirect why the eof cert-manager ingress and solver clean investigation: investigation.svcs.sh the ingress the service pods the ingress is picked up by envoy cert-manager.io/issue-temporary-certificate enough evidence for final conclusions why httpsRedirect: true cannot work why httpsRedirect with an explicit route redirect works verify the final conclusion enable trace logging on istio ingressgateway observe the log disable the httpsRedirect final conclusion istio configuration Here’s my baseline configuration.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 --- apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: letsencrypt-prod-cluster namespace: istio-system spec: acme: email: email@address server: https://acme-v02.api.letsencrypt.org/directory privateKeySecretRef: name: letsencrypt-prod-cluster solvers: - http01: ingress: class: istio --- apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: test-certificate namespace: istio-system spec: secretName: test-certificate-secret duration: 2160h # 90d renewBefore: 360h # 15d isCA: false privateKey: algorithm: RSA encoding: PKCS1 size: 2048 usages: - server auth - client auth dnsNames: - \"test.svcs.sh\" issuerRef: name: letsencrypt-prod-cluster kind: ClusterIssuer group: cert-manager.io --- apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: test-svcs-sh-gtw namespace: test spec: selector: istio: ingressgateway servers: - port: number: 80 name: http protocol: HTTP hosts: - \"test.svcs.sh\" tls: httpsRedirect: true - port: number: 443 name: https protocol: HTTPS tls: mode: SIMPLE credentialName: test-certificate-secret hosts: - \"test.svcs.sh\" --- apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: test-svcs-sh-vs namespace: test spec: hosts: - \"test.svcs.sh\" gateways: - test-svcs-sh-gtw http: - match: - uri: prefix: / route: - destination: port: number: 3000 host: some-backend-service fixing the problem The solution can be found in one of the comments to this GitHub issue3. It’s really simple:\nKeep the httpRedirect set to false in the Gateway. Use a match.uri.prefix with scheme.exact: http and redirect.scheme: https to handle the redirect selectively in a VirtualService. We arrive at the following definition:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 --- apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: test-svcs-sh-gtw namespace: test spec: selector: istio: ingressgateway servers: - port: number: 80 name: http protocol: HTTP hosts: - \"test.svcs.sh\" tls: httpsRedirect: false # Handle redirect in the VirtualService - port: number: 443 name: https protocol: HTTPS tls: mode: SIMPLE credentialName: test-certificate-secret hosts: - \"test.svcs.sh\" --- apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: test-svcs-sh-vs namespace: test spec: hosts: - \"test.svcs.sh\" gateways: - test-svcs-sh-gtw http: - match: - uri: prefix: / scheme: exact: http redirect: scheme: https redirectCode: 302 - match: - uri: prefix: / route: - destination: port: number: 3000 host: some-backend-service Problem fixed. The certificate is issued and the redirect works as expected:\n1 curl -v http://test.svcs.sh/ * Trying 167.235.105.89:80... * Connected to test.svcs.sh (167.235.105.89) port 80 (#0) \u003e GET / HTTP/1.1 \u003e Host: test.svcs.sh \u003e User-Agent: curl/7.86.0 \u003e Accept: */* \u003e * Mark bundle as not supporting multiuse \u003c HTTP/1.1 302 Found \u003c location: https://test.svcs.sh/ \u003c date: Sat, 08 Jul 2023 15:06:07 GMT \u003c server: istio-envoy \u003c content-length: 0 \u003c * Connection #0 to host test.svcs.sh left intact what’s the difference? The obvious question is: what’s different? It works but why? Let’s do some digging. I’m using Istio Gateway so the component to start investigating from is the istio-ingressgateway which is installed into the namespace where Istio installs to.\n1 kubectl get pods -l app=istio-ingressgateway -n istio-system NAME READY STATUS RESTARTS AGE istio-ingressgateway-77cf7f984b-smhhn 1/1 Running 0 2d2h Istio does many interesting things. Most of those features are supported by the Envoy proxy. Istio uses and wraps Envoy extensively. As soon as the Gateway is deployed, an Envoy listener is created, it remains the same during the whole investigation and there’s nothing interesting in it so let’s not focus too much on it.\n1 2 3 4 istioctl proxy-config listeners \\ istio-ingressgateway-77cf7f984b-smhhn.istio-system \\ | grep test 0.0.0.0 8443 SNI: test.svcs.sh Route: https.443.https.test-svcs-sh-gtw.test The HTTPS listener is built from this part of the Gateway declaration:\n1 2 3 4 5 6 7 8 9 - port: number: 443 name: https protocol: HTTPS tls: mode: SIMPLE credentialName: test-certificate-secret hosts: - \"test.svcs.sh\" how istio interacts with envoy proxy Envoy configuration starts with a listener. A listener contains a list of filters. One of the filters deals with HTTP protocol traffic. A filter uses a router do distribute the traffic inside of the mesh. A router contains a list of virtual hosts. A virtual host contains individual routes. From Istio perspective: there’s always going to be one Envoy router dealing with HTTP traffic for each Istio ingress gateway component. Istio Gateway maps to Envoy’s virtual host. Individual routes are constructed from Istio VirtualService.\nThe relevant Istio code dealing with Envoy4.\ngateway httpsRedirect in simple terms I can use the istioctl proxy-config routes command to look at Envoy’s virtual hosts and routes.\nI’m investigating HTTP traffic redirection to HTTPS so the point at which HTTP (port 80) traffic is accepted seems like a good place to start. The name of the virtual host is http.\u003cEnvoy’s listener bind port\u003e, which is usually 8080. I’m looking for test.svcs.sh traffic handling Let’s look at the routing configuration for test.svcs.sh:80.\nHere’s a good starting point to get the hand of some of those commands5 used later in this post.\n1 2 3 4 istioctl proxy-config \\ routes istio-ingressgateway-77cf7f984b-smhhn.istio-system \\ -o yaml \\ | yq '. | filter(.name == \"http.8080\") | .[0].virtualHosts | filter(.name == \"test.svcs.sh:80\")' 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 - domains: - test.svcs.sh includeRequestAttemptCount: true name: test.svcs.sh:80 requireTls: ALL routes: - decorator: operation: test-svcs-sh-vs:80/* match: caseSensitive: true headers: - name: :scheme stringMatch: exact: http prefix: / metadata: filterMetadata: istio: config: /apis/networking.istio.io/v1alpha3/namespaces/test/virtual-service/test-svcs-sh-vs redirect: pathRedirect: \"\" responseCode: FOUND schemeRedirect: https - decorator: operation: some-backend-service.test.svc.cluster.local:3000/* match: caseSensitive: true prefix: / metadata: filterMetadata: istio: config: /apis/networking.istio.io/v1alpha3/namespaces/test/virtual-service/test-svcs-sh-vs route: cluster: outbound|3000||some-backend-service.test.svc.cluster.local maxGrpcTimeout: 0s retryPolicy: hostSelectionRetryMaxAttempts: \"5\" numRetries: 2 retriableStatusCodes: - 503 retryHostPredicate: - name: envoy.retry_host_predicates.previous_hosts typedConfig: '@type': type.googleapis.com/envoy.extensions.retry.host.previous_hosts.v3.PreviousHostsPredicate retryOn: connect-failure,refused-stream,unavailable,cancelled,retriable-status-codes timeout: 0s We can see the requireTls: ALL property.\nOkay, let’s set the Gateway httpsRedirect to false and apply the change. Repeat the last istioctl command. This time the output is:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 - domains: - test.svcs.sh includeRequestAttemptCount: true name: test.svcs.sh:80 routes: - decorator: operation: test-svcs-sh-vs:80/* match: caseSensitive: true headers: - name: :scheme stringMatch: exact: http prefix: / metadata: filterMetadata: istio: config: /apis/networking.istio.io/v1alpha3/namespaces/test/virtual-service/test-svcs-sh-vs redirect: pathRedirect: \"\" responseCode: FOUND schemeRedirect: https - decorator: operation: some-backend-service.test.svc.cluster.local:3000/* match: caseSensitive: true prefix: / metadata: filterMetadata: istio: config: /apis/networking.istio.io/v1alpha3/namespaces/test/virtual-service/test-svcs-sh-vs route: cluster: outbound|3000||some-backend-service.test.svc.cluster.local maxGrpcTimeout: 0s retryPolicy: hostSelectionRetryMaxAttempts: \"5\" numRetries: 2 retriableStatusCodes: - 503 retryHostPredicate: - name: envoy.retry_host_predicates.previous_hosts typedConfig: '@type': type.googleapis.com/envoy.extensions.retry.host.previous_hosts.v3.PreviousHostsPredicate retryOn: connect-failure,refused-stream,unavailable,cancelled,retriable-status-codes timeout: 0s Exactly the same except that the requireTls: ALL is gone. Envoy’s requireTls: ALL setting6 has exactly the same meaning as httpsRedirect: true in Istio Gateway,:\n⁣All requests must use TLS. If a request is not using TLS, a 301 redirect will be sent telling the client to use HTTPS.\nThis can be observed, with httpsRedirect: true:\n1 curl -v http://test.svcs.sh/ * Trying 167.235.105.89:80... * Connected to test.svcs.sh (167.235.105.89) port 80 (#0) \u003e GET / HTTP/1.1 \u003e Host: test.svcs.sh \u003e User-Agent: curl/7.86.0 \u003e Accept: */* \u003e * Mark bundle as not supporting multiuse \u003c HTTP/1.1 301 Moved Permanently \u003c location: https://test.svcs.sh \u003c date: Sat, 08 Jul 2023 15:46:40 GMT \u003c server: istio-envoy \u003c content-length: 0 \u003c * Connection #0 to host test.svcs.sh left intact scheme redirect The fix to the catch-all redirect and working ACME challenge uses the scheme redirect. The scheme redirect has been added as part of this feature request7.\nwhy the eof Indeed, this is a good question. At the end of the day, it shouldn’t matter if httpRedirect: true, or an individual route redirect is in use. Both should work 🤞.\nFrom the perspective of the client the redirect is 301, 302, or any other 3xx supported by Istio.\nSomething else must be going on.\ncert-manager ingress and solver What happens when cert-manager enters the challenge solving state?\nAn Ingress resource is created, as instructed by the ClusterIssuer .spec.acme.solvers. This Ingress points at a solver Service resource. The Service points at the solver pod. Relevant cert-manager source code8.\nclean investigation: investigation.svcs.sh Let’s pick a new domain name and start from the non-working condition by requesting a certificate for investigation.svcs.sh while httpsRedirect is true. This immediately triggers the behavior:\nE0708 16:32:22.491714 1 sync.go:190] cert-manager/challenges \"msg\"=\"propagation check failed\" \"error\"=\"failed to perform self check GET request 'http://investigation.svcs.sh/.well-known/acme-challenge/bgLcgdeOvQ4nbDaD4wENG-qR76dxR8GkZKe2Wp1msX0': Get \\\"https://investigation.svcs.sh/.well-known/acme-challenge/bgLcgdeOvQ4nbDaD4wENG-qR76dxR8GkZKe2Wp1msX0\\\": EOF\" \"dnsName\"=\"investigation.svcs.sh\" \"resource_kind\"=\"Challenge\" \"resource_name\"=\"test-certificate-p7zmc-788698794-972312837\" \"resource_namespace\"=\"istio-system\" \"resource_version\"=\"v1\" \"type\"=\"HTTP-01\" the ingress cert-manager creates a new Ingress in the namespace where Istio is installed:\n1 kubectl get ingress -n istio-system NAME CLASS HOSTS ADDRESS PORTS AGE cm-acme-http-solver-7djvq \u003cnone\u003e investigation.svcs.sh 80 22m 1 kubectl get ingress cm-acme-http-solver-7djvq -n istio-system -o yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 apiVersion: networking.k8s.io/v1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: istio nginx.ingress.kubernetes.io/whitelist-source-range: 0.0.0.0/0,::/0 creationTimestamp: \"2023-07-08T07:17:27Z\" generateName: cm-acme-http-solver- generation: 1 labels: acme.cert-manager.io/http-domain: \"586089708\" acme.cert-manager.io/http-token: \"1089211970\" acme.cert-manager.io/http01-solver: \"true\" name: cm-acme-http-solver-7djvq namespace: istio-system ownerReferences: - apiVersion: acme.cert-manager.io/v1 blockOwnerDeletion: true controller: true kind: Challenge name: test-certificate-p7zmc-788698794-972312837 uid: 94e8a8ca-3ce1-44a5-b08e-b0dccef48e70 resourceVersion: \"57527723\" uid: a0a4d83b-aa5b-4fad-93c1-29a79fc00a77 spec: rules: - host: investigation.svcs.sh http: paths: - backend: service: name: cm-acme-http-solver-dw2ws port: number: 8089 path: /.well-known/acme-challenge/bgLcgdeOvQ4nbDaD4wENG-qR76dxR8GkZKe2Wp1msX0 pathType: ImplementationSpecific status: loadBalancer: {} the service The Ingress use a Service for its backend. The Service:\n1 kubectl get service cm-acme-http-solver-dw2ws -n istio-system -o yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 apiVersion: v1 kind: Service metadata: annotations: auth.istio.io/8089: NONE creationTimestamp: \"2023-07-08T07:17:27Z\" generateName: cm-acme-http-solver- labels: acme.cert-manager.io/http-domain: \"586089708\" acme.cert-manager.io/http-token: \"1089211970\" acme.cert-manager.io/http01-solver: \"true\" name: cm-acme-http-solver-dw2ws namespace: istio-system ownerReferences: - apiVersion: acme.cert-manager.io/v1 blockOwnerDeletion: true controller: true kind: Challenge name: test-certificate-p7zmc-788698794-972312837 uid: 94e8a8ca-3ce1-44a5-b08e-b0dccef48e70 resourceVersion: \"57527721\" uid: 19863c57-d8c1-4a5d-b608-4689825f39cf spec: clusterIP: 10.43.44.72 clusterIPs: - 10.43.44.72 externalTrafficPolicy: Cluster internalTrafficPolicy: Cluster ipFamilies: - IPv4 ipFamilyPolicy: SingleStack ports: - name: http nodePort: 31493 port: 8089 protocol: TCP targetPort: 8089 selector: acme.cert-manager.io/http-domain: \"586089708\" acme.cert-manager.io/http-token: \"1089211970\" acme.cert-manager.io/http01-solver: \"true\" sessionAffinity: None type: NodePort status: loadBalancer: {} pods Since there’s a service, there are also pods, let’s find them with one of the Service selectors:\n1 kubectl get pods -l \"acme.cert-manager.io/http01-solver=true\" -n istio-system NAME READY STATUS RESTARTS AGE cm-acme-http-solver-djwm7 1/1 Running 0 31m the ingress is picked up by envoy Because of the ClusterIssuer .spec.acme.solvers having an istio solver in the list, the solver route is picked up by Envoy:\n1 2 3 istioctl proxy-config \\ routes istio-ingressgateway-77cf7f984b-smhhn.istio-system -o yaml \\ | yq '. | filter(.name == \"http.8080\") | .[0].virtualHosts | filter(.name == \"investigation.svcs.sh:80\")' Produces:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 - domains: - investigation.svcs.sh includeRequestAttemptCount: true name: investigation.svcs.sh:80 requireTls: ALL routes: - decorator: operation: cm-acme-http-solver-dw2ws.istio-system.svc.cluster.local:8089/.well-known/acme-challenge/bgLcgdeOvQ4nbDaD4wENG-qR76dxR8GkZKe2Wp1msX0 match: caseSensitive: true path: /.well-known/acme-challenge/bgLcgdeOvQ4nbDaD4wENG-qR76dxR8GkZKe2Wp1msX0 metadata: filterMetadata: istio: config: /apis/networking.istio.io/v1alpha3/namespaces/istio-system/virtual-service/investigation-svcs-sh-cm-acme-http-solver-7djvq-istio-autogenerated-k8s-ingress route: cluster: outbound|8089||cm-acme-http-solver-dw2ws.istio-system.svc.cluster.local maxGrpcTimeout: 0s retryPolicy: hostSelectionRetryMaxAttempts: \"5\" numRetries: 2 retriableStatusCodes: - 503 retryHostPredicate: - name: envoy.retry_host_predicates.previous_hosts typedConfig: '@type': type.googleapis.com/envoy.extensions.retry.host.previous_hosts.v3.PreviousHostsPredicate retryOn: connect-failure,refused-stream,unavailable,cancelled,retriable-status-codes timeout: 0s - decorator: operation: test-svcs-sh-vs:80/* match: caseSensitive: true headers: - name: :scheme stringMatch: exact: http prefix: / metadata: filterMetadata: istio: config: /apis/networking.istio.io/v1alpha3/namespaces/test/virtual-service/test-svcs-sh-vs redirect: pathRedirect: \"\" responseCode: FOUND schemeRedirect: https - decorator: operation: some-backend-service.test.svc.cluster.local:3000/* match: caseSensitive: true prefix: / metadata: filterMetadata: istio: config: /apis/networking.istio.io/v1alpha3/namespaces/test/virtual-service/test-svcs-sh-vs route: cluster: outbound|3000||some-backend-service.test.svc.cluster.local maxGrpcTimeout: 0s retryPolicy: hostSelectionRetryMaxAttempts: \"5\" numRetries: 2 retriableStatusCodes: - 503 retryHostPredicate: - name: envoy.retry_host_predicates.previous_hosts typedConfig: '@type': type.googleapis.com/envoy.extensions.retry.host.previous_hosts.v3.PreviousHostsPredicate retryOn: connect-failure,refused-stream,unavailable,cancelled,retriable-status-codes timeout: 0s Exactly the same as in case of test.svcs.sh before but with a new route:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 - decorator: operation: cm-acme-http-solver-dw2ws.istio-system.svc.cluster.local:8089/.well-known/acme-challenge/bgLcgdeOvQ4nbDaD4wENG-qR76dxR8GkZKe2Wp1msX0 match: caseSensitive: true path: /.well-known/acme-challenge/bgLcgdeOvQ4nbDaD4wENG-qR76dxR8GkZKe2Wp1msX0 metadata: filterMetadata: istio: config: /apis/networking.istio.io/v1alpha3/namespaces/istio-system/virtual-service/investigation-svcs-sh-cm-acme-http-solver-7djvq-istio-autogenerated-k8s-ingress route: cluster: outbound|8089||cm-acme-http-solver-dw2ws.istio-system.svc.cluster.local maxGrpcTimeout: 0s retryPolicy: hostSelectionRetryMaxAttempts: \"5\" numRetries: 2 retriableStatusCodes: - 503 retryHostPredicate: - name: envoy.retry_host_predicates.previous_hosts typedConfig: '@type': type.googleapis.com/envoy.extensions.retry.host.previous_hosts.v3.PreviousHostsPredicate retryOn: connect-failure,refused-stream,unavailable,cancelled,retriable-status-codes timeout: 0s Explanation:\nIstio Gateway and a VirtualService are created for the domain name investigation.svcs.sh. Istio creates virtual host routes in Envoy for the domain name. The certificate resource for the domain name is requested, the cert-manager ClusterIssuer starts the dance. cert-manager creates an Ingress for the investigation.svcs.sh domain name. This gets picked up by Envoy and the route is rolled into an already existing Envoy virtual host. We see that the route is configured to forward the traffic to the solver service:\n1 2 route: cluster: outbound|8089||cm-acme-http-solver-dw2ws.istio-system.svc.cluster.local But the pod doesn’t receive anything. The log just sits there waiting for something to come through:\n1 kubectl logs --follow cm-acme-http-solver-djwm7 -n istio-system I0708 17:17:28.613173 1 solver.go:39] cert-manager/acmesolver \"msg\"=\"starting listener\" \"expected_domain\"=\"investigation.svcs.sh\" \"expected_key\"=\"bgLcgdeOvQ4nbDaD4wENG-qR76dxR8GkZKe2Wp1msX0.fXDK-85QmDd5AVhxsp1Aj3jeez2KozCIcoubbYfervY\" \"expected_token\"=\"bgLcgdeOvQ4nbDaD4wENG-qR76dxR8GkZKe2Wp1msX0\" \"listen_port\"=8089 Nothing ever comes through.\ncert-manager.io/issue-temporary-certificate One of the comments from the GitHub issue suggests using the cert-manager.io/issue-temporary-certificate: true annotation. Let’s see how that goes. I change the Certificate manifest to:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 --- apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: test-certificate namespace: istio-system annotations: cert-manager.io/issue-temporary-certificate: \"true\" spec: secretName: test-certificate-secret duration: 2160h # 90d renewBefore: 360h # 15d isCA: false privateKey: algorithm: RSA encoding: PKCS1 size: 2048 usages: - server auth - client auth dnsNames: - \"investigation.svcs.sh\" issuerRef: name: letsencrypt-prod-cluster kind: ClusterIssuer group: cert-manager.io cert-manager now logs:\nE0708 17:22:18.802362 1 sync.go:190] cert-manager/challenges \"msg\"=\"propagation check failed\" \"error\"=\"wrong status code '404', expected '200'\" \"dnsName\"=\"investigation.svcs.sh\" \"resource_kind\"=\"Challenge\" \"resource_name\"=\"test-certificate-wq6ft-788698794-972312837\" \"resource_namespace\"=\"istio-system\" \"resource_version\"=\"v1\" \"type\"=\"HTTP-01\" This can be replicated with curl:\n1 2 curl --insecure -I -L -v \\ http://investigation.svcs.sh/.well-known/acme-challenge/bgLcgdeOvQ4nbDaD4wENG-qR76dxR8GkZKe2Wp1msX0 * Trying 167.235.105.89:80... * Connected to investigation.svcs.sh (167.235.105.89) port 80 (#0) \u003e HEAD /.well-known/acme-challenge/bgLcgdeOvQ4nbDaD4wENG-qR76dxR8GkZKe2Wp1msX0 HTTP/1.1 \u003e Host: investigation.svcs.sh \u003e User-Agent: curl/7.86.0 \u003e Accept: */* \u003e * Mark bundle as not supporting multiuse \u003c HTTP/1.1 301 Moved Permanently HTTP/1.1 301 Moved Permanently \u003c location: https://investigation.svcs.sh/.well-known/acme-challenge/bgLcgdeOvQ4nbDaD4wENG-qR76dxR8GkZKe2Wp1msX0 location: https://investigation.svcs.sh/.well-known/acme-challenge/bgLcgdeOvQ4nbDaD4wENG-qR76dxR8GkZKe2Wp1msX0 \u003c date: Sat, 08 Jul 2023 17:25:31 GMT date: Sat, 08 Jul 2023 17:25:31 GMT \u003c server: istio-envoy server: istio-envoy \u003c transfer-encoding: chunked transfer-encoding: chunked \u003c * Connection #0 to host investigation.svcs.sh left intact * Clear auth, redirects to port from 80 to 443 * Issue another request to this URL: 'https://investigation.svcs.sh/.well-known/acme-challenge/bgLcgdeOvQ4nbDaD4wENG-qR76dxR8GkZKe2Wp1msX0' * Trying 167.235.105.89:443... * Connected to investigation.svcs.sh (167.235.105.89) port 443 (#1) * ALPN: offers h2 * ALPN: offers http/1.1 * (304) (OUT), TLS handshake, Client hello (1): * (304) (IN), TLS handshake, Server hello (2): * (304) (IN), TLS handshake, Unknown (8): * (304) (IN), TLS handshake, Certificate (11): * (304) (IN), TLS handshake, CERT verify (15): * (304) (IN), TLS handshake, Finished (20): * (304) (OUT), TLS handshake, Finished (20): * SSL connection using TLSv1.3 / AEAD-CHACHA20-POLY1305-SHA256 * ALPN: server accepted h2 * Server certificate: * subject: serialNumber=1234567890 * start date: Jul 8 17:21:47 2023 GMT * expire date: Oct 6 17:21:47 2023 GMT * issuer: CN=cert-manager.local * SSL certificate verify result: unable to get local issuer certificate (20), continuing anyway. * Using HTTP2, server supports multiplexing * Copying HTTP/2 data in stream buffer to connection buffer after upgrade: len=0 * h2h3 [:method: HEAD] * h2h3 [:path: /.well-known/acme-challenge/bgLcgdeOvQ4nbDaD4wENG-qR76dxR8GkZKe2Wp1msX0] * h2h3 [:scheme: https] * h2h3 [:authority: investigation.svcs.sh] * h2h3 [user-agent: curl/7.86.0] * h2h3 [accept: */*] * Using Stream ID: 1 (easy handle 0x15a811e00) \u003e HEAD /.well-known/acme-challenge/bgLcgdeOvQ4nbDaD4wENG-qR76dxR8GkZKe2Wp1msX0 HTTP/2 \u003e Host: investigation.svcs.sh \u003e user-agent: curl/7.86.0 \u003e accept: */* \u003e * Connection state changed (MAX_CONCURRENT_STREAMS == 2147483647)! \u003c HTTP/2 404 HTTP/2 404 \u003c x-dns-prefetch-control: on x-dns-prefetch-control: on \u003c x-frame-options: SAMEORIGIN x-frame-options: SAMEORIGIN \u003c etag: \"wz51d2hkyd1sp\" etag: \"wz51d2hkyd1sp\" \u003c content-type: text/html; charset=utf-8 content-type: text/html; charset=utf-8 \u003c content-length: 2329 content-length: 2329 \u003c vary: Accept-Encoding vary: Accept-Encoding \u003c date: Sat, 08 Jul 2023 17:25:31 GMT date: Sat, 08 Jul 2023 17:25:31 GMT \u003c x-envoy-upstream-service-time: 11 x-envoy-upstream-service-time: 11 \u003c server: istio-envoy server: istio-envoy \u003c * Connection #1 to host investigation.svcs.sh left intact Hmm, this doesn’t work either but… it’s no longer an EOF, it’s a HTTP 404. This is significant in the context of cert-manager code responsible for the handling of the challenge9. Especially this part:\n1 2 3 4 5 6 7 8 9 TLSClientConfig: \u0026tls.Config{ // If we're following a redirect, it's permissible for it to be HTTPS and // its certificate may be invalid (they are trying to get a certificate, after all!) // See: https://letsencrypt.org/docs/challenge-types/#http-01-challenge // \u003e When redirected to an HTTPS URL, it does not validate certificates (since // \u003e this challenge is intended to bootstrap valid certificates, it may encounter // \u003e self-signed or expired certificates along the way). InsecureSkipVerify: true, }, Aha, cert-manager will accept any certificate as long as there is one.\ncert-manager issued a temporary certificate and stored it in the secret expected by the Istio Gateway. The request on port 80 is redirected to HTTPS at the httpsRedirect level. The request arrives in the mesh and is matched against the VirtualService https match for the /* path prefix. cert-manager ignores invalid certificate and accepts the redirect. The redirect leads to the HTTPS endpoint but the response is HTTP 404. This error comes from some-backend-service. enough evidence for final conclusions Frankly, the virtual host output with cert-manager solver route gives it all away but let’s break it down.\nwhy httpsRedirect: true cannot work The original request from cert-manager self-check arrives on port 80. Envoy immediately redirects it to HTTPS with the HTTP 301 status code. cert-manager follows the redirect. There are two possibilities: The ClusterIssuer uses the cert-manager.io/issue-temporary-certificate: true annotation. Because a temporary certificate exists, the Istio Gateway accepts a HTTPS request and Envoy doesn’t trip over as the redirected request was HTTPS. The request enters the mesh via the Gateway port 443 and is dealt with inside of the route with the decorator.operation: some-backend-service.test.svc.cluster.local:3000/*. It is forwarded to the backend service but backend service doesn’t known anything about /.well-known/acme-challenge/… path, so returns HTTP 404. The ClusterIssuer doesn’t use a temporary certificate. The request enters the mesh via the Gateway port 443 and is dealt with inside of the route with the decorator.operation: some-backend-service.test.svc.cluster.local:3000/*. There’s no certificate under the secret configured for the Gateway, Istio closes the connection. cert-manager HTTP client receives an EOF error. why httpsRedirect with an explicit route redirect works This is really obvious in its non-obviousness:\nThe route for the cert-manager solver sits before other routes defined by the VirtualService. There is no redirect, the handling of the solver request happens before VirtualService. verify the final conclusion To verify the final conclusion, I can do the following:\nWhile cert-manager continues failing the self-check: Enable trace logging on the Istio ingressgateway pod. Disable httpsRedirect. Observe the log. enable trace logging on istio ingressgateway 1 istioctl proxy-config log istio-ingressgateway-77cf7f984b-smhhn.istio-system --level http:trace observe the log 1 kubectl logs --follow istio-ingressgateway-77cf7f984b-smhhn -n istio-system There’s a lot of output so I need to be fast. Among other messages, I can observe:\n2023-07-08T18:43:59.970913Z\ttrace\tenvoy http\t[C8] parsed 75 bytes 2023-07-08T18:44:00.372329Z\ttrace\tenvoy http\t[C144420] parsing 264 bytes 2023-07-08T18:44:00.372364Z\ttrace\tenvoy http\t[C144420] message begin 2023-07-08T18:44:00.372372Z\tdebug\tenvoy http\t[C144420] new stream 2023-07-08T18:44:00.372392Z\ttrace\tenvoy http\t[C144420] completed header: key=Host value=investigation.svcs.sh 2023-07-08T18:44:00.372402Z\ttrace\tenvoy http\t[C144420] completed header: key=User-Agent value=cert-manager-challenges/v1.11.0 (linux/amd64) cert-manager/2a0ef53b06e183356d922cd58af2510d8885bef5 2023-07-08T18:44:00.372408Z\ttrace\tenvoy http\t[C144420] completed header: key=Accept-Encoding value=gzip 2023-07-08T18:44:00.372415Z\ttrace\tenvoy http\t[C144420] onHeadersCompleteBase 2023-07-08T18:44:00.372456Z\ttrace\tenvoy http\t[C144420] completed header: key=Connection value=close 2023-07-08T18:44:00.372464Z\ttrace\tenvoy http\t[C144420] Server: onHeadersComplete size=4 2023-07-08T18:44:00.372475Z\ttrace\tenvoy http\t[C144420] message complete 2023-07-08T18:44:00.372488Z\tdebug\tenvoy http\t[C144420][S18069518382980252164] request headers complete (end_stream=true): ':authority', 'investigation.svcs.sh' ':path', '/.well-known/acme-challenge/bgLcgdeOvQ4nbDaD4wENG-qR76dxR8GkZKe2Wp1msX0' ':method', 'GET' 'user-agent', 'cert-manager-challenges/v1.11.0 (linux/amd64) cert-manager/2a0ef53b06e183356d922cd58af2510d8885bef5' 'accept-encoding', 'gzip' 'connection', 'close' 2023-07-08T18:44:00.372497Z\tdebug\tenvoy http\t[C144420][S18069518382980252164] request end stream 2023-07-08T18:44:00.372602Z\ttrace\tenvoy http\t[C144420][S18069518382980252164] decode headers called: filter=istio.metadata_exchange status=0 2023-07-08T18:44:00.372618Z\ttrace\tenvoy http\t[C144420][S18069518382980252164] decode headers called: filter=istio.alpn status=0 2023-07-08T18:44:00.372623Z\ttrace\tenvoy http\t[C144420][S18069518382980252164] decode headers called: filter=envoy.filters.http.fault status=0 2023-07-08T18:44:00.372626Z\ttrace\tenvoy http\t[C144420][S18069518382980252164] decode headers called: filter=envoy.filters.http.cors status=0 2023-07-08T18:44:00.372634Z\ttrace\tenvoy http\t[C144420][S18069518382980252164] decode headers called: filter=istio.stats status=0 2023-07-08T18:44:00.372642Z\tdebug\tenvoy http\t[C144420][S18069518382980252164] Sending local reply with details direct_response 2023-07-08T18:44:00.372672Z\ttrace\tenvoy http\t[C144420][S18069518382980252164] encode headers called: filter=istio.stats status=0 2023-07-08T18:44:00.372678Z\ttrace\tenvoy http\t[C144420][S18069518382980252164] encode headers called: filter=envoy.filters.http.cors status=0 2023-07-08T18:44:00.372681Z\ttrace\tenvoy http\t[C144420][S18069518382980252164] encode headers called: filter=envoy.filters.http.fault status=0 2023-07-08T18:44:00.372687Z\ttrace\tenvoy http\t[C144420][S18069518382980252164] encode headers called: filter=istio.metadata_exchange status=0 2023-07-08T18:44:00.372696Z\tdebug\tenvoy http\t[C144420][S18069518382980252164] closing connection due to connection close header 2023-07-08T18:44:00.372708Z\tdebug\tenvoy http\t[C144420][S18069518382980252164] encoding headers via codec (end_stream=true): ':status', '301' 'location', 'https://investigation.svcs.sh/.well-known/acme-challenge/bgLcgdeOvQ4nbDaD4wENG-qR76dxR8GkZKe2Wp1msX0' 'date', 'Sat, 08 Jul 2023 17:44:00 GMT' 'server', 'istio-envoy' 'connection', 'close' This is coming from the httpsRedirect: true setting.\ndisable the httpsRedirect Change the httpsRedirect to false and watch the log closely. cert-manager doesn’t waste time. The following happens almost immediately:\n2023-07-08T18:44:09.969969Z\ttrace\tenvoy http\t[C6] parsed 75 bytes 2023-07-08T18:44:10.771798Z\ttrace\tenvoy http\t[C144427] parsing 264 bytes 2023-07-08T18:44:10.771840Z\ttrace\tenvoy http\t[C144427] message begin 2023-07-08T18:44:10.771849Z\tdebug\tenvoy http\t[C144427] new stream 2023-07-08T18:44:10.771876Z\ttrace\tenvoy http\t[C144427] completed header: key=Host value=investigation.svcs.sh 2023-07-08T18:44:10.771888Z\ttrace\tenvoy http\t[C144427] completed header: key=User-Agent value=cert-manager-challenges/v1.11.0 (linux/amd64) cert-manager/2a0ef53b06e183356d922cd58af2510d8885bef5 2023-07-08T18:44:10.771896Z\ttrace\tenvoy http\t[C144427] completed header: key=Accept-Encoding value=gzip 2023-07-08T18:44:10.771905Z\ttrace\tenvoy http\t[C144427] onHeadersCompleteBase 2023-07-08T18:44:10.771908Z\ttrace\tenvoy http\t[C144427] completed header: key=Connection value=close 2023-07-08T18:44:10.771917Z\ttrace\tenvoy http\t[C144427] Server: onHeadersComplete size=4 2023-07-08T18:44:10.771930Z\ttrace\tenvoy http\t[C144427] message complete 2023-07-08T18:44:10.771945Z\tdebug\tenvoy http\t[C144427][S18087745377321120087] request headers complete (end_stream=true): ':authority', 'investigation.svcs.sh' ':path', '/.well-known/acme-challenge/bgLcgdeOvQ4nbDaD4wENG-qR76dxR8GkZKe2Wp1msX0' ':method', 'GET' 'user-agent', 'cert-manager-challenges/v1.11.0 (linux/amd64) cert-manager/2a0ef53b06e183356d922cd58af2510d8885bef5' 'accept-encoding', 'gzip' 'connection', 'close' 2023-07-08T18:44:10.771965Z\tdebug\tenvoy http\t[C144427][S18087745377321120087] request end stream 2023-07-08T18:44:10.772238Z\ttrace\tenvoy http\t[C144427][S18087745377321120087] decode headers called: filter=istio.metadata_exchange status=0 2023-07-08T18:44:10.772273Z\ttrace\tenvoy http\t[C144427][S18087745377321120087] decode headers called: filter=istio.alpn status=0 2023-07-08T18:44:10.772313Z\ttrace\tenvoy http\t[C144427][S18087745377321120087] decode headers called: filter=envoy.filters.http.fault status=0 2023-07-08T18:44:10.772336Z\ttrace\tenvoy http\t[C144427][S18087745377321120087] decode headers called: filter=envoy.filters.http.cors status=0 2023-07-08T18:44:10.772350Z\ttrace\tenvoy http\t[C144427][S18087745377321120087] decode headers called: filter=istio.stats status=0 2023-07-08T18:44:10.772969Z\ttrace\tenvoy http\t[C144427][S18087745377321120087] decode headers called: filter=envoy.filters.http.upstream_codec status=4 2023-07-08T18:44:10.773192Z\ttrace\tenvoy http\t[C144427][S18087745377321120087] decode headers called: filter=envoy.filters.http.router status=1 2023-07-08T18:44:10.773377Z\ttrace\tenvoy http\t[C144427] parsed 264 bytes 2023-07-08T18:44:10.773609Z\ttrace\tenvoy http\t[C144427][S18087745377321120087] continuing filter chain: filter=0x55d94218c000 2023-07-08T18:44:10.778127Z\ttrace\tenvoy http\t[C144428] parsing 256 bytes 2023-07-08T18:44:10.778160Z\ttrace\tenvoy http\t[C144428] message begin 2023-07-08T18:44:10.778180Z\ttrace\tenvoy http\t[C144428] completed header: key=Cache-Control value=no-cache, no-store, must-revalidate 2023-07-08T18:44:10.778193Z\ttrace\tenvoy http\t[C144428] completed header: key=Date value=Sat, 08 Jul 2023 17:44:10 GMT 2023-07-08T18:44:10.778200Z\ttrace\tenvoy http\t[C144428] completed header: key=Content-Length value=87 2023-07-08T18:44:10.778208Z\ttrace\tenvoy http\t[C144428] onHeadersCompleteBase 2023-07-08T18:44:10.778212Z\ttrace\tenvoy http\t[C144428] completed header: key=Content-Type value=text/plain; charset=utf-8 2023-07-08T18:44:10.778221Z\ttrace\tenvoy http\t[C144428] status_code 200 2023-07-08T18:44:10.778225Z\ttrace\tenvoy http\t[C144428] Client: onHeadersComplete size=4 2023-07-08T18:44:10.778331Z\ttrace\tenvoy http\t[C144427][S18087745377321120087] encode headers called: filter=istio.stats status=0 2023-07-08T18:44:10.778389Z\ttrace\tenvoy http\t[C144427][S18087745377321120087] encode headers called: filter=envoy.filters.http.cors status=0 2023-07-08T18:44:10.778423Z\ttrace\tenvoy http\t[C144427][S18087745377321120087] encode headers called: filter=envoy.filters.http.fault status=0 2023-07-08T18:44:10.778494Z\ttrace\tenvoy http\t[C144427][S18087745377321120087] encode headers called: filter=istio.metadata_exchange status=0 2023-07-08T18:44:10.778513Z\tdebug\tenvoy http\t[C144427][S18087745377321120087] closing connection due to connection close header 2023-07-08T18:44:10.778533Z\tdebug\tenvoy http\t[C144427][S18087745377321120087] encoding headers via codec (end_stream=false): ':status', '200' 'cache-control', 'no-cache, no-store, must-revalidate' 'date', 'Sat, 08 Jul 2023 17:44:10 GMT' 'content-length', '87' 'content-type', 'text/plain; charset=utf-8' 'x-envoy-upstream-service-time', '5' 'server', 'istio-envoy' 'connection', 'close' followed by a bunch of requests similar to:\n2023-07-08T18:44:11.968591Z\ttrace\tenvoy http\t[C8] parsed 75 bytes 2023-07-08T18:44:13.019511Z\ttrace\tenvoy http\t[C144430] parsing 264 bytes 2023-07-08T18:44:13.019601Z\ttrace\tenvoy http\t[C144430] message begin 2023-07-08T18:44:13.019611Z\tdebug\tenvoy http\t[C144430] new stream 2023-07-08T18:44:13.019637Z\ttrace\tenvoy http\t[C144430] completed header: key=Host value=investigation.svcs.sh 2023-07-08T18:44:13.019649Z\ttrace\tenvoy http\t[C144430] completed header: key=User-Agent value=cert-manager-challenges/v1.11.0 (linux/amd64) cert-manager/2a0ef53b06e183356d922cd58af2510d8885bef5 2023-07-08T18:44:13.019658Z\ttrace\tenvoy http\t[C144430] completed header: key=Accept-Encoding value=gzip 2023-07-08T18:44:13.019667Z\ttrace\tenvoy http\t[C144430] onHeadersCompleteBase 2023-07-08T18:44:13.019670Z\ttrace\tenvoy http\t[C144430] completed header: key=Connection value=close 2023-07-08T18:44:13.019680Z\ttrace\tenvoy http\t[C144430] Server: onHeadersComplete size=4 2023-07-08T18:44:13.019694Z\ttrace\tenvoy http\t[C144430] message complete 2023-07-08T18:44:13.019710Z\tdebug\tenvoy http\t[C144430][S11783251202062423115] request headers complete (end_stream=true): ':authority', 'investigation.svcs.sh' ':path', '/.well-known/acme-challenge/bgLcgdeOvQ4nbDaD4wENG-qR76dxR8GkZKe2Wp1msX0' ':method', 'GET' 'user-agent', 'cert-manager-challenges/v1.11.0 (linux/amd64) cert-manager/2a0ef53b06e183356d922cd58af2510d8885bef5' 'accept-encoding', 'gzip' 'connection', 'close' 2023-07-08T18:44:13.019723Z\tdebug\tenvoy http\t[C144430][S11783251202062423115] request end stream 2023-07-08T18:44:13.019858Z\ttrace\tenvoy http\t[C144430][S11783251202062423115] decode headers called: filter=istio.metadata_exchange status=0 2023-07-08T18:44:13.019880Z\ttrace\tenvoy http\t[C144430][S11783251202062423115] decode headers called: filter=istio.alpn status=0 2023-07-08T18:44:13.019887Z\ttrace\tenvoy http\t[C144430][S11783251202062423115] decode headers called: filter=envoy.filters.http.fault status=0 2023-07-08T18:44:13.019893Z\ttrace\tenvoy http\t[C144430][S11783251202062423115] decode headers called: filter=envoy.filters.http.cors status=0 2023-07-08T18:44:13.019903Z\ttrace\tenvoy http\t[C144430][S11783251202062423115] decode headers called: filter=istio.stats status=0 2023-07-08T18:44:13.020260Z\ttrace\tenvoy http\t[C144430][S11783251202062423115] decode headers called: filter=envoy.filters.http.upstream_codec status=4 2023-07-08T18:44:13.020367Z\ttrace\tenvoy http\t[C144430][S11783251202062423115] decode headers called: filter=envoy.filters.http.router status=1 2023-07-08T18:44:13.020400Z\ttrace\tenvoy http\t[C144430] parsed 264 bytes 2023-07-08T18:44:13.020544Z\ttrace\tenvoy http\t[C144430][S11783251202062423115] continuing filter chain: filter=0x55d941dc5710 2023-07-08T18:44:13.023215Z\ttrace\tenvoy http\t[C144431] parsing 256 bytes 2023-07-08T18:44:13.023309Z\ttrace\tenvoy http\t[C144431] message begin 2023-07-08T18:44:13.023360Z\ttrace\tenvoy http\t[C144431] completed header: key=Cache-Control value=no-cache, no-store, must-revalidate 2023-07-08T18:44:13.023385Z\ttrace\tenvoy http\t[C144431] completed header: key=Date value=Sat, 08 Jul 2023 17:44:13 GMT 2023-07-08T18:44:13.023403Z\ttrace\tenvoy http\t[C144431] completed header: key=Content-Length value=87 2023-07-08T18:44:13.023445Z\ttrace\tenvoy http\t[C144431] onHeadersCompleteBase 2023-07-08T18:44:13.023464Z\ttrace\tenvoy http\t[C144431] completed header: key=Content-Type value=text/plain; charset=utf-8 2023-07-08T18:44:13.023487Z\ttrace\tenvoy http\t[C144431] status_code 200 2023-07-08T18:44:13.023569Z\ttrace\tenvoy http\t[C144431] Client: onHeadersComplete size=4 2023-07-08T18:44:13.023705Z\ttrace\tenvoy http\t[C144430][S11783251202062423115] encode headers called: filter=istio.stats status=0 2023-07-08T18:44:13.023770Z\ttrace\tenvoy http\t[C144430][S11783251202062423115] encode headers called: filter=envoy.filters.http.cors status=0 2023-07-08T18:44:13.023791Z\ttrace\tenvoy http\t[C144430][S11783251202062423115] encode headers called: filter=envoy.filters.http.fault status=0 2023-07-08T18:44:13.023817Z\ttrace\tenvoy http\t[C144430][S11783251202062423115] encode headers called: filter=istio.metadata_exchange status=0 2023-07-08T18:44:13.023861Z\tdebug\tenvoy http\t[C144430][S11783251202062423115] closing connection due to connection close header 2023-07-08T18:44:13.023898Z\tdebug\tenvoy http\t[C144430][S11783251202062423115] encoding headers via codec (end_stream=false): ':status', '200' 'cache-control', 'no-cache, no-store, must-revalidate' 'date', 'Sat, 08 Jul 2023 17:44:13 GMT' 'content-length', '87' 'content-type', 'text/plain; charset=utf-8' 'x-envoy-upstream-service-time', '3' 'server', 'istio-envoy' 'connection', 'close' and another bunch of requests like this one:\n2023-07-08T18:44:21.968492Z\ttrace\tenvoy http\t[C3] parsed 75 bytes 2023-07-08T18:44:22.749799Z\ttrace\tenvoy http\t[C144440] parsing 265 bytes 2023-07-08T18:44:22.749841Z\ttrace\tenvoy http\t[C144440] message begin 2023-07-08T18:44:22.749850Z\tdebug\tenvoy http\t[C144440] new stream 2023-07-08T18:44:22.749875Z\ttrace\tenvoy http\t[C144440] completed header: key=Host value=investigation.svcs.sh 2023-07-08T18:44:22.749884Z\ttrace\tenvoy http\t[C144440] completed header: key=User-Agent value=Mozilla/5.0 (compatible; Let's Encrypt validation server; +https://www.letsencrypt.org) 2023-07-08T18:44:22.749890Z\ttrace\tenvoy http\t[C144440] completed header: key=Accept value=*/* 2023-07-08T18:44:22.749898Z\ttrace\tenvoy http\t[C144440] completed header: key=Accept-Encoding value=gzip 2023-07-08T18:44:22.749904Z\ttrace\tenvoy http\t[C144440] onHeadersCompleteBase 2023-07-08T18:44:22.749907Z\ttrace\tenvoy http\t[C144440] completed header: key=Connection value=close 2023-07-08T18:44:22.749914Z\ttrace\tenvoy http\t[C144440] Server: onHeadersComplete size=5 2023-07-08T18:44:22.749925Z\ttrace\tenvoy http\t[C144440] message complete 2023-07-08T18:44:22.749939Z\tdebug\tenvoy http\t[C144440][S3714970174883588932] request headers complete (end_stream=true): ':authority', 'investigation.svcs.sh' ':path', '/.well-known/acme-challenge/bgLcgdeOvQ4nbDaD4wENG-qR76dxR8GkZKe2Wp1msX0' ':method', 'GET' 'user-agent', 'Mozilla/5.0 (compatible; Let's Encrypt validation server; +https://www.letsencrypt.org)' 'accept', '*/*' 'accept-encoding', 'gzip' 'connection', 'close' 2023-07-08T18:44:22.749956Z\tdebug\tenvoy http\t[C144440][S3714970174883588932] request end stream 2023-07-08T18:44:22.750082Z\ttrace\tenvoy http\t[C144440][S3714970174883588932] decode headers called: filter=istio.metadata_exchange status=0 2023-07-08T18:44:22.750731Z\ttrace\tenvoy http\t[C144440][S3714970174883588932] decode headers called: filter=istio.alpn status=0 2023-07-08T18:44:22.750789Z\ttrace\tenvoy http\t[C144440][S3714970174883588932] decode headers called: filter=envoy.filters.http.fault status=0 2023-07-08T18:44:22.750842Z\ttrace\tenvoy http\t[C144440][S3714970174883588932] decode headers called: filter=envoy.filters.http.cors status=0 2023-07-08T18:44:22.750865Z\ttrace\tenvoy http\t[C144440][S3714970174883588932] decode headers called: filter=istio.stats status=0 2023-07-08T18:44:22.751403Z\ttrace\tenvoy http\t[C144440][S3714970174883588932] decode headers called: filter=envoy.filters.http.upstream_codec status=4 2023-07-08T18:44:22.751436Z\ttrace\tenvoy http\t[C144440][S3714970174883588932] decode headers called: filter=envoy.filters.http.router status=1 2023-07-08T18:44:22.751443Z\ttrace\tenvoy http\t[C144440] parsed 265 bytes 2023-07-08T18:44:22.751507Z\ttrace\tenvoy http\t[C144440][S3714970174883588932] continuing filter chain: filter=0x55d941e44900 2023-07-08T18:44:22.754621Z\ttrace\tenvoy http\t[C144441] parsing 256 bytes 2023-07-08T18:44:22.754641Z\ttrace\tenvoy http\t[C144441] message begin 2023-07-08T18:44:22.754656Z\ttrace\tenvoy http\t[C144441] completed header: key=Cache-Control value=no-cache, no-store, must-revalidate 2023-07-08T18:44:22.754666Z\ttrace\tenvoy http\t[C144441] completed header: key=Date value=Sat, 08 Jul 2023 17:44:22 GMT 2023-07-08T18:44:22.754671Z\ttrace\tenvoy http\t[C144441] completed header: key=Content-Length value=87 2023-07-08T18:44:22.754710Z\ttrace\tenvoy http\t[C144441] onHeadersCompleteBase 2023-07-08T18:44:22.754720Z\ttrace\tenvoy http\t[C144441] completed header: key=Content-Type value=text/plain; charset=utf-8 2023-07-08T18:44:22.754731Z\ttrace\tenvoy http\t[C144441] status_code 200 2023-07-08T18:44:22.754734Z\ttrace\tenvoy http\t[C144441] Client: onHeadersComplete size=4 2023-07-08T18:44:22.754827Z\ttrace\tenvoy http\t[C144440][S3714970174883588932] encode headers called: filter=istio.stats status=0 2023-07-08T18:44:22.754935Z\ttrace\tenvoy http\t[C144440][S3714970174883588932] encode headers called: filter=envoy.filters.http.cors status=0 2023-07-08T18:44:22.755045Z\ttrace\tenvoy http\t[C144440][S3714970174883588932] encode headers called: filter=envoy.filters.http.fault status=0 2023-07-08T18:44:22.755092Z\ttrace\tenvoy http\t[C144440][S3714970174883588932] encode headers called: filter=istio.metadata_exchange status=0 2023-07-08T18:44:22.755185Z\tdebug\tenvoy http\t[C144440][S3714970174883588932] closing connection due to connection close header 2023-07-08T18:44:22.755289Z\tdebug\tenvoy http\t[C144440][S3714970174883588932] encoding headers via codec (end_stream=false): ':status', '200' 'cache-control', 'no-cache, no-store, must-revalidate' 'date', 'Sat, 08 Jul 2023 17:44:22 GMT' 'content-length', '87' 'content-type', 'text/plain; charset=utf-8' 'x-envoy-upstream-service-time', '3' 'server', 'istio-envoy' 'connection', 'close' Let’s Encrypt talks over port 80. We just observed the solving of the challenge, the certificate has been issued.\nfinal conclusion Keep the httpRedirect set to false in the Gateway. Use a match.uri.prefix with scheme.exact: http and redirect.scheme: https to handle the redirect selectively in a VirtualService. That’s the best way to handle catch-all HTTPS redirect and I was able to fnd an explanation for it.\nLet’s Encrypt HTTP-01 challenge ↩︎\nIstio ServerTLSSettings ↩︎\nhttpsRedirect breaks cert-manager acme challenge, which was working in Istio 1.6 ↩︎\nIstio sources: Envoy interaction ↩︎\nDebugging Envoy and Istiod ↩︎\nEnvoy config.route.v3.VirtualHost.TlsRequirementType ↩︎\nEnvoy: New feature: Scheme redirect in RedirectAction ↩︎\nIngress handling cert-manager source code. ↩︎\ncert-manager ACME HTTP-01 solver testReachability function ↩︎\n","description":"cert-manager ACME Let’s Encrypt with working catch-all HTTPS redirect, the how and the why","tags":["kubernetes","k8s","letsencrypt","tls","cert-manager","istio"],"title":"Istio, cert-manager, Let’s Encrypt and HTTPS redirect","uri":"/posts/2023-07-09-istio-cert-manager-lets-encrypt-and-https-redirect/"},{"content":"","description":"","tags":null,"title":"letsencrypt","uri":"/tags/letsencrypt/"},{"content":"","description":"","tags":null,"title":"privacy","uri":"/tags/privacy/"},{"content":"On 5th of July I have made a number of changes to the privacy settings on this website. Without going too much into detail about reasons:\nThis website no longer uses Google Analytics. Theme fonts are no longer loaded from Google Fonts service. They’re now served from the gruchalski.com domain. If you’d like to do the same, here’s a good place to start: https://github.com/radekg/google-font-download. The newsletter has been cancelled. Mailchimp decided to nuke my account some time ago anyway. I have rolled out self-hosted analytics service. About the analytics service: Reside in German Hetzner Online GmbH data centers. Captured data is not shared, and never will be shared with third-parties. Respects your Do Not Track setting. Does not use cookies. Data is public and can be viewed here. Contains data starting from 5th of July 2023. The Privacy page has been updated accordingly.\n","description":"","tags":["privacy","thoughts"],"title":"Privacy settings updates","uri":"/posts/2023-07-06-privacy-settings-updates/"},{"content":"","description":"","tags":null,"title":"thoughts","uri":"/tags/thoughts/"},{"content":"This is a personal blog of Radek Gruchalski. This privacy policy explains how the collected personal data collected is used.\ntable of contents subprocessors hosting this website analytics service CDN contents GitHub Gist jsDelivr about analytics how is the data used? cookies how to contact me subprocessors hosting this website This website is hosted on GitHub Pages. The GitHub Privacy Statement is available here.\nanalytics service Analytics service is hosted in German Hetzner Online GmbH data centers. Datenschutz of Hetzner Online GmbH is available here.\nCDN contents GitHub Gist Some posts on this blog load code listings from the GitHub Gist service. The GitHub Privacy Statement is available here.\njsDelivr The blog uses jsDelivr CDN for JavaScript resources. The jsDelivr privacy policy is available here.\nabout analytics Self-hosted by the author of this website in German Hetzner Online GmbH data centers. Captured data is not shared, and never will be shared with third-parties. The analytics service respects your Do Not Track setting. The analytics service does not use cookies. Data is public and can be viewed here. Contains data starting from 5th of July 2023. how is the data used? Except of IP addresses shared by your browser while accessing this website and CDN contents, no other data is collected and shared with third-parties.\ncookies This website does not use cookies.\nhow to contact me If you have any questions, you can contact, Radek Gruchalski, directly via:\nEmail: radek@gruchalski.com. ","description":"","tags":null,"title":"Privacy","uri":"/privacy/"},{"content":"Regardless of the technology, most general-purpose software is easy.\nThere’s a set of requirements. Someone implements said requirements. Code gets tested, problems are fixed, and code issues are resolved. Code is promoted to production. New requirements come in, and the process repeats. Dependencies change. Someone tests them and releases a new version. Like anything else critical to the business, software needs to be monitored. Every aspect from the list above can be streamlined and/or automated.\nOpen Source software and libraries: use the global brain. Someone already had that problem.Be a good citizen, open source your code, and contribute back wherever possible, please. Unit and integration tests: clicking through test scenarios does not scale.Someone is paid to sit there and click through tests. One person can click through one scenario at a time. Automate that process. Have thousands of virtual persons doing it at a whim. CI/CD: automated upgrades and production promotion: once the code is automatically tested, there’s nothing preventing automatic production code release. There are exceptions to this rule, of course. YMMV. Automatic monitoring: released software adheres to specific KPIs. Any non-conformity results in automatic rollback or notification to the operation team. Rollbacks can be automated. Backups can be automated. Backup restore can be automated, but human supervision is advised. The software does not live in isolation. Software is constrained by technical, societal, and political issues.\nTechnical ones usually do not go beyond some head-scratching. Maybe there’s VPN, or maybe there’s a need for a proxy. A wire format has to be converted to another one. Maybe there’s very little compute available, or storage can be done only via some third-party object storage. We can generalize: in software, there are no unsolvable technical issues.\nSocietal and political problems are tougher to solve because human interaction is involved, often centered around emotional matters.\nBut two things do not scale for sure:\nMeetings: once someone attends a meeting, they’re bound to commit their full attention. They cannot do anything else at that time. Is the meeting necessary? Custom code: if the global brain does not have the answer, someone has to write the code. A pair of hands can only crank out so many lines per day. Make it count and avoid politics. A brain of a developer is focused on solving a problem. Any interruption derails the thought process. The longer the interruption, the more difficult it is to get the thought process back on track. Political interruptions cause uncertainty, and the focus shifts to a different track, the one the developer is often not accustomed to dealing with. Solutions:\nPrefer asynchronous communication with continuous documentation. Good knowledge is documented, not passed on by word. Passing on knowledge by word promotes cliques. Cliques lead to office politics. If the meeting must be: Announce before what’s the expected outcome. What is the meeting about? Document the outcome: take minutes. Allow healthy deviation from the beaten path but identify side-tracks quickly, and move them offline. Remove political obstacles. Developers are paid to write code, not to participate in the power struggle theatre. ","description":"","tags":["software","thoughts"],"title":"What does not scale in software","uri":"/posts/2022-10-30-what-does-not-scale-in-software/"},{"content":"","description":"","tags":null,"title":"prolog","uri":"/tags/prolog/"},{"content":"","description":"","tags":null,"title":"zanzibar","uri":"/tags/zanzibar/"},{"content":"Up until about a month ago, my understanding of how Zanzibar was supposed to work was wrong. Some quick notes:\nI completely misunderstood Section 2.3. of the whitepaper. The pseudo-code notation is, in fact, a namespace configuration language. Zanzibar tuples aren’t enough to infer relationships in a Zanzibar-style system. This clarifies some of my misconceptions related to the implementation of unions, exclusions, and inheritance. It was clear in my previous posts that all my implementations missed the ability to define such relations. Tuples are a query language. The namespace configuration defines said relations. A configuration consists of a set of relations. A relation can define a set operation over user sets. An operation is usually one of union, exclusion, or intersection. Other operation types can be defined as needed. User sets to apply the operation are defined via the child element. There are three child types: _this(): implies any user in the direct relationship. computed_userset: any computed user referenced via another relationship. tuple_to_userset: a computed user set of a relation defined by another tupleset. Based on those points, I have implemented a tuple parser and a namespace configuration parser with a lexer. The output of the parser could be used to implement the API.\nWorking on the lexer, I have realized that pursuing this project further does not make sense. There are many types of applications: embedded, desktop, server-side microservices, distributed permissions API, sharded systems, geo-sensitive systems, and more.\nA cliché: for a permissions API, there’s no one size fits all.\nAlmost every application is unique in detail. Almost every use case is unique. There’s always an implementation detail preventing one from using an upstream library, or a project, without the necessity for minor adjustments to make it fit. Many applications require the decision engine right at the edge close to the code where the decision has to be made. Thus negating a centralized API. OPA with Rego is the thing to use. No need to reinvent the wheel. Period. Full stop.\nI said this before:\nThe challenge isn’t the API or general-purpose query mechanism. The challenge is the distribution of the facts used to make the decision.\nHence delivering the functional Zanzibar namespace configuration parser and lexer is an excellent moment to hang the towel. The code is available on GitHub.\n","description":"Hanging the towel on the Zanzibar work","tags":["prolog","zanzibar"],"title":"Zanzibar with Prolog - summary","uri":"/posts/2022-10-22-zanzibar-with-prolog-summary/"},{"content":"","description":"","tags":null,"title":"firebuild","uri":"/tags/firebuild/"},{"content":"","description":"","tags":null,"title":"keto","uri":"/tags/keto/"},{"content":"","description":"","tags":null,"title":"opa","uri":"/tags/opa/"},{"content":"I’ve been working my way through The Art of Prolog. The book is a fantastic eye-opener. The first five chapters are very science-heavy as they cover all Prolog principles. What follows are the twenty or so delightful, short, but on-subject chapters explaining the Prolog language in detail. I have a couple of chapters left to go through. I am looking forward to the compiler example from chapter 24.\nHowever, I do have a couple of research problems to look at.\ntable of contents on (SWI-)Prolog last week’s problems general fact container providing a type of the fact to the query next steps scratching the itch on (SWI-)Prolog Before I jump into that, though, some thoughts on Prolog.\nWhat strikes me when writing Prolog code: what, not how. Solving a Problem in Prolog puts the focus on data flow. A continuous data transformation. The Zen of the thought process is minted in composable predicates, which the Prolog VM uses to solve the problem at hand.\nI had such an aha moment six years ago when I was learning Erlang. Bugs appear when the logic applied to the problem isn’t correct.\nThere are no loops, no syntactical conditionals, no return statements, and no state mutation. There is so little noise in the code. Individual predicates replacing traditional conditional branches from imperative languages make the reasoning about each branch easy to follow and test. The code is much more composable and ready to test in isolation. Now I remember why I enjoyed Erlang so much. There is one more language family with such utility—everything Lisp, everything S-expressions.\nProlog is not your grandma’s programming language. Definitely not SWI-Prolog, a Prolog implementation actively developed since 1985. It’s a very modern platform for any task one can imagine having today.\nOn top of my head:\nTCP, UDP, sockets. HTTP servers. TLS support. Multi-threading. RocksDB. Paxos library. Protocol buffers. Coherent testing story. Official Docker image in the Docker Hub, running programs in containers. C and Java interoperability. That, and more, is what SWI-Prolog offers. I haven’t had a chance yet to try Scryer Prolog or Ichiban Prolog, but I expect that having a combination of Prolog and a powerful, expressive, imperative language available within a single program can be a huge productivity boost.\nlast week’s problems In the Zanzibar-style ACLs in Prolog1 article, I experienced a couple of problems prohibiting general applicability to many children, single parent permission inheritance. My original implementation used hard-coded document and directory fact predicates inside of the inherits/3 and API code. I outlined a couple of possible improvements:\nFind a way to provide the type of fact to query for. Or create a general fact container. Both are equally easy to implement, but there are significant differences in the user experience.\ngeneral fact container The inheritance resolution program requires something to resolve the parent of an object with a given ID to traverse the inheritance tree. Given an ID, fetch the parent reference and use it further to seek a solution. Given:\nA needle name in form of an atom A compound container term. Find the compound member named like the atom inside of the container term. For example:\n1 2 3 4 5 search_compound(parent, directory( dirname('documents'), objectid('b94c1f40-818b-47be-882b-ce1a8fbee254'), parent( objectid('851a7566-b83d-4993-bd04-9b8244091d45'))), Out). Given the directory(...) term, the Out value should be parent(objectid('851a7566-b83d-4993-bd04-9b8244091d45')). Frankly, anything that contains parent(...) should be handled the same way. This problem can be solved by using a couple of built-ins: functor/3 and arg/3. Here’s my solution:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 search_compound(Atom, Term, Out) :- compound(Term), functor(Term, _, Y), search_compound(Atom, Term, Out, 1, Y). search_compound(Atom, Term, Value, N, _) :- atom(Atom), arg(N, Term, Value), compound(Value), % To support compounds with arity 0, % change the functor/3 below to % compound_name_arity/3. % **Warning**: This is SWI-Prolog specific, it may not work in other Prolog implementations. functor(Value, Atom, _). search_compound(Atom, Term, Out, N, T) :- N \u003c T, M is N + 1, search_compound(Atom, Term, Out, M, T). This code, given any valid compound, will figure out its arity and recursively try to find an argument with the name under an atom. This solution works pretty well, except for two issues:\nAn argument cannot be of zero arity because the functor/3 predicate raises an exception in that case. For example: 1 2 3 4 5 6 7 8 search_compound(parent, directory( whatever(), dirname('documents'), objectid('b94c1f40-818b-47be-882b-ce1a8fbee254'), parent( objectid('851a7566-b83d-4993-bd04-9b8244091d45'))), Out). % ERROR: Domain error: `compound_non_zero_arity' expected, found `whatever()' But this will be fine:\n1 2 3 4 5 6 7 8 search_compound(whatever, directory( whatever(nil), dirname('documents'), objectid('b94c1f40-818b-47be-882b-ce1a8fbee254'), parent( objectid('851a7566-b83d-4993-bd04-9b8244091d45'))), Out). % Out = whatever(nil). The second problem is that the, in this case, directory(...) term has to be available at the call site. As in, it has to come from some other call site. Ergo, it must be fully typed by someone into the program. A usual case would be to have a bunch of facts in a global database, as I had in my previous article, and I would like to reference those. Hence, I would like to avoid having to type them in. But, it’s not trivial to take a reference to a term of possibly variable arity and pass it as an argument. Prolog provides meta-programming facilities allowing a term to be reconstructed at run time, but it’s not trivial. I’m happy with the code, but for the reasons above, I don’t like this solution.\nproviding a type of the fact to the query In imperative language, I would consider solving this problem by passing a closure that, given an ID of an object, returns the parent reference. Prolog does not offer closures, but it has something that can be compared to a closure, if you squint hard enough… Consider the following code:\n1 2 3 4 5 6 7 8 9 ?- assertz(hello(Arg) :- format('Hello, world, you said: ~q!~n', [Arg])). true. ?- Proc = hello('a'). Proc = hello(a). ?- Proc = hello('a'), Proc. Hello, world, you said: a! Proc = hello(a). What happens here, Proc is given the predicate hello('a') as a value, the standalone Proc. calls that value as a predicate. We can make it better using the built-in =.. (univ) predicate to construct a call site from arguments. Here’s a rudimentary example:\n1 2 3 4 5 6 ?- assertz((call_it(Atom, Arg) :- Proc =.. [Atom, Arg], Proc)). true. ?- call_it(hello, 'a'). Hello, world, you said: a! true. This is much more promising than the previous solution. Using the construct above, I have arrived at the following example:\n1 2 3 4 5 6 7 8 9 10 11 12 entity( objectid('851a7566-b83d-4993-bd04-9b8244091d45'), parent(objectid()), metadata([type, directory, name, 'data'])). find_parent(Id, Out) :- ProcCall =.. [parent_resolver, Id, Out], ProcCall. % I could use call(ProcCall) but this does the job. % The user of the API need to provide this predicate: parent_resolver(Id, Out) :- entity(objectid(Id), Out, _). And the call:\n1 2 ?- find_parent('851a7566-b83d-4993-bd04-9b8244091d45', Out). Out = parent(objectid()). Yes, this is much better. The find_parent/3 predicate would be called by existing inherits/3. The user would be required to provide their parent_resolver/2, which makes sense as the user knows the structure of the compound they work with.\nnext steps The immediate next step is to integrate this into the actual program so that I have a general inheritance API. The next problem is more interesting: the distribution of the individual facts close to the compute so that a small Prolog program can use a minimum necessary number of facts required to make a decision. The simplest way of doing it is to provide something similar to the Open Policy Agent bundles.\nThe second step is a little bit more mundane task to provide a trace facility to the user so that the user knows why permission is granted or denied. Similar to the Open Policy Agent playground trace feature, or the explanation API in Keto. A pure Prolog recipe exists in chapter 17 of The Art of Prolog.\nscratching the itch Having solved this problem in OPA and Prolog, I’m pretty confident the permissions API does not need a database to work. Neither does one want a database for this because:\nA single-node RDBMS will be a single point of failure, always. Building and operating a leader/follower RDBMS setup reliably is not easy. A distributed RDBMS is a massive cost and operational factor. OPA is the best example of how to do this best. Rego is very lightweight. The OPA VM is embeddable and available in many programming languages as a library.\nThat makes it easy to move the compute very close to where the problem is, even into the actual application. Inside of the application, that’s where this journey started for me two years ago. I was trying to scratch my itch, which was to provide permissions API to Firebuild. Today, I know I could have used OPA. But I now understand why that would have been the best choice back then.\nThe problem is not in querying for permissions, the problem is in bringing the facts to where the decision has to be made. The key to solving this in Prolog is sticking to standard Prolog and leveraging whatever implementation there exists for a programming language a developer is using for their application.\nTime to refocus and bring two years’ worth of learnings together.\nZanzibar-style ACLs in Prolog ↩︎\n","description":"","tags":["prolog","zanzibar","opa","keto","firebuild"],"title":"Zanzibar with Prolog, week 2","uri":"/posts/2022-09-10-zanzibar-with-prolog-week-2/"},{"content":"","description":"","tags":null,"title":"datalog","uri":"/tags/datalog/"},{"content":"Back in May, I looked at an implementation of Zanzibar-style ACLs in Open Policy Agent Rego1.\nI’m revisiting that problem while learning Prolog, another language from my never-ending to-learn list. There are some similarities to the previous article, but I’m taking the problem further: I’m adding permissions inheritance from anywhere within the filesystem tree.\ntable of contents the problem iterating on the problem the facts: filesystem the facts: permissions utilities object permissions and inheritance SWI-Prolog trying out object permissions explaining inheritance predicate order on unification trying out inheritance tracing inherits/3 tracing: the curious case of the predicate order querying file permissions directly summary thoughts on Prolog the problem In Zanzibar-style ACLs with OPA Rego1 I have implemented the ACLs for the canonical example from the Zanzibar whitepaper:\nname: \"doc\" relation { name: \"owner\" } relation { name: \"editor\" userset_rewrite { union { child { _this {} } child { computed_userset { relation: \"owner\" } } } } } relation { name: \"viewer\" userset_rewrite { union { child { _this {} } child { computed_userset { relation: \"editor\" } } child { tuple_to_userset { tupleset { relation: \"parent\" } computed_userset { object: $TUPLE_USERSET_OBJECT # parent folder relation: \"viewer\" } } } } } } It reads like this:\nA document has owners. Document owners are also editors. Document editors are also document viewers. Furthermore, the viewers of a parent folder are also viewers of the document. The notation I came up with:\ndoc:some-doc#viewer@doc:some-doc#editor@doc:some-doc#owner doc:some-doc#viewer@folder:some-doc-parent-folder#viewer You can find the complete reasoning behind this in the article linked above in this section.\niterating on the problem While solving this problem in Prolog, I’m going to take this a step further. ACLs can be inherited from any parent directory in the tree, up to the root directory.\nCode examples will operate on UUIDs exclusively but in the article, I will be referring to files and directories by their names. The tree used in this example is small, so that it can be easily reasoned about:\ndata root, a bucket, UUID: 851a7566-b83d-4993-bd04-9b8244091d45 ├-- a-data-item.png file, UUID: 9c57448c-e901-4fd3-a47e-b6ecf0470c5f └-- documents directory, UUID: b94c1f40-818b-47be-882b-ce1a8fbee254 ├-- a-document.md file, UUID: f666cc22-1e92-457b-b7ee-0f6b48825185 └-- pdfs directory, UUID: 7f916bd9-f324-40f2-97d3-95e7cc7d722e ├-- a-file.pdf file, UUID: bee100a0-30b1-4184-94aa-3d408b75e938 └-- another.pdf file, UUID: ecf47371-fd08-44f1-9c06-7c970d948ae8 A permission can be assigned to a file or directory. A permission assigned to a directory applies to all child directories and files. For example:\nAn editor of data directory can edit and view: a-data-item.png, a-document.md, and a-file.pdf. A viewer of pdfs can view a-file.pdf only. A viewer of a-file.pdf can view that file only and no other file inside of the parent directory pdfs. Permissions will be stored outside of filesystem objects and they are optional for an object. An object may or may not have a set of permissions attached. If an object does not have permissions attached, it’s okay, they are simply inherited from a directory above.\nthe facts: filesystem The filesystem is defined using the following facts:\nA directory fact has three properties:\nA name. An object ID. A parent object ID. The root directory does not have a parent and that fact is encoded by using parent(objectid()) with value in objectid(). A document has exactly one parent pointing at a directory.\nthe facts: permissions Permissions are optional for an object hence there are less permissions than a total of directories and documents. A permission fact contains an object ID it attaches to, and a list of permissions.\nPermissions are represented as a list of [role₁, [identity₁, ...], ... roleₙ, [identityₙ, ...]].\nFrom the set of facts, we can deduce the following:\nBerta is the data owner. Charlie is an editor of data. Alan is a viewer of pdfs. Jake can only view the a-file.pdf file. utilities The program requires a bunch of utilities:\nappend(+L1, +L2, -OutList): appends a list to another list, for example append([a, b], [c, d], -Out) → Out = [a, b, c, d]. in_list(+Atom, +List): an implementation of member, it checks if an item exists in the list. find_kv(+Atom, +List, -OutList): given a list of [k₁, v₁, k₂, v₂, ... kₙ, vₙ], finds vₙ for requested kₙ, returns a default value if nothing found. The program here assumes that vₙ is a list. Here’s the code:\nobject permissions and inheritance Now, I need a method of fetching object permissions and figuring out inheritance. Here’s the strategy:\nQuerying for permissions of an object implies that three facts are known: An ID of the object. An identity, in this case, a username. A relation type: viewer, editor, owner. The problem statement says that an owner is an editor, and an editor is a viewer. What does this mean, logically speaking? Well, it simply means that for a user to be a viewer it doesn’t matter if the user is an editor or an owner. Example:\n1 2 3 permissions( objectid('851a7566-b83d-4993-bd04-9b8244091d45'), [owners, [berta], editors, [charlie]]). The above declares a set of permissions for the data directory (the bucket). It says that Berta belongs to the owners and Charlie is an editor. To find out if Berta is an editor, the program needs to find a list of editors and a list of owners. Two lists have to be combined, the result includes berta, Berta is an editor. Regardless of the actual assigned role is. Finding out which role is actually assigned to Berta would imply another API.\nHere’s the code:\nThe relevant part is object_permissions(+Id, +Roles, -CombinedRoles).\nThis predicate recurses on +Roles list, which is given as [role₁, role₂, ... roleₙ]. For each role, the following happens: The program tries to get the permission list for an object ID. As we said, permissions for an object ID are optional, hence we have a guarding object_permissions(_, _, []).. If there are no permissions for an ID, return an empty list. If permissions for the ID exist, call find_kv(+Role, +Permissions, -RolePermissions). Recurse on the +RoleTail. Whatever comes back from find_kv under -RolePermissions gets appended to the accumulator -Out. Let’s load it all up in Prolog and see it in action.\nSWI-Prolog By far the easiest way to start with Prolog is SWI-Prolog. To install SWI-Prolog:\nOn macOS: brew install swi-prolog. On Debian-based Linux: sudo apt-get install swi-prolog -y. The installation will take a bit but when finished, SWI-Prolog can be started from the terminal by executing swipl command.\ntrying out object permissions Okay, we have Prolog available. Let’s set up a workspace and try out those permissions.\n1 2 3 4 5 6 7 8 9 mkdir -p /tmp/zanzibar cd /tmp/zanzibar export site=\"https://gist.githubusercontent.com\" export gist=15bc4c9ea6a9be11a63c79c4b9d392537bfbe03a export base_url=\"${site}/radekg/192ed65e53299bce74f59cfdbabf94ac/raw/${gist}\" wget -O 01-filesystem.pl \"${base_url}/01-filesystem.pl\" wget -O 02-permissions.pl \"${base_url}/02-permissions.pl\" wget -O 03-utilities.pl \"${base_url}/03-utilities.pl\" wget -O 04-inheritance.pl \"${base_url}/04-inheritance.pl\" Start swipl:\n1 swipl Welcome to SWI-Prolog (threaded, 64 bits, version 8.4.3) SWI-Prolog comes with ABSOLUTELY NO WARRANTY. This is free software. Please run ?- license. for legal details. For online help and background, visit https://www.swi-prolog.org For built-in help, use ?- help(Topic). or ?- apropos(Word). ?- Load all 4 files:\n1 2 ?- [\"01-filesystem.pl\", \"02-permissions.pl\", \"03-utilities.pl\", \"04-inheritance.pl\"]. true. So, let’s get a list of all permitted owners, editors, and viewers for object 851a7566-b83d-4993-bd04-9b8244091d45.\n1 2 ?- object_permissions('851a7566-b83d-4993-bd04-9b8244091d45', [owners, editors, viewers], Out). Out = [berta, charlie] Now, owners only:\n1 2 ?- object_permissions('851a7566-b83d-4993-bd04-9b8244091d45', [owners], Out). Out = [berta] . Note: You will have to type the final dot after receiving the answer to come back to the Prolog ?- shell.\nAwesome, so this shows has to retrieve a combined list of users allowed to access an object at an expected level. The role list containing multiple roles, for example [owners, editors, viewers] is a very simple method of saying that all owners belong to a set of owners+editors+viewers.\nNote2: To quit swipl back to the terminal shell, press CTRL+D.\nLet’s move on to the inheritance.\nexplaining inheritance Let’s recap: given a file ID, the program will traverse the parents up to the root directory (the bucket). To solve this question, the following facts need to be known:\nThe object ID for which we want to know if the user has the required permission. The required access level. The identity. The traversal must continue only until:\nThe root has been reached. Or until the user has been found in any inspected permission for the first time. If the user has been found to have permission before reaching the root, there is no point in traversing all the way to the root anymore. This process is implemented in the recursive inherits/3 predicate. This predicate recurses over parent directories. There are three variants of this predicate defined:\nWhen reaching a root (above the root of the bucket), there is nothing else to do, if the user isn’t found to be in a role until now, they are not allowed to perform the requested role: 1 inherits(objectid(), _, _) :- false. When permissions returned from optional_object_permissions/3 contain the searched username, return true because the user is found. The recursion stops, thus the traversal stops. 1 2 3 inherits(objectid(ObjectId), Roles, User) :- optional_object_permissions(ObjectId, Roles, Permissions), in_list(User, Permissions). Otherwise, get the parent directory and recursively call inherits/3 for the parent directory. 1 2 3 inherits(objectid(ObjectId), Roles, User) :- directory(_, objectid(ObjectId), parent(objectid(ParentObjectId))), inherits(objectid(ParentObjectId), Roles, User). predicate order For this code, it is important to remember that the predicate order in Prolog is significant. The second predicate uses in_list/2 to check if the solution has been found, in which case, the recursion gets interrupted. The third predicate does not have a negating \\+ in_list/2 call so the order is important.\nThose three predicates roughly translate to:\nif reached_root false else if user_found in permissions true else fetch_parent recurse Unlike Datalog—a subset of Prolog—Prolog itself evaluates predicates in order. It will attempt to solve a query with the first occurrence of the predicate, if that predicate returns false at any point, Prolog will retry answering the question with the next defined predicate that can be unified. This process repeats until all unifying predicates have been exhausted, at which point, false is returned. Another major difference between Prolog and Datalog: Datalog programs always terminate, and infinite loops aren’t possible while they can happen in Prolog.\nI’m mentioning Datalog because OPA Rego is a Datalog-inspired language.\non unification The in-detail explanation of unification goes beyond this article. I recommend the following article to learn more about unification in Prolog2.\nTwo terms unify if they are the same term or if they contain variables that can be uniformly instantiated with terms in such a way that the resulting terms are equal.\ntrying out inheritance Let’s see how to find out if the user is allowed to become a certain role using inherits/3.\nIs Berta allowed to view a-file.pdf (object ID bee100a0-30b1-4184-94aa-3d408b75e938)? Remember, inherits means exactly that: does the user inherit a role from any parent directory? This means we cannot query directly for a file object ID just yet. So, in this case, we are querying for the parent directory ID: 7f916bd9-f324-40f2-97d3-95e7cc7d722e.\n1 2 ?- inherits(objectid('7f916bd9-f324-40f2-97d3-95e7cc7d722e'), [owners, editors, viewers], berta). true . What about Charlie, is he an editor?\n1 2 ?- inherits(objectid('7f916bd9-f324-40f2-97d3-95e7cc7d722e'), [editors, viewers], charlie). true . But is he an owner?\n1 2 ?- inherits(objectid('7f916bd9-f324-40f2-97d3-95e7cc7d722e'), [owners], charlie). false. tracing inherits/3 Let’s verify that our inherits/3 traverses the directory tree only until the user has been found. We can trace the execution of selected predicates using the built-in trace/1 predicate.\n1 2 3 ?- trace(inherits/3). % inherits/3: [all] true. And retry the query for Berta:\n1 2 3 4 5 6 7 8 ?- inherits(objectid('7f916bd9-f324-40f2-97d3-95e7cc7d722e'), [owners, editors, viewers], berta). T [10] Call: inherits(objectid('7f916bd9-f324-40f2-97d3-95e7cc7d722e'), [owners, editors, viewers], berta) T [19] Call: inherits(objectid('b94c1f40-818b-47be-882b-ce1a8fbee254'), [owners, editors, viewers], berta) T [28] Call: inherits(objectid('851a7566-b83d-4993-bd04-9b8244091d45'), [owners, editors, viewers], berta) T [28] Exit: inherits(objectid('851a7566-b83d-4993-bd04-9b8244091d45'), [owners, editors, viewers], berta) T [19] Exit: inherits(objectid('b94c1f40-818b-47be-882b-ce1a8fbee254'), [owners, editors, viewers], berta) T [10] Exit: inherits(objectid('7f916bd9-f324-40f2-97d3-95e7cc7d722e'), [owners, editors, viewers], berta) true . We can see that the traversal went all the way up to the top data directory, where Berta’s permissions are attached. What about Alan? Alan is a viewer of pdfs. So if we wanted to find out if Alan can view a file inside of that directory, given that his permissions are attached directly to the pdfs directory, we should not see a full traversal to the root. Is that the case?\n1 2 3 4 ?- inherits(objectid('7f916bd9-f324-40f2-97d3-95e7cc7d722e'), [viewers], alan). T [10] Call: inherits(objectid('7f916bd9-f324-40f2-97d3-95e7cc7d722e'), [viewers], alan) T [10] Exit: inherits(objectid('7f916bd9-f324-40f2-97d3-95e7cc7d722e'), [viewers], alan) true . Indeed, the traversal finished early. Of course, the side effect of how traversal is implemented is that if we want to know if Alan is an owner of a file in the pdfs directory, we have to traverse all the way up to reach false.\n1 2 3 4 5 6 7 8 ?- inherits(objectid('7f916bd9-f324-40f2-97d3-95e7cc7d722e'), [owners], alan). T [10] Call: inherits(objectid('7f916bd9-f324-40f2-97d3-95e7cc7d722e'), [owners], alan) T [19] Call: inherits(objectid('b94c1f40-818b-47be-882b-ce1a8fbee254'), [owners], alan) T [28] Call: inherits(objectid('851a7566-b83d-4993-bd04-9b8244091d45'), [owners], alan) T [28] Fail: inherits(objectid('851a7566-b83d-4993-bd04-9b8244091d45'), [owners], alan) T [19] Fail: inherits(objectid('b94c1f40-818b-47be-882b-ce1a8fbee254'), [owners], alan) T [10] Fail: inherits(objectid('7f916bd9-f324-40f2-97d3-95e7cc7d722e'), [owners], alan) false. This could become a bottleneck if there are many, many nested directories but for now, I’m okay with that.\nWe can disable tracing now:\n1 2 3 ?- trace(inherits/3, -all). % inherits/3: Not tracing true. tracing: the curious case of the predicate order An interesting trivia regarding the inherits/3 predicate. Semantically, these two variants solve the same problem:\nVariant 1: 1 2 3 4 5 6 inherits(objectid(ObjectId), Roles, User) :- optional_object_permissions(ObjectId, Roles, Permissions), in_list(User, Permissions). inherits(objectid(ObjectId), Roles, User) :- directory(_, objectid(ObjectId), parent(objectid(ParentObjectId))), inherits(objectid(ParentObjectId), Roles, User). Variant 2: 1 2 3 4 5 6 7 8 inherits(objectid(ObjectId), Roles, User) :- optional_object_permissions(ObjectId, Roles, Permissions), in_list(User, Permissions). inherits(objectid(ObjectId), Roles, User) :- directory(_, objectid(ObjectId), parent(objectid(ParentObjectId))), optional_object_permissions(ObjectId, Roles, Permissions), \\+ in_list(User, Permissions), inherits(objectid(ParentObjectId), Roles, User). Variant 2 is more explicit. However, Variant 2 is also very inefficient:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 ?- inherits(objectid('7f916bd9-f324-40f2-97d3-95e7cc7d722e'), [editors, owners], jake). T [11] Call: inherits(objectid('7f916bd9-f324-40f2-97d3-95e7cc7d722e'), [editors, owners], jake) T [20] Call: inherits(objectid('b94c1f40-818b-47be-882b-ce1a8fbee254'), [editors, owners], jake) T [29] Call: inherits(objectid('851a7566-b83d-4993-bd04-9b8244091d45'), [editors, owners], jake) T [29] Fail: inherits(objectid('851a7566-b83d-4993-bd04-9b8244091d45'), [editors, owners], jake) T [29] Call: inherits(objectid('851a7566-b83d-4993-bd04-9b8244091d45'), [editors, owners], jake) T [29] Fail: inherits(objectid('851a7566-b83d-4993-bd04-9b8244091d45'), [editors, owners], jake) T [20] Fail: inherits(objectid('b94c1f40-818b-47be-882b-ce1a8fbee254'), [editors, owners], jake) T [20] Call: inherits(objectid('b94c1f40-818b-47be-882b-ce1a8fbee254'), [editors, owners], jake) T [29] Call: inherits(objectid('851a7566-b83d-4993-bd04-9b8244091d45'), [editors, owners], jake) T [29] Fail: inherits(objectid('851a7566-b83d-4993-bd04-9b8244091d45'), [editors, owners], jake) T [29] Call: inherits(objectid('851a7566-b83d-4993-bd04-9b8244091d45'), [editors, owners], jake) T [29] Fail: inherits(objectid('851a7566-b83d-4993-bd04-9b8244091d45'), [editors, owners], jake) T [20] Fail: inherits(objectid('b94c1f40-818b-47be-882b-ce1a8fbee254'), [editors, owners], jake) T [20] Call: inherits(objectid('b94c1f40-818b-47be-882b-ce1a8fbee254'), [editors, owners], jake) T [29] Call: inherits(objectid('851a7566-b83d-4993-bd04-9b8244091d45'), [editors, owners], jake) T [29] Fail: inherits(objectid('851a7566-b83d-4993-bd04-9b8244091d45'), [editors, owners], jake) T [29] Call: inherits(objectid('851a7566-b83d-4993-bd04-9b8244091d45'), [editors, owners], jake) T [29] Fail: inherits(objectid('851a7566-b83d-4993-bd04-9b8244091d45'), [editors, owners], jake) T [20] Fail: inherits(objectid('b94c1f40-818b-47be-882b-ce1a8fbee254'), [editors, owners], jake) T [20] Call: inherits(objectid('b94c1f40-818b-47be-882b-ce1a8fbee254'), [editors, owners], jake) T [29] Call: inherits(objectid('851a7566-b83d-4993-bd04-9b8244091d45'), [editors, owners], jake) T [29] Fail: inherits(objectid('851a7566-b83d-4993-bd04-9b8244091d45'), [editors, owners], jake) T [29] Call: inherits(objectid('851a7566-b83d-4993-bd04-9b8244091d45'), [editors, owners], jake) T [29] Fail: inherits(objectid('851a7566-b83d-4993-bd04-9b8244091d45'), [editors, owners], jake) T [20] Fail: inherits(objectid('b94c1f40-818b-47be-882b-ce1a8fbee254'), [editors, owners], jake) T [20] Call: inherits(objectid('b94c1f40-818b-47be-882b-ce1a8fbee254'), [editors, owners], jake) T [29] Call: inherits(objectid('851a7566-b83d-4993-bd04-9b8244091d45'), [editors, owners], jake) T [29] Fail: inherits(objectid('851a7566-b83d-4993-bd04-9b8244091d45'), [editors, owners], jake) T [29] Call: inherits(objectid('851a7566-b83d-4993-bd04-9b8244091d45'), [editors, owners], jake) T [29] Fail: inherits(objectid('851a7566-b83d-4993-bd04-9b8244091d45'), [editors, owners], jake) T [20] Fail: inherits(objectid('b94c1f40-818b-47be-882b-ce1a8fbee254'), [editors, owners], jake) T [20] Call: inherits(objectid('b94c1f40-818b-47be-882b-ce1a8fbee254'), [editors, owners], jake) T [29] Call: inherits(objectid('851a7566-b83d-4993-bd04-9b8244091d45'), [editors, owners], jake) T [29] Fail: inherits(objectid('851a7566-b83d-4993-bd04-9b8244091d45'), [editors, owners], jake) T [29] Call: inherits(objectid('851a7566-b83d-4993-bd04-9b8244091d45'), [editors, owners], jake) T [29] Fail: inherits(objectid('851a7566-b83d-4993-bd04-9b8244091d45'), [editors, owners], jake) T [20] Fail: inherits(objectid('b94c1f40-818b-47be-882b-ce1a8fbee254'), [editors, owners], jake) T [20] Call: inherits(objectid('b94c1f40-818b-47be-882b-ce1a8fbee254'), [editors, owners], jake) T [29] Call: inherits(objectid('851a7566-b83d-4993-bd04-9b8244091d45'), [editors, owners], jake) T [29] Fail: inherits(objectid('851a7566-b83d-4993-bd04-9b8244091d45'), [editors, owners], jake) T [29] Call: inherits(objectid('851a7566-b83d-4993-bd04-9b8244091d45'), [editors, owners], jake) T [29] Fail: inherits(objectid('851a7566-b83d-4993-bd04-9b8244091d45'), [editors, owners], jake) T [20] Fail: inherits(objectid('b94c1f40-818b-47be-882b-ce1a8fbee254'), [editors, owners], jake) T [20] Call: inherits(objectid('b94c1f40-818b-47be-882b-ce1a8fbee254'), [editors, owners], jake) T [29] Call: inherits(objectid('851a7566-b83d-4993-bd04-9b8244091d45'), [editors, owners], jake) T [29] Fail: inherits(objectid('851a7566-b83d-4993-bd04-9b8244091d45'), [editors, owners], jake) T [29] Call: inherits(objectid('851a7566-b83d-4993-bd04-9b8244091d45'), [editors, owners], jake) T [29] Fail: inherits(objectid('851a7566-b83d-4993-bd04-9b8244091d45'), [editors, owners], jake) T [20] Fail: inherits(objectid('b94c1f40-818b-47be-882b-ce1a8fbee254'), [editors, owners], jake) T [11] Fail: inherits(objectid('7f916bd9-f324-40f2-97d3-95e7cc7d722e'), [editors, owners], jake) false. Variant 1 is a much more efficient:\n1 2 3 4 5 6 7 8 ?- inherits(objectid('7f916bd9-f324-40f2-97d3-95e7cc7d722e'), [editors, owners], jake). T [11] Call: inherits(objectid('7f916bd9-f324-40f2-97d3-95e7cc7d722e'), [editors, owners], jake) T [20] Call: inherits(objectid('b94c1f40-818b-47be-882b-ce1a8fbee254'), [editors, owners], jake) T [29] Call: inherits(objectid('851a7566-b83d-4993-bd04-9b8244091d45'), [editors, owners], jake) T [29] Fail: inherits(objectid('851a7566-b83d-4993-bd04-9b8244091d45'), [editors, owners], jake) T [20] Fail: inherits(objectid('b94c1f40-818b-47be-882b-ce1a8fbee254'), [editors, owners], jake) T [11] Fail: inherits(objectid('7f916bd9-f324-40f2-97d3-95e7cc7d722e'), [editors, owners], jake) false. While I wasn’t able to understand why, I will come back to this problem in the future. This is a good opportunity to learn how to debug Prolog programs.\nquerying file permissions directly In the previous step, we had no ability to query for file permissions directly. Instead of a file ID, we had to use the ID of its parent directory. Let’s define an API to query for file permissions directly. There are two cases to consider:\nPermissions can be attached directly to a file. This is the case for Jake who can view a-file.pdf and nothing else. If there are no permissions attached directly to a file, resort to the parent traversal technique. Here’s the code:\nThere’s a lot of duplication here. This API could be generalized, but for now, it will be sufficient. There are three different APIs depending on the access level we intend to query for.\nLet’s look at one of them:\n1 2 3 4 5 6 7 % Viewer query API: is_doc_viewer(objectid(X), User) :- object_permissions(X, [viewers, editors, owners], Permitted), in_list(User, Permitted). is_doc_viewer(objectid(X), User) :- document(_, objectid(X), parent(ParentId)), inherits(ParentId, [viewers, editors, owners], User). Remembering that the predicate order is important, querying for a document ID will first check if there are permissions defined for the file object ID. This can be considered an O(1) operation, pretty cheap in terms of computing so safe to start with. If permissions have been found, we check if the user has been found in the complete list for that object.\nOtherwise, we revert to the inheritance check. The fallback happens in case of:\nThere are no permissions attached directly to the file object. Permissions were found but used hasn’t been granted the requested role directly on the file object. I like this example because it shows how efficient Prolog code can be. It translates to some thing like:\nfunction run_inheritance_check(objectid, roles, user) parentid = get_document(objectid) return inherits(parentid, roles, user) end function if exist, permitted = object_permissions(objectid, roles); exist if in_list(user, permitted) return true // else return run_inheritance_check(objectid, roles, user) else return run_inheritance_check(objectid, roles, user) end if Note: It reminds me very much of Erlang. The first version of Erlang was implemented in Prolog and the influence is clearly visible to this day.\nLet’s load this up in swipl.\nFirst, download the API program from GitHub:\n1 2 cd /tmp/zanzibar wget -O 05-api.pl \"${base_url}/05-api.pl\" If your previous swipl session is still running, simply load the API program:\n1 2 ?- [\"05-api.pl\"]. true. Let’s talk about Jake.\n1 2 3 4 5 6 7 8 9 10 11 % Can Jake view the file a-file.pdf (object ID bee100a0-30b1-4184-94aa-3d408b75e938)? ?- is_doc_viewer(objectid('bee100a0-30b1-4184-94aa-3d408b75e938'), jake). true . % Can Jake view the sibling another.pdf (object ID ecf47371-fd08-44f1-9c06-7c970d948ae8)? ?- is_doc_viewer(objectid('ecf47371-fd08-44f1-9c06-7c970d948ae8'), jake). false. % Is Jake an editor of the file he can see? ?- is_doc_editor(objectid('bee100a0-30b1-4184-94aa-3d408b75e938'), jake). false. What about Alan?\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 % Can Alan see files in the pdfs directory? ?- is_doc_viewer(objectid('ecf47371-fd08-44f1-9c06-7c970d948ae8'), alan). true . ?- is_doc_viewer(objectid('bee100a0-30b1-4184-94aa-3d408b75e938'), alan). true . % But he should not be allowed to edit: ?- is_doc_editor(objectid('bee100a0-30b1-4184-94aa-3d408b75e938'), alan). false. % Alan doesn't own anything, certainly. ?- is_doc_owner(objectid('bee100a0-30b1-4184-94aa-3d408b75e938'), alan). false. Actually, that’s not true. Alan owns an old Volvo. But he doesn’t own any files so everything works as expected.\nsummary This program is 52 lines of code, half of it is a clumsy, non-general API. The other half contains:\nAlmost general solver for any inheritance problem where an object has exactly one parent. Almost, because inherits/3 directly references directory(X, Y, Z). facts. To generalize this solver for other fact types, one has to either: find a way to provide the type of a fact to query for, or create a general fact container. The second approach would probably be easier to implement. Frankly, the reason why I’m calling the fact a directory and a document is illustrative only. A general solver for any number of roles. Roles in permissions are represented as a list of [role₁, [identity₁, ...], ... roleₙ, [identityₙ, ...]]. There can be any number of roles in that list. The API is where the knowledge of exact roles is required. The API could be generalized. thoughts on Prolog This section is strongly inspired by the Prolog Wikipedia page3.\nProlog, short for PROgramming in LOGic, is nothing like your regular programming language. It focuses on declarative style logic programming. Programs are expressed as relations represented by facts and rules. The computation is executed by running queries over these relations. The program applies predefined logic and evaluates to true/false (or yes/no, tomato, tomato).\nThat, however, doesn’t mean that Prolog cannot be applied to interesting problems. In fact, Prolog is widely used in artificial intelligence, theorem proving, expert systems, or—its original intended application—natural language processing.\nProlog’s heritage includes the research on theorem provers and other automated deduction systems developed in the 1960s and 1970s. The first Prolog was developed and implemented in Marseille, France, in 1972 by Alain Colmerauer and Philippe Roussel. Their work was based on Robert Kowalski’s procedural implementation of Horn clauses at the University of Edinburgh. The first implementation of Prolog was called Marseille Prolog, the interpreter was implemented in Fortran by Gerard Battani and Henri Meloni. David H. D. Warren took this original interpreter to the University of Edinburgh where he developed an alternative front-end. Edinburgh Prolog syntax was born. This syntax is used by most of the modern Prolog implementations.\nThe language does not have conditional statements, there are no loops, and there’s no state mutation. Prolog provides logical variables, integers, floats, atoms, and compound terms. Working with lists is not uncommon. Individual implementations may provide additional types. For example, SWI-Prolog provides strings, the empty list Nil ([]), list cons-cell ([|]), and dictionaries.\nThe original pure Prolog was restricted to the use of a resolution theorem prover with Horn clauses:\n1 H :- B₁, ..., Bₙ. Which means:\nTo solve H, solve B₁ and … Bₙ.\nLater, Prolog has been extended to include negation as failure. Conditions in a form not(Bₜ) are shown by trying and failing to solve corresponding positive conditions Bₜ.\nAt least a couple of dozens free and commercial implementations of Prolog exist. This Wikipedia page gives a brief summary4. But even that isn’t complete. There exist Prolog implementations for almost every major programming language. For example:\nichiban/prolog5 for Go. mthom/scryer-prolog6 for Rust. These implementations enable scripting capabilities in otherwise compiled, statically typed languages.\nI am very impressed with Prolog, but also kind of disappointed with myself that I haven’t learned it earlier. By the way, a month from now, Prolog will be 50 years old. That’s 10 years older than me.\nBetter late than never.\nZanzibar-style ACLs with OPA Rego ↩︎ ↩︎\nUnification in Prolog ↩︎\nProlog Wikipedia page ↩︎\nComparison of Prolog implementations ↩︎\nichiban/prolog ↩︎\nmthom/scryer-prolog ↩︎\n","description":"","tags":["prolog","datalog","zanzibar","opa"],"title":"Zanzibar-style ACLs with Prolog","uri":"/posts/2022-09-03-zanzibar-style-acls-with-prolog/"},{"content":"","description":"","tags":null,"title":"fortran","uri":"/tags/fortran/"},{"content":"An exercise on its own rather than a conscious future career choice. This is a short sentence describing my recent venture into Fortran. The language has been on my to learn list for many, many years, but Fortran requires a particular type of problem. The first version of Fortran, the first high-level programming language, appeared in 1957. Fortran is still in active development, it has seen several iterations over the decades. Fortran is the oldest, actively maintained programming language in use today.\nFortran has its niche in scientific computing. Weather simulation, oceanographic simulations, aerospace, automotive, high-performance computing, finance. Anywhere where there’s a need to process vast arrays of numbers, most likely in parallel—that’s where you’ll find Fortran code. The main reasons for this:\nArray-oriented operations, for example, multiply every array element by two: [10, 20, 45, 65] * 2. Or, multiply one array over another: [10, 30, 46, 32] * [100, 134, 32, 87]. Multi-dimensional arrays: up to 15 dimensions out of the box with a short syntax to declare them. For example: real(8) :: var_name(10:20:30). Parallel computing: Fortran coarrays are a relatively simple mechanism to distribute work across CPUs, pin work to selected CPUs, and synchronize the computation across CPUs. Potentially thousands of them. However, Fortran isn’t a tool you’d apply in a modern microservice architecture. It lacks any networking capabilities, there are no operations on byte arrays, no sockets, no TCP, numbers overflow, so one has to be rather careful, and so on. It’s possible to use C interoperability to bolt sockets on, but that doesn’t feel like Fortran anymore.\nFortran provides logical values (booleans), integers, reals, complex numbers, and characters (a string). Modern Fortran provides derived types, which can be compared to structures or records in other languages. Those can be used to construct class-like structures with methods operating on an internal state of the type.\nFinding a reasonable problem for Fortran outside its scientific niche requires an open mind. It is definitely a command line application doing some sort of number crunching with a sprinkle of command line output. I was recently diving into the formulas required to calculate German salary social contributions for employers, as in, the total cost of an employee for a given gross salary. This sounded like a relatively applicable problem.\nSo, what does the Fortran ecosystem look like today?\nWhile working on my German salary calculator1, I had the chance to explore the ecosystem and use some cool language features. Starting a new Fortran application is very easy. On macOS, gcc comes with GFortran compiler. The Fortran community built and maintains a tool called FPM, the Fortran Package Manager2. FPM is modeled after Rust’s cargo, and it is written in Fortran itself. FPM allows:\nCreating a new Fortran application. Link and compile an application. This eliminates the need to maintain tedious Makefiles for linking complex programs. Manage application dependencies. Dependencies can be pulled directly from Git remotes, it’s possible to work with tags and branches. Run unit tests. There are some very interesting libraries to help write decent, modern, compact programs. The most notable ones:\nFLAP3: a command line argument parser. stdlib4: an implementation of a standard library for Fortran as the language does not have one. neural-fortran5: a microframework for building parallel neural nets. functional-fortran6: Functional programming for modern Fortran. There are, of course, dozens of algorithm implementations to choose from.\nI used Visual Studio Code to write my simple program. The community doesn’t let you hang here. There are extensions for many editors providing syntax highlighting. I went for an extension called Modern Fortran7. Besides extensions, there is a Fortran language server enabling code suggestions and inline error notifications. I must admit, the ecosystem is very impressive and makes picking up Fortran a fun and straightforward task.\nI have experienced the following firsthand:\nFPM makes it easy to create and manage a program. Handling command line arguments is easy with FLAP. Fortran’s syntax is verbose but easy to read. Modules allow for easy logic encapsulation. Modules make code portable between programs. Derived types are like classes. These can be used to manage an object-local state. It’s possible to have private or public properties, functions, and subroutines. The language is extensible. It’s possible to create custom operators and override existing ones. Fortran provides a mechanism to write generic code. Derived types support inheritance. Programs compile to native binaries. Functions and subroutines can be pure (no side effects). These are verified by the compiler and serve as a hint that they can be executed in parallel. Working with modular code is easy. Fortran supports selective module member usage (like import in other languages). There are other features that I haven’t had a chance to use, but I learned about them:\nPreviously mentioned array-oriented operations. My program does not need to calculate more than one salary breakdown, so I haven’t needed those. Elementary procedures. Such procedures accept a scalar argument but can be applied to an array. Fortran arrays have no limitations of lower index. Virtually every other programming language comes with zero- or one-indexed arrays. Arrays in Fortran can be arbitrarily indexed. For example, the lower index can be -10. Fortran has lbound(array) and ubound(array) functions for fetching lower and upper array bounds. These, of course, support multiple dimensions. By the way, Fortran arrays are one-indexed by default. coarrays: my program doesn’t need parallel, multi-CPU processing, so I haven’t looked at these in detail, but from what I have seen, these can serve as a poor man’s plane for a multi-CPU actor-like processing framework. Teams, events, and collectives: an abstraction over coarrays enabling easier parallel development. Would I use Fortran today for a new application? It depends. If I ever have to work on multi-dimensional data, I will consider pulling it into my toolbox.\nI enjoyed modern Fortran because:\nIt made me grateful for all the modern programming languages. So you don’t like how Go handles errors? You don’t like arrow functions in modern JavaScript? Or maybe you don’t like C# verbosity? Who cares, it’s a tool for the job. Try Fortran, and you will appreciate how good we have it today. But it’s not as bad as I make it sound. Diving into the longest-maintained programming language was a great way to observe how much modern languages are influenced by the concepts developed 60 years ago. I can confidently pick up old Fortran code and understand what’s happening. For sure, Fortran77 or Fortran90 will be even more verbose than what I have written so far, but the general concept is the same. Considering that Fortran has its stronghold in many, many places and is not going anywhere, it’s a skill worth having. As a final note, during my learning, I supported myself with the Modern Fortran book written by Milan Curcic8, and I can’t recommend it enough. The book is easy to follow and comes with fun examples and exercises. If you, like me, would like to dive deeper into Fortran, make sure you get your hands on a copy.\nGerman salary calculator ↩︎\nThe Fortran Package Manager ↩︎\nFortran library: FLAP ↩︎\nFortran library: stdlib ↩︎\nFortran library: neural-fortran ↩︎\nFortran library: functional-fortran ↩︎\nModern Fortran VSC extension ↩︎\nModern Fortran book written by Milan Curcic ↩︎\n","description":"","tags":["fortran"],"title":"I enjoyed modern Fortran","uri":"/posts/2022-08-28-i-enjoyed-modern-fortran/"},{"content":"Six years ago, during my short stint at The Weather Company, I was on the lookout for a programming language that would let me build software with no dependency on the runtime. There were two obvious candidates: Go and Rust. For many reasons, Rust had me a little bit anxious. The borrow checker, which I just could not fully understand back in the day, was the main sticking point. There were others like the concurrency story, which wasn’t very clear, with multiple implementations competing for the crown.\nGo is an opinionated programming language. Some people have a problem with that, and I found that refreshing. It took a few hours to get comfortable with the syntax, but within a day, I had my first program completed. What a breath of fresh air that was.\nThe compiler was fast. The resulting binaries were small compared to your regular JVM-based language build artifacts or Erlang distributions. There was no runtime! Dropping them into a scratch Docker image resulted in very small, portable, quick and easy to deploy programs. One binary per target architecture, and I could cross-compile to different architectures right on my computer. Working with dependencies wasn’t as easy as it is today with modules, but even something like dep was easy enough. I very much like how dependencies come simply by telling the toolchain to import a library from the URL. The ecosystem was very rich. There were so many libraries to choose from and learn from. Some things took longer to learn. Mastering channels and figuring out how to write concurrent code required some practice. I was never bothered with nil. I have worked with nulls for long enough to acknowledge their existence and deal with them. I also don’t have a problem with how error values are handled in Go. They’re explicit, and I like explicit code. Admittedly, sometimes it is a bit tedious having to handle them every second or third line, but that’s a small price to pay for all other benefits.\nThere were some things I was missing. Before diving into Go, I worked a lot with Erlang and Scala so not having pattern matching, higher-kinded types, currying, and sometimes generics, was often limiting.\nRegardless, I have written so much Go in the last few years, and I enjoyed every single line of code.\nGo gets things done.\nrust Last year I worked on the multi-tenant YugabyteDB implementation. That project took me out of my comfort zone in a positive way.\nAs part of that solution, I’ve built a Postgres extension in C, and I also contributed a couple of features to YugabyteDB, which required working with C++. I did not work with those two technologies before at all. But the amazing thing happened: I experienced the exact problem that Rust was claiming to be solving. GCC was barfing at me about variable lifetimes. Like I said, my C and C++ were pretty clumsy. But it gave me a chance to to understand lifetimes first hand.\nAnd recently, Jatin asked me the following question:\nWhat’s your koolaid now? Go? Rust?\nOf course, I answered that I write a lot of Go. However, that question had some interesting side effects.\nRust is very popular. The ecosystem is growing, there is so much interesting code written in it nowadays, and I’ve been pondering the idea of giving it another try anyway. And it finally happened, it was fun!\nThe tooling is really good. I used rustup to install Rust and the toolchain. My editor of choice is Visual Studio Code. Instead of the Rust extension, I decided to use rust-analyzer. This extension provides inline type annotations, code hints, syntax highlighting, go to definition, and more.\nThis time, rather than going through The Rust Book1, I jumped right in with a goal of building a complete program.\nThe best method to evaluate a programming language is to apply it to the same class of problems one is already familiar with. As I have written many command line tools and plenty of networking code with TLS support, I went for a command line TCP echo server and client with TLS support in Rust. That felt challenging enough:\nCommand line with flags and arguments. TLS with custom roots, certificates and private keys. A concurrent TCP server. A TCP client. Certainly, Rust has gotten easier to use than what I used to remember from years ago. Certainly, the right tooling makes it easier to work with. Borrow checker, mutability, and lifetimes: while writing my first program, I haven’t had a need to think much about those. That was the biggest surprise.\nSome additional thoughts:\nBuilding command line applications with clap2 crate is very easy. Rust’s pattern matching is very refreshing. It’s one of those things I’ve been unconsciously missing in Go. Unlike Scala, Rust’s pattern matching is exhaustive. Immutability by default. I enjoyed Erlang a lot. In Erlang, everything is immutable. I like this property because it makes is very easy to follow when state changes in the program. The std::result::Result\u003cT, E\u003e type seems to be Rust’s composability workhorse. The ? operator is magical. Given function’s std::result::Result\u003cT, E\u003e return type, if the code inside of the function results in an Error, the function returns early with an error. Otherwise, the variable (do we even have a word describing an immutable variable?) in the let variable = expression?; takes the value of T and the function execution continues uninterrupted. A fancy try. The code looks as clean as Scala with its map, flat map, flatten, and whatnot, but it handles errors magically. I was expecting that the std::option::Option\u003cT\u003e type would be more common in my code, surprisingly that’s not the case. std::result::Result\u003cT, E\u003e works better. Rust has type aliases. I was caught out by dependency features in Cargo. It’s worth reading this article3 before starting with Rust. Macros will be a different game, but they are very powerful. Inline type hints provided by rust-analyzer can be difficult to work with because they don’t show type names with full namespaces. I often see something returns an Error, but what exactly? std::io::Error? rustls::Error? tokio_rustls::webpki::Error? Cargo is very nice. It was six years ago, no surprise here. There are many ways of doing concurrent code in Rust. I went with Tokio4, and I don’t think I’ll be looking for a different approach. Personally, I find working with Rust’s documentation pretty difficult, the type syntax with lifetime annotations is noisy, but I’m sure it will get better over time. I attribute this to my lack of experience rather than an issue with the documentation itself. Rust documentation comments are executable5. I haven’t explored them yet but they’ll be definitely very useful. but why? Is there anything wrong with Go? No, there’s nothing wrong with it. I don’t see myself leaving Go anytime soon. Go has some very cool interfaces for working with HTTP, TLS, and SSH. Ultimately Go gets things done, indeed.\nI have looked into Rust simply out of curiosity. It’s been a couple of evenings so far, 10 hours in total, maybe? But I enjoyed this brief exposure, and now I’m even more curious. It is good to be able to read Rust, and there are some very interesting developments in the WebAssembly area that I am looking forward to exploring.\nThe Rust Programming Language Book ↩︎\nclap-rs/clap crate on GitHub ↩︎\nCargo [features] explained with examples ↩︎\ntokio-rs/tokio crate on GitHub ↩︎\nRust documentation comments ↩︎\n","description":"","tags":["rust"],"title":"Learning Rust","uri":"/posts/2022-08-20-learning-rust/"},{"content":"","description":"","tags":null,"title":"rust","uri":"/tags/rust/"},{"content":"I’ve been looking into upgrading Istio using canary upgrades. Canary upgrades let me test a new version of Istio by migrating part of the workloads to the new version and observing the impact of the change. If anything goes wrong, I can roll back to the old version. Old and new Istio versions run side-by-side until it’s verified that pods work fine. When everything is okay, I can safely remove the old version. All examples I was looking at are somewhat confusing. I wanted a down-to-the-bottom list of things one has to do to execute a canary upgrade properly.\nHere’s my take on it.\nistioctl manifest generate The official Istio documentation describes a few possible Istio installation methods. These are:\nistioctl install using Helm charts kubectl apply with manifests generated with istio manifest generate deprecated istioctl operator method I prefer the istio manifest generate method because I can feed it with the IstioOperator manifest, so that generated manifests get fused with the IstioOperator resource.\nfirst installation, the right way Okay, say that I have a cluster where Istio was installed like this:\nIstioOperator for Istio 1.13.5: 1 2 3 4 5 6 7 8 9 10 11 cat \u003e /tmp/1-13-5.yaml \u003c\u003cEOF apiVersion: install.istio.io/v1alpha1 kind: IstioOperator metadata: name: 1-13-5 namespace: istio-system spec: profile: \"demo\" revision: 1-13-5 tag: 1.13.5 EOF The revision property is the important one. It uses dashes instead of dots as separators because the revision property doesn’t support dots.\nManifests generated with istioctl manifest generate: 1 istioctl manifest generate --filename /tmp/1-13-5.yaml \u003e /tmp/istio-1-13-5.yaml The trick is to use istioctl version of Istio for which I generate manifests, which isn’t a problem because the relevant istioctl version can be downloaded from Istio GitHub releases1. Wrapping istioctl in a thin Docker image isn’t a problem.\nGenerated manifests contain CRDs and other Istio resources. To install Istio: 1 kubectl apply -f /tmp/istio-1-13-5.yaml default Istio revision The thing with using revisions is that, when not using explicit revisions, Istio assumes that whatever version is installed, is the so-called default revision. The default revision has a specific semantic meaning. After Istio documentation2:\nInjects sidecars for the istio-injection=enabled namespace selector, the sidecar.istio.io/inject=true object selector, and the istio.io/rev=default selectors. Validates Istio resources. Steals the leader lock from non-default revisions and performs singleton mesh responsibilities (such as updating resource statuses). To turn a revision into a default revision, I use the following command:\n1 istioctl tag set default --revision 1-13-5 If I don’t want the default revision, as in, if I prefer always using explicit revisions, instead of using istio-injection=enabled namespace labels, I should use istio.io/rev=1-13-5 label. Don’t use both labels; use istio.io/rev=1-13-5 only. It doesn’t work with both, there will be no sidecar injected when both labels are in use.\nthe cluster: a recap Okay, so here’s the original cluster:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 cat \u003e /tmp/cluster.yaml \u003c\u003cEOF kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 name: istio-cluster nodes: - role: control-plane kubeadmConfigPatches: - | kind: InitConfiguration nodeRegistration: kubeletExtraArgs: node-labels: \"ingress-ready=true\" extraPortMappings: - containerPort: 80 hostPort: 80 protocol: TCP - containerPort: 443 hostPort: 443 protocol: TCP - role: worker - role: worker EOF kind create cluster --config=/tmp/cluster.yaml cat \u003e /tmp/1-13-5.yaml \u003c\u003cEOF apiVersion: install.istio.io/v1alpha1 kind: IstioOperator metadata: name: 1-13-5 namespace: istio-system spec: profile: \"demo\" revision: 1-13-5 tag: 1.13.5 EOF kubectl create ns istio-system # using istioctl 1.13.5: istioctl manifest generate --filename /tmp/1-13-5.yaml \u003e /tmp/istio-1-13-5.yaml kubectl apply -f /tmp/istio-1-13-5.yaml kubectl wait --for=condition=available \\ -n istio-system \\ --timeout=\"180s\" \\ deployment/istiod-1-13-5 run some workloads 1 2 3 kubectl create ns bookinfo kubectl label ns bookinfo istio.io/rev=1-13-5 kubectl apply -n bookinfo -f https://raw.githubusercontent.com/istio/istio/1.13.5/samples/bookinfo/platform/kube/bookinfo.yaml Istio canary upgrade The fun part, it’s very easy:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 cat \u003e /tmp/1-14-1.yaml \u003c\u003cEOF apiVersion: install.istio.io/v1alpha1 kind: IstioOperator metadata: name: 1-14-1 namespace: istio-system spec: profile: \"demo\" revision: 1-14-1 tag: 1.14.1 EOF # using istioctl 1.14.1: istioctl manifest generate --filename /tmp/1-14-1.yaml \u003e /tmp/istio-1-14-1.yaml kubectl apply -f /tmp/istio-1-14-1.yaml kubectl wait --for=condition=available \\ -n istio-system \\ --timeout=\"180s\" \\ deployment/istiod-1-14-1 At this point I have istiod-1-13-5 and istiod-1-14-1 running.\n1 kubectl get deployments -n istio-system NAME READY UP-TO-DATE AVAILABLE AGE istio-egressgateway 1/1 1 1 2m11s istio-ingressgateway 1/1 1 1 2m11s istiod-1-13-5 1/1 1 1 2m11s istiod-1-14-1 1/1 1 1 75s Move the bookinfo workload to the new version:\n1 2 3 4 5 6 7 8 9 10 11 12 kubectl label ns bookinfo istio.io/rev=1-14-1 --overwrite kubectl rollout restart deployment -n bookinfo details-v1 sleep 20 kubectl rollout restart deployment -n bookinfo ratings-v1 sleep 20 kubectl rollout restart deployment -n bookinfo productpage-v1 sleep 20 kubectl rollout restart deployment -n bookinfo reviews-v1 sleep 20 kubectl rollout restart deployment -n bookinfo reviews-v2 sleep 20 kubectl rollout restart deployment -n bookinfo reviews-v3 And that’s it. Job done. If everything works, I can uninstall Istio 1.13.5 from the cluster. If things don’t work, I can move the workload back to the previous version.\nFinally, I can set the default version to the new one:\n1 istioctl tag set default --revision 1-14-1 so that I can use istio-injection=enabled namespace label, or the sidecar.istio.io/inject=true selector.\nwhat happens when tag set default is executed When a tag is created with istioctl tag set default --revision command, istioctl creates a MutatingWebhookConfiguration in the istio-system namespace. Without the default, we have:\n1 kubectl get mutatingwebhookconfiguration -n istio-system NAME WEBHOOKS AGE istio-sidecar-injector-1-13-5 2 94s istio-sidecar-injector-1-14-1 2 38s With the default tag, we have:\n1 kubectl get mutatingwebhookconfiguration -n istio-system NAME WEBHOOKS AGE istio-revision-tag-default 4 2m9s istio-sidecar-injector-1-13-5 2 6m1s istio-sidecar-injector-1-14-1 2 5m5s The tag is set from this place3 in Istio code, and generated here4.\nnotes on CDRs Istio CRDs are, at minimum, one version backward compatible. They are supposed to work fine when upgrading from 1.x to 1.x+1. When upgrading across multiple versions, the documentation suggests doing it version-by-version. For example, upgrading from 1.11.x to 1.14.x should be done as:\n1.11.x -\u003e 1.12.x -\u003e 1.13.x -\u003e 1.14.x Skipping versions can be done but is not recommended.\nIstio GitHub releases ↩︎\nIstio Canary Upgrades documentation ↩︎\nistioctl 1.14.1: setTag ↩︎\nistioctl 1.14.1: tag.Generate ↩︎\n","description":"","tags":["istio","kubernetes"],"title":"Istio canary upgrades","uri":"/posts/2022-06-19-istio-canary-upgrades/"},{"content":"","description":"","tags":null,"title":"Categories","uri":"/categories/"},{"content":"","description":"","tags":null,"title":"grpc","uri":"/tags/grpc/"},{"content":"","description":"","tags":null,"title":"iam","uri":"/tags/iam/"},{"content":"","description":"","tags":null,"title":"keycloak","uri":"/tags/keycloak/"},{"content":"","description":"","tags":null,"title":"Keycloak","uri":"/categories/keycloak/"},{"content":"","description":"","tags":null,"title":"keycloak18","uri":"/tags/keycloak18/"},{"content":"","description":"","tags":null,"title":"streaming","uri":"/tags/streaming/"},{"content":"Streaming data is a commodity. Thanks to all sorts of streaming data sources we can build reactive systems whereby an event occuring in one corner of the system triggers events somewhere else. Streaming data speeds up processes because businesses can react to events instead of proactively having to ask for “what’s new”. IAM is no exception. Actually, an IAM system should be a source of critical information shared with other systems because IAM events are core to everything else happening within a modern, data-driven organization.\nWhy? Here are some reasons:\nA sign-up triggers a welcome email and a follow-up email campaign. A customer modifying their profile propagates new data to a CRM or an HR system. A large number of login attempts triggers alerts or flag an account with suspicious activity. A membership change reconfigures external systems, for example, database permissions or Kafka ACLs. It’s a general auditing mechanism. Data from other systems can be actively fused directly back to IAM in response to selected events. You can surely come up with other reasons within your business domain.\nVarious IAM solutions offer different notification methods, webhooks are the most common. Systems like Auth0 or Ory offer webhook extension points. An action, for example, a sign in triggers a preconfigured URI with some payload. When triggered, the endpoint can further pass the received information, and sometimes modify it.\nThe problem with this approach is that we are often faced with a biased choice of events we can subscribe to. This choice might be for several less and more nefarious reasons. Maybe the original architect of the IAM solution didn’t think of a particular use case? Or specific events would enable users to implement what the provider offers in a higher-paid plan. Sometimes we are lucky and can contribute additional notifications, but there’s often friction. Open-source is excellent, but many maintainers suffer feature creep and hesitate to take in one-off features serving a limited number of users because every additional feature comes with a maintenance burden.\nAn event bus is a better solution. All events flow through the event bus, there is a universal way to subscribe to the event bus. Events contain raw information, they can be observed and reacted on. Many modern infrastructure components come with an event bus. Docker and Kubernetes come to mind. Surprisingly, Kafka doesn’t have one.\nKeycloak Event Listener SPI Keycloak has one, too, and it comes in the form of an Event Listener SPI. An implementation of an Event Listener SPI plugs in directly into the core of Keycloak and receives all events happening inside of Keycloak deployment. Keycloak comes with a default implementation - its logging mechanism. There are one hundred and four events in Keycloak 18.0.1 we can act on. That’s pretty significant. There are some very useful event types defined in there. User-related, for example:\nLogin, logout. OAuth client login and logout. Account linking. Consent grant, update, and rejection. Various account information changes. But also internal Keycloak changes, the most significant one being CLIENT_UPDATE because it carries all information modified in OAuth clients, even event authorization services!\nHave a look yourself1.\nbut I said streaming Let’s face it. Tailing and parsing logs isn’t the most convenient way of working with streaming data. The best would be to have a method to consume structured data. Like JSON. Or protocol buffers. Well, the good news is that Event Listener is a Keycloak SPI. We can write one. More than 2.5 years ago, I wrote one and put it somewhere on GitHub, then forgot about it. I rediscovered it recently while cleaning up old GitHub repositories. I have updated it for Keycloak 18 and made it available on GitHub.\nWhat is it?\nKeycloak protobuf SPI2 streams protobuf Keycloak events in real-time to a gRPC server. Keycloak protobuf event server3 is a reference implementation of the gRPC server. Keycloak protobuf SPI compose4 contains a reference Docker Compose environment with instructions required to run Keycloak behind an Envoy proxy with the event listener SPI, all TLS-enabled. This SPI shows how to turn Keycloak into a streaming IAM source. I have chosen gRPC, I can’t remember the exact reason, but nothing is preventing us from streaming those events directly to Apache Kafka or any other system.\nhow: try it out If you have Docker and Docker Compose available, you can try it now, and it shouldn’t take more than 10 minutes of your time. Simply follow the instructions from the Keycloak protobuf SPI compose4 repository readme.\nThe Docker Compose configuration in that example builds on my previous articles showing how to run local Keycloak with TLS behind an Envoy proxy, so there is a little friction related to setting up TLS. The following sections of one of the previous articles about Keycloak contain relevant details:\nKeycloak 17.0.0 with TLS in Docker compose behind Envoy proxy: envoy configuration5 Keycloak 17.0.0 with TLS in Docker compose behind Envoy proxy:certificates5 Once running and configured, you can see what’s going on by looking at the logs of the gRPC server:\n1 docker logs -f $(docker ps | grep keycloak-protobuf-event-server | awk '{print $1}') Open the browser and go to https://idp-dev.gruchalski.com, click on the Administration Console link to navigate to Keycloak login screen. The username is admin, password is admin. In the left menu, click on Events.\nNext, click on the Events Config tab and focus the Event Listeners field, you will see a drop-down with the new SPI listed:\nSelect the new SPI, switch all possible options to On, and click Save.\nLook at the log of the event server:\n2022-06-18T11:00:33.359Z [INFO] keycloak-protobuf-event-server: OnAdminEvent: admin-event=\"time:1655550032713 realmId:{value:\\\"a3149aad-f5bf-4213-b771-004c95322ce9\\\"} authDetails:{realmId:{value:\\\"a3149aad-f5bf-4213-b771-004c95322ce9\\\"} clientId:{value:\\\"1d0bdadd-b82b-49dd-9193-f18a2e9e461c\\\"} userId:{value:\\\"cff2f608-293b-4cac-b640-fb399009274c\\\"} ipAddress:{value:\\\"172.18.0.4\\\"}} resourceType:{value:\\\"REALM\\\"} operationType:UPDATE resourcePath:{value:\\\"events/config\\\"} representation:{value:\\\"{\\\\\\\"eventsEnabled\\\\\\\":true,\\\\\\\"eventsListeners\\\\\\\":[\\\\\\\"jboss-logging\\\\\\\",\\\\\\\"keycloak-protobuf-spi-event-listener\\\\\\\"],\\\\\\\"enabledEventTypes\\\\\\\":[\\\\\\\"LOGIN\\\\\\\",\\\\\\\"LOGIN_ERROR\\\\\\\",\\\\\\\"REGISTER\\\\\\\",\\\\\\\"REGISTER_ERROR\\\\\\\",\\\\\\\"LOGOUT\\\\\\\",\\\\\\\"LOGOUT_ERROR\\\\\\\",\\\\\\\"CODE_TO_TOKEN\\\\\\\",\\\\\\\"CODE_TO_TOKEN_ERROR\\\\\\\",\\\\\\\"CLIENT_LOGIN\\\\\\\",\\\\\\\"CLIENT_LOGIN_ERROR\\\\\\\",\\\\\\\"FEDERATED_IDENTITY_LINK\\\\\\\",\\\\\\\"FEDERATED_IDENTITY_LINK_ERROR\\\\\\\",\\\\\\\"REMOVE_FEDERATED_IDENTITY\\\\\\\",\\\\\\\"REMOVE_FEDERATED_IDENTITY_ERROR\\\\\\\",\\\\\\\"UPDATE_EMAIL\\\\\\\",\\\\\\\"UPDATE_EMAIL_ERROR\\\\\\\",\\\\\\\"UPDATE_PROFILE\\\\\\\",\\\\\\\"UPDATE_PROFILE_ERROR\\\\\\\",\\\\\\\"UPDATE_PASSWORD\\\\\\\",\\\\\\\"UPDATE_PASSWORD_ERROR\\\\\\\",\\\\\\\"UPDATE_TOTP\\\\\\\",\\\\\\\"UPDATE_TOTP_ERROR\\\\\\\",\\\\\\\"VERIFY_EMAIL\\\\\\\",\\\\\\\"VERIFY_EMAIL_ERROR\\\\\\\",\\\\\\\"VERIFY_PROFILE\\\\\\\",\\\\\\\"VERIFY_PROFILE_ERROR\\\\\\\",\\\\\\\"REMOVE_TOTP\\\\\\\",\\\\\\\"REMOVE_TOTP_ERROR\\\\\\\",\\\\\\\"GRANT_CONSENT\\\\\\\",\\\\\\\"GRANT_CONSENT_ERROR\\\\\\\",\\\\\\\"UPDATE_CONSENT\\\\\\\",\\\\\\\"UPDATE_CONSENT_ERROR\\\\\\\",\\\\\\\"REVOKE_GRANT\\\\\\\",\\\\\\\"REVOKE_GRANT_ERROR\\\\\\\",\\\\\\\"SEND_VERIFY_EMAIL\\\\\\\",\\\\\\\"SEND_VERIFY_EMAIL_ERROR\\\\\\\",\\\\\\\"SEND_RESET_PASSWORD\\\\\\\",\\\\\\\"SEND_RESET_PASSWORD_ERROR\\\\\\\",\\\\\\\"SEND_IDENTITY_PROVIDER_LINK\\\\\\\",\\\\\\\"SEND_IDENTITY_PROVIDER_LINK_ERROR\\\\\\\",\\\\\\\"RESET_PASSWORD\\\\\\\",\\\\\\\"RESET_PASSWORD_ERROR\\\\\\\",\\\\\\\"RESTART_AUTHENTICATION\\\\\\\",\\\\\\\"RESTART_AUTHENTICATION_ERROR\\\\\\\",\\\\\\\"IDENTITY_PROVIDER_LINK_ACCOUNT\\\\\\\",\\\\\\\"IDENTITY_PROVIDER_LINK_ACCOUNT_ERROR\\\\\\\",\\\\\\\"IDENTITY_PROVIDER_FIRST_LOGIN\\\\\\\",\\\\\\\"IDENTITY_PROVIDER_FIRST_LOGIN_ERROR\\\\\\\",\\\\\\\"IDENTITY_PROVIDER_POST_LOGIN\\\\\\\",\\\\\\\"IDENTITY_PROVIDER_POST_LOGIN_ERROR\\\\\\\",\\\\\\\"IMPERSONATE\\\\\\\",\\\\\\\"IMPERSONATE_ERROR\\\\\\\",\\\\\\\"CUSTOM_REQUIRED_ACTION\\\\\\\",\\\\\\\"CUSTOM_REQUIRED_ACTION_ERROR\\\\\\\",\\\\\\\"EXECUTE_ACTIONS\\\\\\\",\\\\\\\"EXECUTE_ACTIONS_ERROR\\\\\\\",\\\\\\\"EXECUTE_ACTION_TOKEN\\\\\\\",\\\\\\\"EXECUTE_ACTION_TOKEN_ERROR\\\\\\\",\\\\\\\"CLIENT_REGISTER\\\\\\\",\\\\\\\"CLIENT_REGISTER_ERROR\\\\\\\",\\\\\\\"CLIENT_UPDATE\\\\\\\",\\\\\\\"CLIENT_UPDATE_ERROR\\\\\\\",\\\\\\\"CLIENT_DELETE\\\\\\\",\\\\\\\"CLIENT_DELETE_ERROR\\\\\\\",\\\\\\\"CLIENT_INITIATED_ACCOUNT_LINKING\\\\\\\",\\\\\\\"CLIENT_INITIATED_ACCOUNT_LINKING_ERROR\\\\\\\",\\\\\\\"TOKEN_EXCHANGE\\\\\\\",\\\\\\\"TOKEN_EXCHANGE_ERROR\\\\\\\",\\\\\\\"OAUTH2_DEVICE_AUTH\\\\\\\",\\\\\\\"OAUTH2_DEVICE_AUTH_ERROR\\\\\\\",\\\\\\\"OAUTH2_DEVICE_VERIFY_USER_CODE\\\\\\\",\\\\\\\"OAUTH2_DEVICE_VERIFY_USER_CODE_ERROR\\\\\\\",\\\\\\\"OAUTH2_DEVICE_CODE_TO_TOKEN\\\\\\\",\\\\\\\"OAUTH2_DEVICE_CODE_TO_TOKEN_ERROR\\\\\\\",\\\\\\\"AUTHREQID_TO_TOKEN\\\\\\\",\\\\\\\"AUTHREQID_TO_TOKEN_ERROR\\\\\\\",\\\\\\\"PERMISSION_TOKEN\\\\\\\",\\\\\\\"DELETE_ACCOUNT\\\\\\\",\\\\\\\"DELETE_ACCOUNT_ERROR\\\\\\\"],\\\\\\\"adminEventsEnabled\\\\\\\":true,\\\\\\\"adminEventsDetailsEnabled\\\\\\\":true}\\\"} error:{noValue:{}}\" This is a string-serialized protobuf object printed to the log. What can we infer from this event?\nIt’s an admin event: OnAdminEvent: admin-event=\"time:1655549196131. Modified entity was a realm: resourceType:{value:\\\"REALM\\\"}. We have opted-in for admin events: \\\\\\\"adminEventsEnabled\\\\\\\":true. We hvae opted-in for regular events: \\\\\\\"eventsEnabled\\\\\\\":true. Any action executed in the Master realm will now be logged. For example, log out from the realm and observe the log:\n2022-06-18T11:03:16.875Z [INFO] keycloak-protobuf-event-server: OnEvent: event=\"time:1655550196794 type:LOGOUT realmId:{value:\\\"a3149aad-f5bf-4213-b771-004c95322ce9\\\"} clientId:{noValue:{}} userId:{value:\\\"cff2f608-293b-4cac-b640-fb399009274c\\\"} sessionId:{value:\\\"241a5172-107d-48e0-b5c2-d6b83966648f\\\"} ipAddress:{value:\\\"172.18.0.4\\\"} error:{noValue:{}} details:{key:\\\"redirect_uri\\\" value:\\\"https://idp-dev.gruchalski.com/admin/master/console/#/realms/master/events-settings\\\"}\" Log back in:\n2022-06-18T11:03:34.235Z [INFO] keycloak-protobuf-event-server: OnEvent: event=\"time:1655550214221 realmId:{value:\\\"a3149aad-f5bf-4213-b771-004c95322ce9\\\"} clientId:{value:\\\"security-admin-console\\\"} userId:{value:\\\"cff2f608-293b-4cac-b640-fb399009274c\\\"} sessionId:{value:\\\"5ccb5161-c3b1-4dee-935a-6c67bb8400da\\\"} ipAddress:{value:\\\"172.18.0.4\\\"} error:{noValue:{}} details:{key:\\\"auth_method\\\" value:\\\"openid-connect\\\"} details:{key:\\\"auth_type\\\" value:\\\"code\\\"} details:{key:\\\"code_id\\\" value:\\\"5ccb5161-c3b1-4dee-935a-6c67bb8400da\\\"} details:{key:\\\"consent\\\" value:\\\"no_consent_required\\\"} details:{key:\\\"redirect_uri\\\" value:\\\"https://idp-dev.gruchalski.com/admin/master/console/#/realms/master/events-settings\\\"} details:{key:\\\"username\\\" value:\\\"admin\\\"}\" 2022-06-18T11:03:34.741Z [INFO] keycloak-protobuf-event-server: OnEvent: event=\"time:1655550214737 type:CODE_TO_TOKEN realmId:{value:\\\"a3149aad-f5bf-4213-b771-004c95322ce9\\\"} clientId:{value:\\\"security-admin-console\\\"} userId:{value:\\\"cff2f608-293b-4cac-b640-fb399009274c\\\"} sessionId:{value:\\\"5ccb5161-c3b1-4dee-935a-6c67bb8400da\\\"} ipAddress:{value:\\\"172.18.0.4\\\"} error:{noValue:{}} details:{key:\\\"client_auth_method\\\" value:\\\"client-secret\\\"} details:{key:\\\"code_id\\\" value:\\\"5ccb5161-c3b1-4dee-935a-6c67bb8400da\\\"} details:{key:\\\"grant_type\\\" value:\\\"authorization_code\\\"} details:{key:\\\"refresh_token_id\\\" value:\\\"2a19e586-bf5c-417f-b314-75df22335b33\\\"} details:{key:\\\"refresh_token_type\\\" value:\\\"Refresh\\\"} details:{key:\\\"scope\\\" value:\\\"openid profile email\\\"} details:{key:\\\"token_id\\\" value:\\\"ce75b467-73e3-46b7-b97b-a259780b12ce\\\"}\" And so on.\nOf course, in a real implementation, you’d be streaming those events directly to something like Kafka instead of printing them to the screen, so other apps in your company could react to those.\nEvent listeners are registered on a per-realm basis. To observe events in another realm, you need to repeat the installation steps on every realm for which you want to observe events.\nget in touch Are you integrating Keycloak into your company and need a partner who can help you take it beyond where you thought it was possible? Reach out to me at radek.gruchalski@klarrio.com.\nKeycloak 18.0.1 event types ↩︎\nKeycloak protobuf SPI ↩︎\nKeycloak protobuf event server ↩︎\nKeycloak protobuf SPI compose ↩︎ ↩︎\nKeycloak 17.0.0 with TLS in Docker compose behind Envoy proxy ↩︎ ↩︎\n","description":"reactive IAM with Keycloak event listeners","tags":["keycloak","keycloak18","grpc","streaming","iam"],"title":"Streaming Keycloak events","uri":"/posts/2022-06-18-streaming-keycloak-events/"},{"content":"","description":"","tags":null,"title":"rego","uri":"/tags/rego/"},{"content":"In the previous article on OPA1, I asked this question:\nwhy would Ory Keto drop OPA from its implementation?\nWhat’s Ory Keto? After Ory Keto documentation2:\nOry Keto is the first and only open source implementation of “Zanzibar: Google’s Consistent, Global Authorization System”.\nThe question bothered me to the point that I sat down and reread the Zanzibar white paper3 in search for clues on OPA applicability to this problem. It’s been about a year since I last looked into Zanzibar so I’m coming back to this with a fresh mind.\nGoogle Zanzibar white paper describes a consistent globally distributed authorization system. The paper discusses three facets:\nThe notation: defines a method to describe ACLs and their hierarchy. The API: defines an ACL read, write, check, and expand API. The architecture and design: describes how Google implemented Zanzibar. The ACL notation format:\nnamespace:object#relation@namespace:user Rules should be read from right to left. That rule means: a user is in a relation relation to an object. Namespaces offer a logical grouping of object types.\nThe user part of the ACL can be substituted with a userset:\nallows ACLs to refer to groups and thus supports representing nested group membership\nEssentially, it could be a result of a different rule evaluation. A userset is always an object#relation.\nThe white paper gives two prominent examples. The first one (page 2, table 1):\nMembers of group:eng are viewers of doc:readme.\ndoc:readme#viewer@group:eng#member The second one (page 4, figure 1) is written in pseudo code:\nSimple namespace configuration with concentric relations on documents. All owners are editors, and all editors are viewers. Further, viewers of the parent folder are also viewers of the document.\nname: \"doc\" relation { name: \"owner\" } relation { name: \"editor\" userset_rewrite { union { child { _this {} } child { computed_userset { relation: \"owner\" } } } } } relation { name: \"viewer\" userset_rewrite { union { child { _this {} } child { computed_userset { relation: \"editor\" } } child { tuple_to_userset { tupleset { relation: \"parent\" } computed_userset { object: $TUPLE_USERSET_OBJECT # parent folder relation: \"viewer\" } } } } } } This one is somewhat troublesome because we aren’t offered a notation. Let’s try reverse-engineering it. Each line builds on the previous one to arrive at the final notation:\n# document has owners; # per the white paper: the namespace and the relation are predefined in client configuration; # clearly noted here: doc#owner # document owners are document editors: doc#editor@doc#owner # document editors are document viewers: doc#viewer@doc#editor@doc#owner # the complete ACL notation for the first part, with namespaces for completeness: doc:some-doc#viewer@doc:some-doc#editor@doc:some-doc#owner This isn’t complete yet because the latter statement is still missing:\nFurther, viewers of the parent folder are also viewers of the document.\nThere doesn’t seem to be a way to encode this information continuously within the first rule. The decision tree is:\nis a viewer ├\u003c if is an editor; is editor -\u003c if is an owner └\u003c if is a viewer of the parent folder The Zanzibar notation doesn’t have a union notion. So there have to be two rules defined like this:\ndoc:some-doc#viewer@doc:some-doc#editor@doc:some-doc#owner doc:some-doc#viewer@folder:some-doc-parent-folder#viewer A few interesting things can be inferred from this notation:\nChecks are in the form of narrow set ⊂ wider set ⊂ wider set .... The white paper isn’t explicit about it but checks can be seemingly chained together as long as we’re chaining on the same object and narrowing down within a set of the same the user/userset. I actually like how Keto refers to this s subject. Not sure how ergonomic this really is but interesting to keep in mind. Order doesn’t matter, reversing individual ACLs produces the same end result. doc:some-doc#viewer@doc:some-doc#editor@doc:some-doc#owner doc:some-doc#viewer@folder:some-doc-parent-folder#viewer produces the same union as:\ndoc:some-doc#viewer@folder:some-doc-parent-folder#viewer doc:some-doc#viewer@doc:some-doc#editor@doc:some-doc#owner To find all viewers of the document doc:some-doc, one needs to find (and evaluate) all ACLs starting with doc:some-doc#viewer. To find out what relations can be used in combination with doc:some-doc, one needs to list all ACLs starting with doc:some-doc#. Whatever the decision tree is, it’s possible to find all ACLs contributing to a single union by finding all rules with the required object and relation prefix, then evaluating them. As the Zanzibar white paper suggests, individual rules could be evaluated in parallel. It’s handy to have the ergonomy of using strings as user identifiers as Ory Keto does it. how easy is it to translate zanzibar notation to rego OPA policies are written using Rego:\n… a declarative language … purpose-built for expressing policies over complex hierarchical data structures.\nA handy way to work with policies is to write them in rego files. A policy file starts with a package declaration and includes one or more rules. Policy documents may define variables, import data from data documents, and include other policies so rules can be combined and reused. There’s a great introduction to Rego on the OPA website4.\nIt should be possible to rewrite a Zanzibar ACL as a Rego policy.\na member of the engineering group is a document viewer The first example from the white paper would be:\nFeeding it with:\n1 2 3 4 { \"doc\": \"readme\", \"groups\": [\"employee\", \"eng\"] } produces:\n1 2 3 { \"doc_viewer\": true } An example of an invalid input:\n1 2 3 4 { \"doc\": \"readme\", \"groups\": [\"hr\"] } returns:\n1 2 3 { \"doc_viewer\": false } This policy can be tested with OPA Playground.\nowner is an editor and a viewer, a viewer of the parent folder is also a document viewer The second example could be translated to the following Rego representation:\nFor this input:\n1 2 3 4 { \"userid\": \"owner\", \"parent_viewers\": [\"viewer\"] } The output is:\n1 2 3 4 5 6 { \"editor\": false, \"owner\": false, \"parent_viewer\": true, \"viewer\": true } Similarly, for:\n1 2 3 4 { \"userid\": \"owner\", \"viewers\": [\"viewer\"] } we get:\n1 2 3 4 5 { \"editor\": false, \"owner\": false, \"viewer\": false } This policy can be also tested with the Playground.\nlet’s take this up a notch I’m going to build on the second policy. For the next step, I’m going to put the policy and some data on the OPA server. The server is running in a container with in-memory storage. Data will be lost on the restart but it doesn’t matter right now.\n1 2 3 docker run --rm \\ -ti openpolicyagent/opa:0.40.0 \\ run --server --log-format=json-pretty --set=decision_logs.console=true I’ve made a couple of significant changes:\nWhen working with the OPA server, policies make decisions based on structured data loaded onto the server. Those data documents provide facts about the external world. Instead of passing facts to the policies via input (like I did with input.viewers, input.editors, and input.owners in the original implementation), I’m using this data storage mechanism. OPA cannot answer queries about data not it doesn’t know about. Storing data on the server also allows importing it into Rego documents. This is an ergonomic way of working with OPA. The previous iteration of this document was written for the playground. There’s more structure in the data model. I’m storing user identifiers as relationship object keys. Here, user-id and another-user-id have a viewer relationship. It’s a key/value storage. 1 2 3 4 5 6 { \"viewers\": { \"user-id\": true, \"another-user-id\": true } } With this, I can use OPA patch support to add, modify, and remove object-to-subject relationships. First, the modified policy document:\nwith relevant test data:\ndocuments: 1 2 3 4 5 6 7 8 9 10 11 12 { \"readme\": { \"properties\": { \"directory\": \"parent-folder-id\" }, \"permissions\": { \"editors\": {}, \"owners\": {}, \"viewers\": {} } } } folders: 1 2 3 4 5 6 7 { \"parent-folder-id\": { \"permissions\": { \"viewers\": {} } } } In real-world those document and folder IDs would be something more cryptic, for example, an UUID. Here, I’m using literal values for better readability. This data model isn’t optimal, it could be improved. But it is good enough to visualize what’s happening so it’s good enough for this article.\nAssuming that the Docker container is already running, I can load the data from the terminal:\n1 2 3 4 5 6 7 8 9 10 11 12 # load the policy: curl --silent -X PUT --data-binary @doc-policy.rego \\ localhost:8181/v1/policies/doc-policy # load the data: curl --silent -X PUT \\ -H 'Content-Type: application/json' \\ --data-binary @data-docs.json \\ localhost:8181/v1/data/docs curl --silent -X PUT \\ -H 'Content-Type: application/json' \\ --data-binary @data-folders.json \\ localhost:8181/v1/data/folders All default data is loaded into OPA. What’s worth paying the attention to:\nThe path the JSON data is loaded into reflects the policy import: /v1/data/docs maps directly to import data.docs, /v1/data/folders maps directly to import data.folders. First, a query to check if a user is a viewer of the readme document:\n1 2 3 curl --silent -X POST localhost:8181/v1/data/doc/viewer \\ -H 'Content-Type: application/json' \\ -d '{\"input\": {\"docid\": \"readme\", \"userid\": \"user-viewer\"}}' | jq '.' 1 2 3 4 { \"decision_id\": \"b5f14892-d92c-4804-8a02-150e3d437da5\", \"result\": false } It’s false, as expected. There are no permissions defined in the data document. I will now give the user-viewer permission to view the document:\n1 2 3 4 5 6 7 8 9 # the path is a combination of the JSON placement and the path # within the JSON data document: curl --silent -X PATCH localhost:8181/v1/data/docs/readme/permissions/viewers \\ -H 'Content-Type: application/json-patch+json' \\ -d '[{ \"op\": \"add\", \"path\": \"user-viewer\", \"value\": true }]' Can the user view the readme document now?\n1 2 3 curl --silent -X POST localhost:8181/v1/data/doc/viewer \\ -H 'Content-Type: application/json' \\ -d '{\"input\": {\"docid\": \"readme\", \"userid\": \"user-viewer\"}}' | jq '.' 1 2 3 4 { \"decision_id\": \"4c73a96a-1f5f-4c08-9339-816ef97b7328\", \"result\": true } They do. This works with a direct viewer permission assignment. One of the ACL statements is that a viewer of a parent folder is a viewer of the document. I can test that by removing the direct permission and giving the user-viewer a viewer permission on the folder:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # remove the direct permission: curl --silent -X PATCH localhost:8181/v1/data/docs/readme/permissions/viewers \\ -H 'Content-Type: application/json-patch+json' \\ -d '[{ \"op\": \"remove\", \"path\": \"user-viewer\" }]' # document contains a property with an ID of its parent folder: export parent_folder_id=$(curl --silent localhost:8181/v1/data/docs/readme/properties/directory | jq '.result' -r) # assign the `viewer` permission to the `user-viewer` on the `parent folder`: curl --silent -X PATCH localhost:8181/v1/data/folders/${parent_folder_id}/permissions/viewers \\ -H 'Content-Type: application/json-patch+json' \\ -d '[{ \"op\": \"add\", \"path\": \"user-viewer\", \"value\": true }]' And verify:\n1 2 3 curl --silent -X POST localhost:8181/v1/data/doc/viewer \\ -H 'Content-Type: application/json' \\ -d '{\"input\": {\"docid\": \"readme\", \"userid\": \"user-viewer\"}}' | jq '.' 1 2 3 4 { \"decision_id\": \"a9ed829a-c66e-4435-8e97-85a644ac879b\", \"result\": true } It indeed works. To be completely sure, let’s remove the permission from the parent folder and repeat the query:\n1 2 3 4 5 6 7 8 9 curl --silent -X PATCH localhost:8181/v1/data/folders/${parent_folder_id}/permissions/viewers \\ -H 'Content-Type: application/json-patch+json' \\ -d '[{ \"op\": \"remove\", \"path\": \"user-viewer\" }]' curl --silent -X POST localhost:8181/v1/data/doc/viewer \\ -H 'Content-Type: application/json' \\ -d '{\"input\": {\"docid\": \"readme\", \"userid\": \"user-viewer\"}}' | jq '.' 1 2 3 4 { \"decision_id\": \"83e2b6e0-ef0f-4d6d-9e2a-5e9cb740c50f\", \"result\": false } Let’s test that the first rule of the policy holds ⤳ an owner:\n1 2 3 4 5 6 7 curl --silent -X PATCH localhost:8181/v1/data/docs/readme/permissions/owners \\ -H 'Content-Type: application/json-patch+json' \\ -d '[{ \"op\": \"add\", \"path\": \"user-owner\", \"value\": true }]' is an editor: 1 2 3 curl --silent -X POST localhost:8181/v1/data/doc/editor \\ -H 'Content-Type: application/json' \\ -d '{\"input\": {\"docid\": \"readme\", \"userid\": \"user-owner\"}}' | jq '.' 1 2 3 4 { \"decision_id\": \"9e95e5dd-1622-496b-8ccd-50bb2ac7fa7b\", \"result\": true } and a viewer: 1 2 3 curl --silent -X POST localhost:8181/v1/data/doc/viewer \\ -H 'Content-Type: application/json' \\ -d '{\"input\": {\"docid\": \"readme\", \"userid\": \"user-owner\"}}' | jq '.' 1 2 3 4 { \"decision_id\": \"b960f9c9-3d2a-4cb1-afbd-53df618a36f0\", \"result\": true } This works surprisingly well.\nobservations A few observations after researching this solution:\nThe Zanzibar namespace translates very nicely to the object type: for example, the doc namespace maps very well to a Rego doc policy. A Rego doc policy is what would be attached to any document. Zanzibar relationship maps to a Rego rule: onwer, editor, viewer, those are Zanzibar relationships and Rego rules. Object identifier and user identifier are variables: doc:document-id#relation@user:user-id says that a user with a user-id is in a relationship with a document with an id of document-id. This ACL will hold for any document and any user. Hence document-id and user-id are variable. caveats I haven’t answered the question from the beginning of this write-up. It’s a little bit too early for that. I am able to translate Zanzibar ACLs to Rego but a Zanzibar-like ACL solution does much more than an ACL evaluation. There are the following aspects of Zanzibar still to evaluate:\nAPI to read relationships for an object. API to read all users for an object and relationship. API to read all users for an object regardless of the relationship. Expand API. A viable, somewhat scalable architecture for storing individual ACLs. A method to reference them in a robust way when querying usersets. Nevertheless, it’s a good start.\nOPA: logical or conditions ↩︎\nOry Keto documentation ↩︎\nZanzibar: Google’s Consistent, Global Authorization System ↩︎\nPolicy Language ↩︎\n","description":"what does it take to convert Zanzibar ACLs to OPA Rego?","tags":["opa","rego","zanzibar"],"title":"Zanzibar-style ACLs with OPA Rego","uri":"/posts/2022-05-07-zanzibar-style-acls-with-opa-rego/"},{"content":"","description":"","tags":null,"title":"git","uri":"/tags/git/"},{"content":"","description":"","tags":null,"title":"go","uri":"/tags/go/"},{"content":"","description":"","tags":null,"title":"golang","uri":"/tags/golang/"},{"content":"If you’re working with go on a regular basis, chances are you have come across the problem of working with private modules. Let’s quickly recap:\nYour organization hosts go modules at github.com/your-org. There’s some private project at github.com/your-org/awesome-stuff. This private project depends on other private modules, you have this in your go.mod: module github.com/your-org/awesome-stuff go 1.17 require ( github.com/your-org/awesome-auth-lib v0.1.3 github.com/your-org/awesome-logging-lib v0.2.6 ) You already have your export GOPRIVATE=github.com/your-org in ~/.bash_profile so that go toolchain doesn’t try downloading private modules from the public go proxy. You have also already found out that for a good measure you need this in your ~/.gitconfig:\n[url \"git@github.com:\"] insteadOf = https://github.com This is what every article about using private go modules explains. And this works. This works for 99% of all cases.\nWhat’s the final percent, you may ask. Let’s assume that your organization has a partner who happens to also use go and has their own private go modules hosted on GitHub. Their code is hosted in the github.com/awesome-partner organization. One day they come to you and ask to contribute to their code. For whatever reason, you decide that you’re going to use a different GitHub identity with a different key pair - it’s a different GitHub account whatsoever.\nSo you follow the instructions for configuring multiple identities. You add the default username and email address to your ~/.gitconfig:\n1 2 3 4 [user] # Please adapt and uncomment the following lines: name = John Doe email = john.doe@your-org.com Which you probably already have anyway. You also configure your SSH so you can use your partner’s repositories:\n1 2 3 4 Host awesome-partner HostName github.com User git IdentityFile ~/.ssh/github-partner And updated repository local configuration: origin, username and email address:\n1 2 3 4 5 [remote \"origin\"] url = git@awesome-partner:awesome-partner/cool-stuff.git fetch = +refs/heads/*:refs/remotes/origin/* [user] email = john.doe.partners@your-org.com With this, you can now clone and pull partner’s go project:\n1 2 3 mkdir -p $GOPATH/src/github.com/awesome-partner cd $GOPATH/src/github.com/awesome-partner git clone git@awesome-partner:awesome-partner/cool-stuff.git As it happens, the partner also uses plenty of their own secret go sauce. You find something like this:\nmodule github.com/awesome-partner/cool-stuff go 1.16 require ( github.com/awesome-partner/secret-sauce v1.10.17 # more secret sauce... ) You run go mod tidy and things go wrong. Things go wrong because of the insteadOf rule in the ~/.gitconfig file.\nwhat you really want Instead of:\n[url \"git@github.com:\"] insteadOf = https://github.com do this:\n[url \"git@github.com:your-org\"] insteadOf = https://github.com/your-org [url \"git@awesome-partner:awesome-partner\"] insteadOf = https://github.com/awesome-partner The rule for your organization uses the default github.com hostname so this saves you from having to modify origins for every repository belonging to your organization. This is your bread and butter so you don’t want to do this often. The default rule makes the daily work ergonomic.\nAll other rules exist for those odd cases when you need to work with your partner’s code. This is where you’ll have to update origins in the repository-local .git/config.\nConsider always scoping those rules to the organization.\n","description":"because every tutorial on the internet misses that one percent…","tags":["go","golang","git"],"title":"Private go modules with multiple git identities","uri":"/posts/2022-04-28-private-go-modules-with-multiple-git-identities/"},{"content":"This one is like riding a bicycle. Once you know it, you know it. I’ve been going down some Kubernetes rabbit holes and I’ve landed on OPA - Open Policy Agent.\nThe Open Policy Agent (OPA, pronounced “oh-pa”) is an open source, general-purpose policy engine that unifies policy enforcement across the stack. OPA provides a high-level declarative language that lets you specify policy as code and simple APIs to offload policy decision-making from your software. You can use OPA to enforce policies in microservices, Kubernetes, CI/CD pipelines, API gateways, and more.\nBasically, it’s a declarative decision engine which evaluates Rego policies for a given input.\nRego was inspired by Datalog, which is a well understood, decades old query language. Rego extends Datalog to support structured document models such as JSON.\nRego queries are assertions on data stored in OPA. These queries can be used to define policies that enumerate instances of data that violate the expected state of the system.\nRego comes packed with some very neat features.\nThe OPA project comes with an extensive, easy to follow and brief (good!!) documentation. However, there was one question to which I could not find an answer: how do I express:\n(if this OR this) AND that\nHere’s my semi-realistic problem:\ngiven a go structure with a bunch of string properties if a user reading the structure: belongs to an HR role they can see all values of all fields, regardless if a field is restricted, or not restricted belongs to any other role they can see non-restricted fields only ONLY when the user is enabled For the following structure:\n1 2 3 4 5 6 7 8 9 type Person struct { first_name string last_name string email_address string // restricted address1 string // restricted address2 string // restricted postal_code string // restricted city string } anyone who’s not a member of the HR team should see ******** instead of actual values.\nThere are two known facts while evaluating this policy:\nThe list of restricted fields: email_address, address1, address2, and postal_code. The input, an example goes like this: 1 2 3 4 5 6 7 { \"field\": \"email_address\", \"subject\": { \"enabled\": true, \"groups\": [\"employee\", \"hr\"] } } and:\n1 2 3 4 5 6 7 { \"field\": \"email_address\", \"subject\": { \"enabled\": true, \"groups\": [\"employee\", \"chef\"] } } Okay, let’s start with the policy:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 package play # By default the decision is false: default can_read_restricted_field = false # The list of restricted fields is known: restricted_fields := {\"email_address\", \"address1\", \"address2\", \"postal_code\"} is_restricted_field { # A field is restricted if it is found in the `restricted_fields` set. restricted_fields[input.field] } can_read_restricted_field_when_enabled { # Not complete yet... input.subject.enabled } is_hr { # An employee is a member of the HR team if one # of their groups is `hr`. input.subject.groups[_] = \"hr\" } I want this:\nIf the employee works in HR, there is no need to check if a field is restricted. Otherwise (if not is_hr), the field can be accessed only when it isn’t restricted. The first condition is easy:\n1 2 3 can_read_restricted_field { is_hr } The second one is also easy:\n1 2 3 4 can_read_restricted_field { not is_hr not is_restricted_field } What wasn’t clear to me was how do I make these two conditions into a single logical OR operation? It’s really easy, the full policy looks like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 package play # By default the decision is false: default can_read_restricted_field = false default can_read_restricted_field_when_enabled = false # The list of restricted fields is known: restricted_fields := {\"email_address\", \"address1\", \"address2\", \"postal_code\"} can_read_restricted_field { is_hr } can_read_restricted_field { not is_hr not is_restricted_field } can_read_restricted_field_when_enabled { can_read_restricted_field input.subject.enabled } is_restricted_field { # A field is restricted if it is found in the `restricted_fields` set. restricted_fields[input.field] } is_hr { # An employee is a member of the HR team if one # of their groups is `hr`. input.subject.groups[_] = \"hr\" } It essentially boils down to:\n1 2 3 4 5 6 7 8 9 or_rule {} or_rule {} some_other_rule {} or_and_others { or_rule some_other_rule } This policy returns for can_read_restricted_field_when_enabled:\nfalse for disabled hr and field email_address false for disabled hr and field first_name true for enabled hr, any field false for enabled chef, any field from the restricted list true for enabled chef, any field outside of the restricted list false for disabled chef, any field outside of the restricted list That’s it, pretty cool. Nothing that cannot be done with a bunch of conditionals and loops but on a highly variable input iterating with Rego will be much faster!\nclosing thought Turns out to be completely unrelated:\nRego is inspired by Datalog. Datalog is a a subset of Prolog. Prolog would be a perfect language to implement Google’s Zanzibar in. Why did Keto project drop the OPA route? ","description":"because an and isn’t always the end of it","tags":["opa","rego","zanzibar","ory","keto"],"title":"OPA: logical or conditions","uri":"/posts/2022-04-27-opa-logical-or-conditions/"},{"content":"","description":"","tags":null,"title":"ory","uri":"/tags/ory/"},{"content":"","description":"","tags":null,"title":"cdc","uri":"/tags/cdc/"},{"content":"","description":"","tags":null,"title":"YugabyteDB","uri":"/categories/yugabytedb/"},{"content":"Shortly after I have published the previous article on YugabyteDB CDC1, the amazing Yugabyte team released the 2.13 version of the database with a beta implementation of the new change data capture SDK. Before diving into the new SDK, let’s quickly recap the first implementation.\nThis is a living article. Updates will be listed here.\nfirst CDC version: xcluster With the first implementation, we can create and delete JSON based streams, where each stream is created for exactly one table. Each individual stream delivers messages for three types of database events: inserts (WRITE and APPLY), updates (WRITE and APPLY) and deletes (DELETE and APPLY).\n1 2 3 4 5 6 7 8 // https://github.com/yugabyte/yugabyte-db/blob/v2.11.2/src/yb/cdc/cdc_service.proto#L165 message CDCRecordPB { enum OperationType { WRITE = 1; DELETE = 2; APPLY = 3; } // ... For majority of the use cases, this might be sufficient but certain operations aren’t possible:\nNo truncate support. No DDL related events. No transaction indication. With the xcluster implementation, the CDC service delivers messages on the tablet level. Each individual insert, update, or delete results in multiple messages: one for a tablet leader and one for each follower.\nThis results in plenty of duplication on the consumer side and an operational complexity of tracking the change source of the truth. This first CDC implementation hands over the complexity of the architecture to the consumer. Instead of working just on the row (entity) level, the consumer must also understand the nature of how the cluster is deployed and replicated.\nThe new CDC SDK addresses a large number of those problems.\nCDC SDK Let’s start the CDC SDK investigation by looking at the CDC 2.13 service definition:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 // https://github.com/yugabyte/yugabyte-db/blob/v2.13.0/src/yb/cdc/cdc_service.proto#L47 service CDCService { rpc CreateCDCStream (CreateCDCStreamRequestPB) returns (CreateCDCStreamResponsePB); rpc DeleteCDCStream (DeleteCDCStreamRequestPB) returns (DeleteCDCStreamResponsePB); rpc ListTablets (ListTabletsRequestPB) returns (ListTabletsResponsePB); rpc GetChanges (GetChangesRequestPB) returns (GetChangesResponsePB); rpc GetCheckpoint (GetCheckpointRequestPB) returns (GetCheckpointResponsePB); rpc UpdateCdcReplicatedIndex (UpdateCdcReplicatedIndexRequestPB) returns (UpdateCdcReplicatedIndexResponsePB); rpc BootstrapProducer (BootstrapProducerRequestPB) returns (BootstrapProducerResponsePB); rpc GetLatestEntryOpId (GetLatestEntryOpIdRequestPB) returns (GetLatestEntryOpIdResponsePB); rpc GetCDCDBStreamInfo (GetCDCDBStreamInfoRequestPB) returns (GetCDCDBStreamInfoResponsePB); rpc SetCDCCheckpoint (SetCDCCheckpointRequestPB) returns (SetCDCCheckpointResponsePB) { option (yb.rpc.trivial) = true; }; } This doesn’t look much different from the previous version. There are a couple of new operations and some request and response types contain new SDK-specific fields.\nAn example, there is a single CreateCDCStream method taking the CreateCDCStreamRequestPB argument known from the previous incarnation. However, the request argument contains some new capabilities.\nIt’s obvious that one needs to know the correct set of parameters to work with the SDK. A reference implementation would be handy. As it happens, there is one: the YugabyteDB Debezium connector2.\nThe most relevant pieces of the connector code:\nYugabyteDBConnector.validateTServerConnection(...)3: handles stream creation and setup. YugabyteDBConnector.fetchTabletList(...)4: fetches list of tablet IDs to consume changes for. YugabyteDBStreamingChangeEventSource.getChanges2(...)5: handles CDC SDK rows returned from GetChanges operation. creating a stream To create a CDC SDK stream, called a database stream in 2.13, we have to call the CreateCDCStream operation with an exact combination of CreateCDCStreamRequestPB parameters. Hints are in two places:\nYugabyteDB Debezium connector: calls an underlying Java client with required parameters. Currently undocumented yb-admin create_change_data_stream operation6; this one calls into the ClusterAdminClient::CreateCDCSDKDBStream7 operation. To create a database stream, the request needs to contain:\nA database name (namespace). Record type CDCRecordType.CHANGE: defines what each individual record contains: CHANGE: changed values only; it appears that this is the required value, AFTER: the entire row after the change, ALL: the before and after value of the row. Record format must be CDCRecordFormat.PROTO: data is in the protobuf format. Source type must be CDCRequestSource.CDCSDK, other values: XCLUSTER: simple CDC form. 2DC: two datacenter deployments in YugabyteDB leverage change data capture at the core. Checkpoint type must be IMPLICT or EXPLICIT: when IMPLICIT, CDC SDK automatically manages checkpoints. If the stream has been successfully created, the ID of the stream will be in CreateCDCStreamResponsePB.db_stream_id field:\n1 2 3 4 5 6 // https://github.com/yugabyte/yugabyte-db/blob/v2.13.0/src/yb/cdc/cdc_service.proto#L124 message CreateCDCStreamResponsePB { optional CDCErrorPB error = 1; optional bytes stream_id = 2; optional bytes db_stream_id = 3; } consuming the stream As with the original CDC, to consume a stream, we have to call the GetChanges operation. The GetChangesRequestPB type looks like this in 2.13:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 // https://github.com/yugabyte/yugabyte-db/blob/v2.13.0/src/yb/cdc/cdc_service.proto#L178 message GetChangesRequestPB { optional bytes stream_id = 1; // Tablet to get the changes for. optional bytes tablet_id = 2; // Checkpoint to start reading from (exclusive). // Start reading from the first record after this checkpoint. optional CDCCheckpointPB from_checkpoint = 3; // Maximum records to read. optional uint32 max_records = 4; // Whether the caller knows the tablet address or needs to use us as a proxy. optional bool serve_as_proxy = 5 [default = true]; optional bytes db_stream_id = 6; optional bytes table_id = 7; optional CDCSDKCheckpointPB from_cdc_sdk_checkpoint = 8; } Most interesting fields:\ndb_stream_id: takes the database stream ID returned in CreateCDCStreamResponsePB.db_stream_id; we can see that the original CDC is still supported with the stream_id field. from_cdc_sdk_checkpoint: CDC SDK streams require this new checkpoint format. The table_id field does not appear to be used.\nTo consume changes from the CDC SDK database stream, we have to execute a GetChanges request with the following configurattion:\nGetChangesRequestPB.db_stream_id: contains the database stream ID. GetChangesRequestPB.from_cdc_sdk_checkpoint: contains the checkpoint, an empty checkpoint is an instance of a checkpoint with all values set to defaults; this will make a consumer receive all changes from the beginning of the stream. GetChangesRequestPB.tablet_id: tablet ID is required. discovering tablet IDs to consume changes from The implication of the GetChangesRequestPB.tablet_id being required, is that we have to find all tablet IDs for each table of a database.\nForunately, the Debezium connector does this already. We can use the YugabyteDBConnector.fetchTabletList(...)4 method to reverse engineer this step.\nIt boils down to listing all tables using the master service ListTables request and filtering for PGSQL relations, where the relation type isn’t INDEX_TABLE_RELATION or SYSTEM_TABLE_RELATION (effectively, USER_TABLE_RELATION only).\n1 2 3 4 5 6 // https://github.com/yugabyte/yugabyte-db/blob/v2.13.0/src/yb/master/master_types.proto#L22 enum RelationType { SYSTEM_TABLE_RELATION = 1; USER_TABLE_RELATION = 2; INDEX_TABLE_RELATION = 3; } For each discovered table, we list its tablets using the master service GetTableLocations RPC call. As a result, we have a list of all tablet IDs we want to consume from.\nThere will be one consumer per tablet.\nWe can, of course, consume changes for specific tables (or tablets) only. To do so, we can skip tablets we don’t want to consume from.\nthe checkpoint The old checkpoint type contains term and index fields delivered in an OpIdPB message. The term is most likely referring to the Raft voter term of the tablet, index is most likely a Raft log entry index of the tablet.\nThe CDC SDK introduces a new checkpoint type. The new checkpoint type doesn’t use the OpId anymore. It defines term and index fields directly, next to additional properties. The protobuf type:\n1 2 3 4 5 6 7 8 9 10 11 12 // https://github.com/yugabyte/yugabyte-db/blob/v2.13.0/src/yb/cdc/cdc_service.proto#L162 message CDCSDKCheckpointPB { optional int64 term = 1; optional int64 index = 2; // write_id, key which is the reverse_index_iterator are used to resume from partially // streamed intents when the number of intents to be streamed are more than the intent max batch // size optional bytes key = 3; optional int32 write_id = 4 [default = 0]; // snapshot_time is used in the context of bootstrap process optional uint64 snapshot_time = 5; } The write_id is the order of changes within a multi-statement transaction and does not make sense when key is null.\nI’m not sure what’s snapshot_time for. Something to investigate further, snapshot handling is here8.\ncomparing checkpoints Checkpoints can be compared by term, index and write_id. These properties should be compared in order:\nHigher term implies higher checkpoint, regardless of the index values. If term is equal, higher index indicates a newer event. write_id makes sense only within a transaction, higher write_id implies a newer event within a single transaction. checkpoint management The CDC service manages checkpoints by itself when the CDC checkpoint type is IMPLICIT. Only very specific use cases would require the client to manage checkpoints manually, in the EXPLICIT checkpoint type mode.\nFor example, a client buffering records before processing them might require fine-grained checkpoint control.\nIt’s not clear to me how to put a CDC stream in a mode where CDC checkpoint can be set. Every time I attempted calling SetCDCCheckpoint for an EXPLICIT setting, YugabyteDB TServers crash.\nWeird, considering that the Java client tests appear to be testing this functionality9.\ncheckpoint management vs consumer offset management So I kind of made the consumer offset management term up here. This is a term from Apache Kafka world and there’s no reference to it anywhere in YugabyteDB documentation. A single stream ID+tablet ID combination can have multiple CDC consumers sourcing events from, and that’s perfectly fine.\nFor example:\nConsumer A performs audits. Consumer B observes selected tables and rewrites specific updates to an internal ERP system. Consumer C observes individual colums for secret values and raises alerts when those values appear. Each of those consumers is a standalone program with its own lifecycle. They are started and stopped independently. Each of them needs to have their own last processed checkpoint (a consumer offset) stored somwehere. It’s clear that the SetCDCCheckpoint operation isn’t designed to perform this role because it does not have a notion of a consumer ID.\nInstead, a consumer should process changes received from GetChanges. When finished, or when it is ready to commit a batch of processed changes, it should store the term and index for a stream ID+tablet ID combination. Next to those values, it needs to store a consumer name. The name should be unique to the consumer application but shared between different starts of the same consumer instance.\nI’m not sure if my assumptions are correct, but I would definitely try this approach if I was to build anything on the YugabyteDB CDC SDK. I would probably store checkpoints by myself in a dedicated table. The table could have the following schema:\n1 2 3 4 5 6 7 create table consumer_offsets ( streamID bytea, tabletID bytea, consumerID varchar(64), term bigint, index bigint ); processing changes The meat of every CDC consumer is the change processing. A response from GetChanges operation for a database stream returns an array of CDCSDKProtoRecordPB. Each of those items has the following structure:\n1 2 3 4 5 6 7 // https://github.com/yugabyte/yugabyte-db/blob/v2.13.0/src/yb/cdc/cdc_service.proto#L355 message CDCSDKProtoRecordPB { optional RowMessage row_message = 1; // Op id information to identify duplicate entries optional CDCSDKOpIdPB cdc_sdk_op_id = 2; } where a row message is:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 // https://github.com/yugabyte/yugabyte-db/blob/v2.13.0/src/yb/cdc/cdc_service.proto#L319 message RowMessage { enum Op { UNKNOWN = -1; INSERT = 0; UPDATE = 1; DELETE = 2; BEGIN = 3; COMMIT = 4; DDL = 5; TRUNCATE = 6; READ = 7; } optional bytes transaction_id = 1; optional uint64 commit_time = 2; optional string table = 3; optional Op op = 4; repeated DatumMessagePB new_tuple = 5; repeated DatumMessagePB old_tuple = 6; repeated TypeInfo new_typeinfo = 7; // Schema information used in case of DDL optional CDCSDKSchemaPB schema = 8; // Schema version optional uint32 schema_version = 9; // New table name, used in the case of rename table optional string new_table_name = 10; optional string pgschema_name = 11; // truncate info optional yb.tserver.TruncateRequestPB truncate_request_info = 13; } The inline Op identifies the type of the operation. This is where the CDC SDK shows its biggest difference from the original CDC solution. The CDC SDK provides additional context for DDL events, table truncation events, and transaction state. The YugabyteDBStreamingChangeEventSource.getChanges2(...) Debezium connector method mentioned earlier is a reference for event processing.\nThere are three groups of messages:\nTransactional messages: Op.BEGIN and Op.COMMIT. DDL messages: Op.DDL. DML messages: Op.INSERT, Op.UPDATE, Op.DELETE and Op.TRUNCATE. I was not able to trigger anything resulting in the Op.READ message type.\nDDL messages Here’s an example of a DDL message emitted after creating a table:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 { \"checkpoint\": { \"op_id\": { \"term\": 2, \"index\": 3 } }, \"cdc_sdk_proto_records\": [ { \"row_message\": { \"table\": \"region\", \"op\": 5, \"schema\": { \"column_info\": [ { \"name\": \"region_id\", \"type\": { \"main\": 2 }, \"is_key\": true, \"is_hash_key\": true, \"is_nullable\": false, \"oid\": 21 }, { \"name\": \"region_description\", \"type\": { \"main\": 5 }, \"is_key\": false, \"is_hash_key\": false, \"is_nullable\": true, \"oid\": 1042 } ], \"tab_info\": { \"default_time_to_live\": 0, \"num_tablets\": 3, \"is_ysql_catalog_table\": false } }, \"schema_version\": 0, \"pgschema_name\": \"public\" } } ], \"cdc_sdk_checkpoint\": { \"term\": 2, \"index\": 3, \"write_id\": 0, \"snapshot_time\": 0 } } DDM messages Let’s have a look at the data modification events in the following order: insert, update, delete, and truncate. Find out how the cluster is started and configured10.\ninsert: psql \"host=localhost port=35432 user=yugabyte dbname=cdctest\" -c \"insert into region values (1, 'test')\": 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 { \"checkpoint\": { \"op_id\": { \"term\": 1, \"index\": 4 } }, \"cdc_sdk_proto_records\": [ { \"row_message\": { \"transaction_id\": \"NTEwMzRlZTMtMjVkYS00YWRjLWJlYjItNjBmZjcwNjc4YTMz\", \"table\": \"region\", \"op\": 3 } }, { \"row_message\": { \"transaction_id\": \"NTEwMzRlZTMtMjVkYS00YWRjLWJlYjItNjBmZjcwNjc4YTMz\", \"table\": \"region\", \"op\": 0, \"new_tuple\": [ { \"column_name\": \"region_id\", \"column_type\": 21, \"Datum\": { \"DatumInt32\": 1 } }, { \"column_name\": \"region_description\", \"column_type\": 1042, \"Datum\": { \"DatumString\": \"test\" } } ], \"old_tuple\": [ { \"Datum\": null }, { \"Datum\": null } ], \"pgschema_name\": \"public\" }, \"cdc_sdk_op_id\": { \"term\": 1, \"index\": 4, \"write_id\": 1, \"write_id_key\": \"eFEDTuMl2krcvrJg/3BnijPcf/6GQXF3Ly5/wFQ=\" } }, { \"row_message\": { \"transaction_id\": \"NTEwMzRlZTMtMjVkYS00YWRjLWJlYjItNjBmZjcwNjc4YTMz\", \"table\": \"region\", \"op\": 4 }, \"cdc_sdk_op_id\": { \"term\": 1, \"index\": 4, \"write_id\": 0 } } ], \"cdc_sdk_checkpoint\": { \"term\": 1, \"index\": 4, \"write_id\": 0 } } The psql client wraps statements in a transaction. This gives us the opportunity to see what transactional messages look like.\nWe have three messages: Op.BEGIN, Op.INSERT, Op.COMMIT.\nOp.BEGIN and Op.COMMIT should be used as a boundary of consumer offset acceptance. Consumer offset should be advanced only after an Op.COMMIT message is processed. These could be useful in scenarios where a new value should only be visible after a batch of changes enclosed in the transaction.\nThe Op.INSERT message new_tuple field contains inserted values. old_tuple contains no values, which is to be expected.\nupdate: psql \"host=localhost port=35432 user=yugabyte dbname=cdctest\" -c \"update region set region_description='updated' where region_id=1\": 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 { \"checkpoint\": { \"op_id\": { \"term\": 1, \"index\": 6 } }, \"cdc_sdk_proto_records\": [ { \"row_message\": { \"transaction_id\": \"ZDk1NTA2NDQtNThmMC00N2JiLWExNDEtMTI2OTdjZWE1ODRi\", \"table\": \"region\", \"op\": 3 } }, { \"row_message\": { \"transaction_id\": \"ZDk1NTA2NDQtNThmMC00N2JiLWExNDEtMTI2OTdjZWE1ODRi\", \"table\": \"region\", \"op\": 1, \"new_tuple\": [ { \"column_name\": \"region_id\", \"column_type\": 21, \"Datum\": { \"DatumInt32\": 1 } }, { \"column_name\": \"region_description\", \"column_type\": 1042, \"Datum\": { \"DatumString\": \"updated\" } } ], \"old_tuple\": [ { \"Datum\": null }, { \"Datum\": null } ], \"pgschema_name\": \"public\" }, \"cdc_sdk_op_id\": { \"term\": 1, \"index\": 6, \"write_id\": 0, \"write_id_key\": \"eNlVBkRY8Ee7oUESaXzqWEvcf/6GQXXRDYl/tQ==\" } }, { \"row_message\": { \"transaction_id\": \"ZDk1NTA2NDQtNThmMC00N2JiLWExNDEtMTI2OTdjZWE1ODRi\", \"table\": \"region\", \"op\": 4 }, \"cdc_sdk_op_id\": { \"term\": 1, \"index\": 6, \"write_id\": 0 } } ], \"cdc_sdk_checkpoint\": { \"term\": 1, \"index\": 6, \"write_id\": 0 } } Again, a statement has been wrapped in a transaction so we have received three messages.\nThe new_tuple field contains values after the update. old_tuple does not contain any data, this is most likely the effect of CDCRecordType: CHANGE.\ndelete: psql \"host=localhost port=35432 user=yugabyte dbname=cdctest\" -c \"delete from region where region_id=1\": 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 { \"checkpoint\": { \"op_id\": { \"term\": 1, \"index\": 8 } }, \"cdc_sdk_proto_records\": [ { \"row_message\": { \"transaction_id\": \"NTQyMGRhYjUtYTJhMS00Y2FlLTk3OWMtMWUxMWJmMzgyNDMx\", \"table\": \"region\", \"op\": 3 } }, { \"row_message\": { \"transaction_id\": \"NTQyMGRhYjUtYTJhMS00Y2FlLTk3OWMtMWUxMWJmMzgyNDMx\", \"table\": \"region\", \"op\": 2, \"new_tuple\": [ { \"Datum\": null } ], \"old_tuple\": [ { \"column_name\": \"region_id\", \"column_type\": 21, \"Datum\": { \"DatumInt32\": 1 } } ], \"pgschema_name\": \"public\" }, \"cdc_sdk_op_id\": { \"term\": 1, \"index\": 8, \"write_id\": 0, \"write_id_key\": \"eFQg2rWioUyul5weEb84JDHcf/6GQXoUJj9/tQ==\" } }, { \"row_message\": { \"transaction_id\": \"NTQyMGRhYjUtYTJhMS00Y2FlLTk3OWMtMWUxMWJmMzgyNDMx\", \"table\": \"region\", \"op\": 4 }, \"cdc_sdk_op_id\": { \"term\": 1, \"index\": 8, \"write_id\": 0 } } ], \"cdc_sdk_checkpoint\": { \"term\": 1, \"index\": 8, \"write_id\": 0 } } Yet again, the statement is wrapped in a transacton. We can see that the new_tuple is empty, which makes sense. old_tuple contains the value of the primary key of a deleted item.\ntruncate: psql \"host=localhost port=35432 user=yugabyte dbname=cdctest\" -c \"truncate table region cascade\": This statement does not seem to be handled properly on the database level yet. Issuing a truncate statement breaks the stream (the consumer is not able to get changes anymore), and TServers log errors like these:\nyb-tserver-shared-3 | [libprotobuf FATAL google/protobuf/reflection_ops.cc:59] CHECK failed: (to-\u003eGetDescriptor()) == (descriptor): Tried to merge messages of different types (merge yb.tablet.TruncatePB to yb.tserver.TruncateRequestPB) yb-tserver-shared-3 | libc++abi: terminating with uncaught exception of type google::protobuf::FatalException: CHECK failed: (to-\u003eGetDescriptor()) == (descriptor): Tried to merge messages of different types (merge yb.tablet.TruncatePB to yb.tserver.TruncateRequestPB) yb-tserver-shared-3 | [libprotobuf FATAL google/protobuf/reflection_ops.cc:59] CHECK failed: (to-\u003eGetDescriptor()) == (descriptor): Tried to merge messages of different types (merge yb.tablet.TruncatePB to yb.tserver.TruncateRequestPB) yb-tserver-shared-3 | libc++abi: terminating with uncaught exception of type google::protobuf::FatalException: CHECK failed: (to-\u003eGetDescriptor()) == (descriptor): Tried to merge messages of different types (merge yb.tablet.TruncatePB to yb.tserver.TruncateRequestPB) no primary key, no CDC? All those tests were done on a table with a primary key. Naturally, I wanted to check if it is possible to retrieve changes from a table without a primary key:\n1 2 3 psql \"host=localhost port=35432 user=yugabyte dbname=cdctest\" -c \"create table test_table(a text, b text)\" psql \"host=localhost port=35432 user=yugabyte dbname=cdctest\" -c \"insert into test_table values ('test value 1a', 'test value 1b')\" psql \"host=localhost port=35432 user=yugabyte dbname=cdctest\" -c \"insert into test_table values ('test value 2a', 'test value 2b')\" Trying to get changes for tablets of that table returns a CDC RPC error similar to:\nreason= | CDC rpc error: code: 10 (INVALID_REQUEST), status: 4 (INVALID_ARGUMENT) | message: Tablet ID d17e6b58a79f418dbae5e95978bf2f59 is not part of stream ID 2861d4b6f25d49f7982bcfb2592b54f0 | source: ../../ent/src/yb/cdc/cdc_service.cc@457 This would imply that it’s impossible to use CDC SDK on tables without a primary key. And indeed, this part of the CDC service11:\n1 2 3 4 5 6 7 // internally if any of the table doesn't have a primary key, then do not create // a CDC stream ID for that table if (!YsqlTableHasPrimaryKey(table-\u003eschema())) { LOG(WARNING) \u003c\u003c \"Skipping CDC stream creation on \" \u003c\u003c table-\u003ename().table_name() \u003c\u003c \" because it does not have a primary key\"; continue; } confirms that. The original, simple CDC has the same restriction.\nchanges on the entity level Another major difference from the first CDC implementation is that change events are per row (entity). There’s no longer an event per tablet. The internal CDC service implementation locates a tablet leader and redelivers tablet leader events only.\nOrder of events reflects order of operations on a row. An arrival of event implies that all previous operations on that row have been committed.\nconsuming from latest operation Sometimes we might want to skip consuming historical changes and consume from the latest entry only. There are two possibilities, depending on how the database stream has been created:\nCDC checkpoint type IMPLICIT: this is the default behaviour when reusing stream IDs. CDC checkpoint type EXPLICIT: use GetCheckpoint to discover latest saved checkpoint, a value of term: 0, index: 0 might indicate that no SetCDCCheckpoint has been called, consider using GetLatestEntryOpId in that case. reference go client My reference CDC SDK client can be found in this GitHub repository12.\nsummary It’s clear that the CDC SDK is a higher level solution than the first CDC implementation.\nCurrent state:\nDoes what it says on the tin. Has rough edges and requires some clarifications: Fully functional support for truncate will be great to have. No support for tables without primary key. This would be great to have, it would enable some very interesting use cases for continous backup/restore. What’s the checkpoint’s snapshot_time? What can it be used for? Why crashes when executing SetCDCCheckpoint in EXPLICIT mode? YugabyteDB CDC is one of the features to track. The documentation in GitHub13 suggests some pretty cool features coming.\nYugabyteDB change data capture ↩︎\nYugabyteDB Debezium connector ↩︎\nYugabyteDB Debezium connector: YugabyteDBConnector.validateTServerConnection(...) ↩︎\nYugabyteDB Debezium connector: YugabyteDBConnector.fetchTabletList(...) ↩︎ ↩︎\nYugabyteDB Debezium connector: YugabyteDBStreamingChangeEventSource.getChanges2(...) ↩︎\nCurrently undocumented yb-admin create_change_data_stream operation ↩︎\nYugabyteDB source code: ClusterAdminClient::CreateCDCSDKDBStream ↩︎\nsnapshot handling in YugabyteDB CDC SDK producer ↩︎\nJava client checkpoint handling tests ↩︎\nYugabyteDB change data capture: cluster configuration ↩︎\nYugabyteDB source code: do not create a CDC stream if table doesn’t have a primary key ↩︎\nYugabyteDB CDC test golang client ↩︎\n2.13 DocDB CDC documentation ↩︎\n","description":"an overview of the all-new CDC SDK beta","tags":["yugabytedb","cdc"],"title":"YugabyteDB CDC SDK beta, a high level overview","uri":"/posts/2022-03-19-yugabytedb-cdc-sdk-beta-a-high-level-overview/"},{"content":" 19th of March, 2022:\nInterested in YugabyteDB CDC? YugabyteDB 2.13 comes with an all-new beta of a CDC SDK: YugabyteDB CDC SDK beta, a high level overview.\nToday I’m going to look at change data capture (CDC) in YugabyteDB. Change data capture is a data integration method based on the identification, capture, and delivery of data changes. Enterprises can choose this method to identify what data changes in the database and to act on those changes in real-time. What are the use cases for CDC? For example:\nA fulfillment enterprise can track new stock booking-ins and automatically fulfill backorders. An enterprise can track account creation and modification to trigger automatic CRM updates. An online shop can react to updates in exchange rates to automatically recalculate prices for different currencies. A compliance department can automatically detect when certain user accounts or any other item in the database does not comply with a legal framework. In the simplest form, organizations can synchronize different databases in real-time. However, YugabyteDB offers a feature called xCluster, an asynchronous cluster to cluster replication. Possibilities are endless and can be applied to pretty much every possible business problem. The gist here is: reacting to changes in real-time.\nYugabyteDB is a horizontally scalable 100% open-source distributed SQL database providing Cassandra (YCQL) and PostgreSQL (YSQL) compatible APIs. For PostgreSQL API, YugabyteDB uses the actual PostgreSQL 11.2 engine. With just PostgreSQL, a company could use the NOTIFY/SUBSCRIBE functionality for reactivity within the database, and a CDC solution like Debezium for outside-of-the-database integration.\nYugabyteDB currently does not support NOTIFY/SUBSCRIBE but it comes with built-in support for CDC. CDC operates on a lower level than said PostgreSQL functionality, but it’s a very good foundation to build a general purpose NOTIFY/SUBSCRIBE alternative. The downside is, the data will always leave the cluster in order to be processed.\ncommercial plug If your company is looking for someone who can help you migrate your database to the cloud, or your company needs a solid partner who knows a thing or two about YugabyteD—reach out to Radek at radek.gruchalski@klarrio.com.\nAt Klarrio, we do all sorts of amazing stuff with all sorts of technologies. Bleeding edge, cloud, hybrid, but also boring and old school. We’re not chasing the new shiny. The best tool for the job.\nGet in touch to find out more.\nanalysis of a YugabyteDB CDC client I started my investigation by looking at the YugabyteDB CDC Java client - yb-cdc. The client is part of the YugabyteDB source code repository and it can be found here1. The code footprint is pretty small so this makes it easier to identify required steps.\nThe Java client comes with six classes:\norg.yb.cdc.CmdLineOptions org.yb.cdc.LogClient org.yb.cdc.LogConnector org.yb.cdc.Main org.yb.cdc.OutputClient org.yb.cdc.Poller We can safely ignore CmdLineOptions, Main, LogClient, and OutputClient, they’re just a boilerplate to have a functional Java program. OutputClient is an interface a real Java program needs to implement to receive changes from the Poller instance. LogClient is a default implementation of the OutputClient. That leaves us with two classes having some significance.\nThe LogConnector class contains the YugabyteDB client and performs the setup of a CDC session. The Poller performs the data capture.\nthe connector Let’s look at the LogConnector. There’s only a constructor a run() method. The constructor does the following:\nCreates YugabyteDB async and sync clients (this is an implementation detail of how the Java client works). Lists all tables: In the YugabyteDB RPC API world, this call returns all tables for all databases in a YugabyteDB cluster. To find out more about the YugabyteDB RPC API, read this article2.\nThe response is a protobuf ListTablesRequestPB message:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 message ListTablesResponsePB { // The error, if an error occurred with this request. optional MasterErrorPB error = 1; message TableInfo { required bytes id = 1; required string name = 2; optional TableType table_type = 3; optional NamespaceIdentifierPB namespace = 4; optional RelationType relation_type = 5 [default = USER_TABLE_RELATION]; optional SysTablesEntryPB.State state = 6; } repeated TableInfo tables = 2; } The id is a table ID, name is a table name, and the NamespaceIdentifierPB namespace contains the information about the database the table belongs to. The NamespaceIdentifierPB message looks like this:\n1 2 3 4 5 6 7 8 9 10 message NamespaceIdentifierPB { // The namespace ID to fetch info. optional bytes id = 1; // The namespace name to fetch info. optional string name = 2; // Database type. optional YQLDatabase database_type = 3 [ default = YQL_DATABASE_CQL ]; } The id is the namespace ID, name is the database name and the database_type will hold the value of YQL_DATABASE_PGSQL for YSQL tables.\nOnce all tables are listed, the LogConnector filters retrieved table infos for the required table indicated with the table_name command-line argument; this argument has the following form \u003ckeyspace\u003e.\u003ctable-name\u003e; the client splits the argument by a . and uses the pair to find the table internal table ID. Lists cluster tablet servers. At this point in time, the LogConnector is connected and it verified that the table for which the CDC consumer is to be created, exists. Once run() is executed, the connector will:\nCheck if the CDC stream ID is given, if not, it will create a CDC using the CreateCDCStreamRequestPB message defined in master/master_replication.proto. Find all tablets for a table using the GetTableLocationsRequestPB defined in master/master_client.proto. For each tablet, create a Poller. the poller The Poller class is a little bit more complex than the connector but what it does, is rather simple to understand. The work starts by calling the poll() method.\nUsing a randomly selected TServer address, the poller sends the GetChangesRequestPB message to the master replication service. The message definition is in cdc/cdc_service.proto:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 message GetChangesRequestPB { optional bytes stream_id = 1; // Tablet to get the changes for. optional bytes tablet_id = 2; // Checkpoint to start reading from (exclusive). // Start reading from the first record after this checkpoint. optional CDCCheckpointPB from_checkpoint = 3; // Maximum records to read. optional uint32 max_records = 4; // Whether the caller knows the tablet address or needs to use us as a proxy. optional bool serve_as_proxy = 5 [default = true]; } The request requires a stream_id, a tablet_id, and the checkpoint metadata. The checkpoint metadata definition is:\n1 2 3 message CDCCheckpointPB { optional OpIdPB op_id = 1; } where op_id is defined in util/opid.proto:\n1 2 3 4 5 message OpIdPB { // The term of an operation or the leader's sequence id. required int64 term = 1; required int64 index = 2; } Once the response arrives, the response data propagates to the Void doHandlePoll(GetChangesResponse getChangesResponse) method. The doHandlePoll processes individual records from the response, updates the Poller internal term and index, and executes poll() once again. Thus, the Poller continuously polls for new data and each tablet has its own Poller instance.\nbuilding a CDC solution from scratch To verify the knowledge, I am going to build a very simple CDC client. Instead of Java, I used Go with the Go RPC client3.\nYou can find the CDC client here4.\nthe YugabyteDB cluster To be able to test the solution, I need a YugabyteDB cluster with some data in it. I’m going to use the YugabyteDB multi-tenant PaaS demo5, which I’ve created a while ago.\nMy environment looks like this:\n1 2 3 4 $ docker-compose -v Docker Compose version v2.2.3 $ docker -v Docker version 20.10.12, build e91ed57 On Linux, Docker Compose needs to be installed separate from Docker. Instructions here6.\n1 2 3 4 5 6 7 8 9 10 11 cd /tmp git clone https://github.com/radekg/yugabyte-db-multi-tenant-paas-demo.git cd yugabyte-db-multi-tenant-paas-demo # build the Docker image used by the infrastructure make docker-image-upstream # point to an env file export DC_ENV_FILE=.env # start the cluster docker compose --env-file \"$(pwd)/${DC_ENV_FILE}\" \\ -f compose-masters.yml \\ -f compose-tservers-shared.yml up This will take a few seconds to settle. There will be:\nthree master servers, three TServers, an Envoy proxy in front of TServers. Let’s verify that everything is working. In another terminal:\n1 2 docker exec -ti yb-master-n1 /bin/bash -c \\ 'yb-admin -master_addresses yb-master-n1:7100,yb-master-n2:7100,yb-master-n3:7100 list_all_masters' should produce something similar to:\nMaster UUID RPC Host/Port State Role 44dec826f77148929c12e4afd2ead389 yb-master-n1:7100 ALIVE FOLLOWER b8db32a7f9f549c2bdbd824a07d173fb yb-master-n2:7100 ALIVE LEADER 1f549268a41f4759ab821042f3f2490c yb-master-n3:7100 ALIVE FOLLOWER Your UUIDs will be different and your cluster may have a different leader. Let’s look at our TServers, again, TServer UUIDs will be different:\n1 2 docker exec -ti yb-master-n1 /bin/bash -c \\ 'yb-admin -master_addresses yb-master-n1:7100,yb-master-n2:7100,yb-master-n3:7100 list_all_tablet_servers' Tablet Server UUID RPC Host/Port Heartbeat delay Status Reads/s Writes/s Uptime SST total size SST uncomp size SST #files Memory ce5dbc4bdc574b33b52313f0ea8bf89d yb-tserver-shared-2:9100 0.43s ALIVE 0.00 0.00 157 0 B 0 B 0 45.09 MB 49e47380d5cc4e53b8991d5f4034b596 yb-tserver-shared-1:9100 0.43s ALIVE 0.00 0.00 157 0 B 0 B 0 45.09 MB e313e4dd6cd1410189fa73562a47fba4 yb-tserver-shared-3:9100 0.43s ALIVE 0.00 0.00 158 0 B 0 B 0 44.04 MB load the data into the cluster To connect via YSQL connection, you will have to connect via Envoy. Docker Compose exposes Envoy to your host on port 35432. We can try this with:\n1 2 psql \"host=localhost port=35432 user=yugabyte dbname=yugabyte\" \\ -c \"select table_name from information_schema.tables limit 1\" The password is yugabyte.\nThe output is:\ntable_name ---------------- pg_default_acl (1 row) Let’s create a database and load the data:\n1 2 3 4 5 6 7 psql \"host=localhost port=35432 user=yugabyte dbname=yugabyte\" \\ -c \"create database cdctest\" # the database name changes here: psql \"host=localhost port=35432 user=yugabyte dbname=cdctest\" \\ -f sql-init-northwind-tenant1.sql psql \"host=localhost port=35432 user=yugabyte dbname=cdctest\" \\ -f sql-init-northwind-data-tenant1.sql Don’t worry about the tenant1 suffix. It doesn’t matter. Let’s check what tables do we have in our database:\n1 2 psql \"host=localhost port=35432 user=yugabyte dbname=cdctest\" \\ -c \"select table_name from information_schema.tables where table_schema='public'\" table_name ------------------------ categories customer_demographics customers customer_customer_demo employees suppliers products region shippers orders territories employee_territories order_details us_states (14 rows) 1 2 psql \"host=localhost port=35432 user=yugabyte dbname=cdctest\" \\ -c \"select count(*) from customers\" count ------- 91 (1 row) The database is initialized and ready to work with.\nrun the minimal client Time to build and run the minimal CDC client:\n1 2 3 4 cd /tmp git clone https://github.com/radekg/yugabyte-db-cdctest.git cd yugabyte-db-cdctest/ docker build -t local/yugabyte-db-cdctest:latest . and run it against the cluster:\n1 2 3 4 5 docker run --net=yb-dbnet --rm \\ -ti local/yugabyte-db-cdctest:latest \\ --database=cdctest \\ --table=region \\ --masters=yb-master-n1:7100,yb-master-n2:7100,yb-master-n3:7100 After a short moment, the CDC data should start streaming to the terminal. This initial output is the data inserted into the database during the initial import.\nNote: the network used by the cluster is called yb-dbnet and that’s the network we run the CDC container in.\nCDC payloads Let’s ignore the initial data streamed to us and focus on subsequent changes. While the cluster and the CDC client are running, execute the following query:\n1 2 psql \"host=localhost port=35432 user=yugabyte dbname=cdctest\" \\ -c \"insert into region values (5, 'test')\" The CDC program is going to output a JSON object similar to:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 { \"records\": [ { \"time\": 6740254233675632640, \"operation\": 1, \"key\": [ { \"key\": \"cmVnaW9uX2lk\", \"value\": { \"Value\": { \"Int16Value\": 5 } } } ], \"changes\": [ { \"key\": \"cmVnaW9uX2Rlc2NyaXB0aW9u\", \"value\": { \"Value\": { \"StringValue\": \"dGVzdA==\" } } } ], \"transaction_state\": { \"transaction_id\": \"5f3eR+OKSbWaOYDWE+FRIQ==\", \"tablets\": [ \"MzczZDRmM2E5N2JhNDk5Mzk1ZWI5NzRlNWQyOGM3NGU=\" ] } }, { \"time\": 6740254233712037888, \"operation\": 3, \"transaction_state\": { \"transaction_id\": \"5f3eR+OKSbWaOYDWE+FRIQ==\", \"commit_hybrid_time\": 6740254233699799040 }, \"partition\": { \"partition_key_start\": \"VVU=\", \"partition_key_end\": \"qqo=\" } } ], \"checkpoint\": { \"op_id\": { \"term\": 2, \"index\": 61 } } } There’s going to be one JSON object per each individual tablet. What can we infer from this data?\nrecords.*.operation is the operation type, one of: 1 2 3 4 5 6 7 type CDCRecordPB_OperationType int32 const ( CDCRecordPB_WRITE CDCRecordPB_OperationType = 1 CDCRecordPB_DELETE CDCRecordPB_OperationType = 2 CDCRecordPB_APPLY CDCRecordPB_OperationType = 3 ) In this case, the operation was write followed by an apply.\nrecords.*.key contains the row key for which the write happened, that’s where we can find our 5, the cmVnaW9uX2lk is a base64 encoded region_id; the key is an array of objects, which suggests that there may be multiple entries when the table has a composite key, records.*.changes is an array of objects with a key and a value, the key of each change, is the colume name, if we base64 decode the cmVnaW9uX2Rlc2NyaXB0aW9u value, we get region_description, which is the column name where the textual value goes, the value of each individual change contains a typed value for the column, this is probably where the largest chunk of processing would go in a real application (do discover the value type, it could probably be correlated by loading the table schema beforehand GetTableSchemaRequestPB message defined in master/master_ddl.proto), in our case, the base64 decoded dGVzdA== is test - exactly what we have inserted, the complete changeset contains the partition information; combined with the tablet ID of each individual change, it would be possible to react only to the changes happening within a specific geographical location; find out more7. Okay, let’s see what happens when we issue an update:\n1 2 psql \"host=localhost port=35432 user=yugabyte dbname=cdctest\" \\ -c \"update region set region_description='new value' where region_id=5\" Results in JSON similar to:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 { \"records\":[ { \"time\":6740254233675632640, \"operation\":1, \"key\":[ { \"key\":\"cmVnaW9uX2lk\", \"value\":{ \"Value\":{ \"Int16Value\":5 } } } ], \"changes\":[ { \"key\":\"cmVnaW9uX2Rlc2NyaXB0aW9u\", \"value\":{ \"Value\":{ \"StringValue\":\"dGVzdA==\" } } } ], \"transaction_state\":{ \"transaction_id\":\"5f3eR+OKSbWaOYDWE+FRIQ==\", \"tablets\":[ \"MzczZDRmM2E5N2JhNDk5Mzk1ZWI5NzRlNWQyOGM3NGU=\" ] } }, { \"time\":6740254233712037888, \"operation\":3, \"transaction_state\":{ \"transaction_id\":\"5f3eR+OKSbWaOYDWE+FRIQ==\", \"commit_hybrid_time\":6740254233699799040 }, \"partition\":{ \"partition_key_start\":\"VVU=\", \"partition_key_end\":\"qqo=\" } }, { \"time\":6740257126013730816, \"operation\":1, \"key\":[ { \"key\":\"cmVnaW9uX2lk\", \"value\":{ \"Value\":{ \"Int16Value\":5 } } } ], \"changes\":[ { \"key\":\"cmVnaW9uX2Rlc2NyaXB0aW9u\", \"value\":{ \"Value\":{ \"StringValue\":\"bmV3IHZhbHVl\" } } } ], \"transaction_state\":{ \"transaction_id\":\"B7Ewh2S6RmW7T9R0kx/JSw==\", \"tablets\":[ \"MzczZDRmM2E5N2JhNDk5Mzk1ZWI5NzRlNWQyOGM3NGU=\" ] } }, { \"time\":6740257126053367808, \"operation\":3, \"transaction_state\":{ \"transaction_id\":\"B7Ewh2S6RmW7T9R0kx/JSw==\", \"commit_hybrid_time\":6740257126043394048 }, \"partition\":{ \"partition_key_start\":\"VVU=\", \"partition_key_end\":\"qqo=\" } } ], \"checkpoint\":{ \"op_id\":{ \"term\":2, \"index\":63 } } } Again, one object per tablet. This time we have four records. A write, apply, write, and apply again. We can see that our new value is returned here, the bmV3IHZhbHVl is a base64 encoded new value string.\nAlso, it seems that for each column, we get a full log of subsequent changes! That’s pretty cool.\nLet’s try a delete:\n1 2 psql \"host=localhost port=35432 user=yugabyte dbname=cdctest\" \\ -c \"delete from region where region_id=5\" Which will output the following, for each tablet:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 { \"records\":[ { \"time\":6740254233675632640, \"operation\":1, \"key\":[ { \"key\":\"cmVnaW9uX2lk\", \"value\":{ \"Value\":{ \"Int16Value\":5 } } } ], \"changes\":[ { \"key\":\"cmVnaW9uX2Rlc2NyaXB0aW9u\", \"value\":{ \"Value\":{ \"StringValue\":\"dGVzdA==\" } } } ], \"transaction_state\":{ \"transaction_id\":\"5f3eR+OKSbWaOYDWE+FRIQ==\", \"tablets\":[ \"MzczZDRmM2E5N2JhNDk5Mzk1ZWI5NzRlNWQyOGM3NGU=\" ] } }, { \"time\":6740254233712037888, \"operation\":3, \"transaction_state\":{ \"transaction_id\":\"5f3eR+OKSbWaOYDWE+FRIQ==\", \"commit_hybrid_time\":6740254233699799040 }, \"partition\":{ \"partition_key_start\":\"VVU=\", \"partition_key_end\":\"qqo=\" } }, { \"time\":6740257126013730816, \"operation\":1, \"key\":[ { \"key\":\"cmVnaW9uX2lk\", \"value\":{ \"Value\":{ \"Int16Value\":5 } } } ], \"changes\":[ { \"key\":\"cmVnaW9uX2Rlc2NyaXB0aW9u\", \"value\":{ \"Value\":{ \"StringValue\":\"bmV3IHZhbHVl\" } } } ], \"transaction_state\":{ \"transaction_id\":\"B7Ewh2S6RmW7T9R0kx/JSw==\", \"tablets\":[ \"MzczZDRmM2E5N2JhNDk5Mzk1ZWI5NzRlNWQyOGM3NGU=\" ] } }, { \"time\":6740257126053367808, \"operation\":3, \"transaction_state\":{ \"transaction_id\":\"B7Ewh2S6RmW7T9R0kx/JSw==\", \"commit_hybrid_time\":6740257126043394048 }, \"partition\":{ \"partition_key_start\":\"VVU=\", \"partition_key_end\":\"qqo=\" } }, { \"time\":6740259770935541760, \"operation\":2, \"key\":[ { \"key\":\"cmVnaW9uX2lk\", \"value\":{ \"Value\":{ \"Int16Value\":5 } } } ], \"transaction_state\":{ \"transaction_id\":\"w/OzDpk5TQSZrSOR2lz6yg==\", \"tablets\":[ \"MzczZDRmM2E5N2JhNDk5Mzk1ZWI5NzRlNWQyOGM3NGU=\" ] } }, { \"time\":6740259771120664576, \"operation\":3, \"transaction_state\":{ \"transaction_id\":\"w/OzDpk5TQSZrSOR2lz6yg==\", \"commit_hybrid_time\":6740259771113287680 }, \"partition\":{ \"partition_key_start\":\"VVU=\", \"partition_key_end\":\"qqo=\" } } ], \"checkpoint\":{ \"op_id\":{ \"term\":2, \"index\":65 } } } The delete resulted in six rows. So clearly the history of changes for each key is persisted. The two new operation types are 2 and 3: delete and apply respectively.\nWhat’s Looking at the protobuf object reveals more interesting detail:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 message CDCRecordPB { enum OperationType { WRITE = 1; DELETE = 2; APPLY = 3; SPLIT_OP = 4; } optional uint64 time = 1; optional OperationType operation = 2; // Primary key of the record that changed repeated KeyValuePairPB key = 3; // Key-value pairs (column_name : value) of changes / before record / after record repeated KeyValuePairPB changes = 4; repeated KeyValuePairPB before = 5; // NOT CURRENTLY USED repeated KeyValuePairPB after = 6; // NOT CURRENTLY USED optional tablet.TransactionStatePB transaction_state = 7; // If transaction_state.status is APPLYING, then partitions will contain the partition for the // tablet being polled. optional PartitionPB partition = 9; // If operation type is a SPLIT_OP, then include the split request. optional tablet.SplitTabletRequestPB split_tablet_request = 10; } It seems like we may be getting more insight into what we had before the change and what we have after the change - before, after, and changes fields. However, as the comments say, it’s not supported yet.\nit’s powerful Very straightforward to approach and a great foundation for integrating distributed SQL with real-time pipelines. CDC data can be easily ingested and transformed with Kafka or another data streaming solution to power all sorts of business applications inside of an enterprise.\nHaving an out-of-the-box possibility to fuse distributed SQL and real-time streaming data is a superpower with unlimited applications in today’s data-driven world.\nThat’s it for today, dipping the toes in YugabyteDB CDC.\nYugabyteDB CDC Java client ↩︎\nA brief look at YugabyteDB RPC API ↩︎\nYugabyteDB Go RPC client ↩︎\nYugabyteDB CDC demo client ↩︎\nYugabyteDB multi-tenant PaaS demo ↩︎\nInstall Compose on Linux systems ↩︎\nGeo-Partitioning of Data in YugabyteDB ↩︎\n","description":"real-time systems + distributed SQL","tags":["yugabytedb","cdc"],"title":"YugabyteDB change data capture","uri":"/posts/2022-02-23-yugabytedb-change-data-capture/"},{"content":"With every version, the number of PostgreSQL features supported by YugabyteDB inevitably increases. This is good! However, it is pretty difficult to come by a comprehensive list of unsupported features. There’s a roadmap on GitHub1, but the roadmap lists top-level features and misses the exact list of statements and keywords. There are also GitHub issues, but there’s over 11k of them so it could be pretty difficult to filter out what’s really unsupported.\nI have been recently looking at exactly that problem while preparing the end-user documentation for one of the projects where YugabyteDB plays the core role.\nAs it turns out, there’s a very simple method to learn what’s missing. PostgreSQL contains a YACC grammar file, and yugabyteDB contains the same-but-slightly-modified gram.y file2.\nWe can learm some interesting things from the grammar, for example these YugabyteDB specific definitions:\n#define parser_yyerror(msg) scanner_yyerror(msg, yyscanner) #define parser_errposition(pos) scanner_errposition(pos, yyscanner) #define parser_ybc_not_support(pos, feature) \\ ybc_not_support(pos, yyscanner, feature \" not supported yet\", -1) #define parser_ybc_warn_ignored(pos, feature, issue) \\ ybc_not_support_signal(pos, yyscanner, feature \" not supported yet and will be ignored\", issue, WARNING) #define parser_ybc_signal_unsupported(pos, feature, issue) \\ ybc_not_support(pos, yyscanner, feature \" not supported yet\", issue) #define parser_ybc_not_support_in_templates(pos, feature) \\ ybc_not_support_in_templates(pos, yyscanner, feature \" is not supported in template0/template1 yet\") #define parser_ybc_beta_feature(pos, feature, has_own_flag) \\ check_beta_feature(pos, yyscanner, has_own_flag ? \"FLAGS_ysql_beta_feature_\" feature : NULL, feature) These definitions are used further in the grammar to annotate which features are not yet implemented. The ones which are of the most interest to us are:\nparser_ybc_not_support: indicates an unsupported feature, no tracking issue, parser_ybc_warn_ignored: indicates an unsupported and ignored feature, these features generate warnings during query execution but do not result in errors, the issue argument identifies a GitHub issue used to track the feature, parser_ybc_signal_unsupported: indicates an unsupported feature, these features result in errors during query execution, the issue argument identifies a GitHub issue used to track the feature. With this input, we can rather quickly get a good overview of what’s not yet in YugabyteDB.\nAnd, we can do it for each individual version. Let’s try for 2.11.1:\n1 2 3 4 export YBDBV=2.11.1 cd /tmp wget https://raw.githubusercontent.com/yugabyte/yugabyte-db/v${YBDBV}/src/postgres/src/backend/parser/gram.y -O gram-${YBDBV}.y grep parser_ybc_not_support gram-${YBDBV}.y | grep -v parser_ybc_not_support_in_templates Produces output similar to:\n#define parser_ybc_not_support(pos, feature) \\ | AlterObjectDependsStmt { parser_ybc_not_support(@1, \"This statement\"); } | AlterSystemStmt { parser_ybc_not_support(@1, \"This statement\"); } | AlterCompositeTypeStmt { parser_ybc_not_support(@1, \"This statement\"); } | AlterPublicationStmt { parser_ybc_not_support(@1, \"This statement\"); } | AlterSubscriptionStmt { parser_ybc_not_support(@1, \"This statement\"); } | AlterTSConfigurationStmt { parser_ybc_not_support(@1, \"This statement\"); } | AlterTSDictionaryStmt { parser_ybc_not_support(@1, \"This statement\"); } | ClusterStmt { parser_ybc_not_support(@1, \"This statement\"); } | CreateAmStmt { parser_ybc_not_support(@1, \"This statement\"); } | CreateAssertStmt { parser_ybc_not_support(@1, \"This statement\"); } | CreateConversionStmt { parser_ybc_not_support(@1, \"This statement\"); } | CreateMatViewStmt { parser_ybc_not_support(@1, \"This statement\"); } | CreatePublicationStmt { parser_ybc_not_support(@1, \"This statement\"); } | CreatePLangStmt { parser_ybc_not_support(@1, \"This statement\"); } | CreateSubscriptionStmt { parser_ybc_not_support(@1, \"This statement\"); } | CreateStatsStmt { parser_ybc_not_support(@1, \"This statement\"); } | CreateTransformStmt { parser_ybc_not_support(@1, \"This statement\"); } | DropAssertStmt { parser_ybc_not_support(@1, \"This statement\"); } | DropPLangStmt { parser_ybc_not_support(@1, \"This statement\"); } | DropSubscriptionStmt { parser_ybc_not_support(@1, \"This statement\"); } | DropTransformStmt { parser_ybc_not_support(@1, \"This statement\"); } | RefreshMatViewStmt { parser_ybc_not_support(@1, \"This statement\"); } | LoadStmt { parser_ybc_not_support(@1, \"This statement\"); } | ReindexStmt { parser_ybc_not_support(@1, \"This statement\"); } | SecLabelStmt { parser_ybc_not_support(@1, \"This statement\"); } parser_ybc_not_support(@6, \"CREATE SCHEMA with elements\"); parser_ybc_not_support(@4, \"CREATE SCHEMA with elements\"); parser_ybc_not_support(@1, \"CREATE STATISTICS\"); parser_ybc_not_support(@1, \"CREATE STATISTICS\"); parser_ybc_not_support(@1, \"CREATE LANGUAGE\"); parser_ybc_not_support(@1, \"CREATE LANGUAGE\"); parser_ybc_not_support(@1, \"DROP LANGUAGE\"); ... Okay, so we can use the same method to search for:\nignored features: grep parser_ybc_warn_ignored gram-${YBDBV}.y: unsupported features tracked in GitHub issues: grep parser_ybc_signal_unsupported gram-${YBDBV}.y To gain more context into surrounding code, use -B num-lines flag of grep.\ntracking progress between versions Now that we can identify unsupported and ignored features, let’s see if we can identify progress between versions. We are going to compare 2.11.1 and 2.11.2.\n1 2 3 4 5 6 7 cd /tmp export YBDBV_OLD=2.11.1 export YBDBV_NEW=2.11.2 wget https://raw.githubusercontent.com/yugabyte/yugabyte-db/v${YBDBV_OLD}/src/postgres/src/backend/parser/gram.y -O gram-${YBDBV_OLD}.y grep parser_ybc_not_support gram-${YBDBV_OLD}.y | grep -v parser_ybc_not_support_in_templates | wc -l wget https://raw.githubusercontent.com/yugabyte/yugabyte-db/v${YBDBV_NEW}/src/postgres/src/backend/parser/gram.y -O gram-${YBDBV_NEW}.y grep parser_ybc_not_support gram-${YBDBV_NEW}.y | grep -v parser_ybc_not_support_in_templates | wc -l The line counts are 139 and 136 respectively. Let’s see what has been removed from 2.11.2, which indicates that feature support has been added:\n1 2 grep parser_ybc_not_support gram-${YBDBV_OLD}.y | grep -v parser_ybc_not_support_in_templates \u003e out-${YBDBV_OLD} grep parser_ybc_not_support gram-${YBDBV_NEW}.y | grep -v parser_ybc_not_support_in_templates \u003e out-${YBDBV_NEW} and compare the out files:\n1 diff out-${YBDBV_OLD} out-${YBDBV_NEW} which produces:\n1 2 3 4 5 6 13d12 \u003c | CreateMatViewStmt { parser_ybc_not_support(@1, \"This statement\"); } 23d21 \u003c | RefreshMatViewStmt { parser_ybc_not_support(@1, \"This statement\"); } 44d41 \u003c parser_ybc_not_support(@1, \"DROP MATERIALIZED VIEW\"); This indicates that the 2.11.2 version of YugabyteDB added support for CREATE/REFRESH/DROP MATERIALIZED VIEW statements.\nIndeed, if we look at the release notes for 2.11.23, we will find the following line: [YSQL] Add support for CREATE, DROP, and REFRESH MATERIALIZED VIEW.\nRelease notes also suggest that ALTER TYPE .. RENAME TO and ADD CONSTRAINT .. UNIQUE .. USING INDEX are supported in 2.11.2. Let’s try to identify those using this method:\n1 2 3 grep parser_ybc_signal_unsupported gram-${YBDBV_OLD}.y \u003e out-parser_ybc_signal_unsupported-${YBDBV_OLD} grep parser_ybc_signal_unsupported gram-${YBDBV_NEW}.y \u003e out-parser_ybc_signal_unsupported-${YBDBV_NEW} diff out-parser_ybc_signal_unsupported-${YBDBV_OLD} out-parser_ybc_signal_unsupported-${YBDBV_NEW} gives:\n1 2 3 4 5 6 7 8 9d8 \u003c parser_ybc_signal_unsupported(@1, \"ALTER INDEX\", 1130); 60,62d58 \u003c parser_ybc_signal_unsupported(@1, \"CREATE MATERIALIZED VIEW\", 1131); \u003c parser_ybc_signal_unsupported(@1, \"CREATE MATERIALIZED VIEW\", 1131); \u003c parser_ybc_signal_unsupported(@1, \"REFRESH MATERIALIZED VIEW\", 1131); 97d92 \u003c parser_ybc_signal_unsupported(@1, \"ALTER TYPE\", 1893); There it is. Next to already established materialized views features, we can find new ALTER INDEX and ALTER TYPE. Let’s gain a little bit more context into, say, ALTER TYPE change.\nWe know that the feature has been added in 2.11.2, those lines do not exist in the output for 2.11.2 but exist in the output for 2.11.1. So we can do:\n1 2 3 4 5 grep 'parser_ybc_signal_unsupported(@1, \"ALTER TYPE\", 1893)' \\ -B 5 gram-${YBDBV_OLD}.y \u003e out-alter-type-${YBDBV_OLD} grep 'parser_ybc_signal_unsupported(@1, \"ALTER TYPE\", 1893)' \\ -B 5 gram-${YBDBV_NEW}.y \u003e out-alter-type-${YBDBV_NEW} diff out-alter-type-${YBDBV_OLD} out-alter-type-${YBDBV_NEW} gives:\n1 2 3 4 5 6 7 8 11,17d10 \u003c | ALTER TYPE_P any_name RENAME TO name \u003c { \u003c parser_ybc_signal_unsupported(@1, \"ALTER TYPE\", 1893); \u003c -- \u003c n-\u003emissing_ok = false; \u003c $$ = (Node *)n; \u003c } For some of those statements, it might be necessary to adjust the value of -B. In this case 5 was enough, we can see that the parser_ybc_signal_unsupported for ALTER TYPE_P any_name RENAME TO name disappeared from the grammar file in 2.11.2.\nWith little to no work, it’s possible to quickly compare the level of support for all three definitions across whatever arbitrary versions we are interested in.\nYugabyteDB roadmap ↩︎\nYugabyteDB Postgres YACC grammar ↩︎\nYugabyteDB 2.11.2 release notes ↩︎\n","description":"when the roadmap isn’t enough","tags":["yugabytedb"],"title":"Identifying Postgres features unsupported in YugabyteDB","uri":"/posts/2022-02-21-identifying-postgres-features-unsupported-in-yugabytedb/"},{"content":"","description":"","tags":null,"title":"envoy","uri":"/tags/envoy/"},{"content":"I wanted to look into Keycloak.X for quite a while. Keycloak.X is a lighter, faster, easier, more scalable, more cloud-native solution than the—now legacy—WildFly based Keycloak.\nKeycloak.X is now officially known as Keycloak 17.0.0, the first official Quarkus-based version. It’s been released a few days ago1 and so it was the right time to look at it.\nI’m going to show you how I run Keycloak 17.0.0 with TLS behind Envoy proxy with Docker Compose. I’ve blogged about Keycloak and Keycloak behind Envoy before so this article is a recap of some of the previous articles from this blog.\nLet’s go.\ndirectory structure Here’s what we are dealing with:\n[rad] keycloak-compose (keycloak-17) $ tree -a . . ├── .docker │ └── keycloak │ └── Dockerfile ├── compose.yml └── etc └── envoy └── envoy-keycloak.yaml Keycloak Dockerfile Quarkus-based Keycloak 17.0.0 cannot be started without executing the build step. From Keycloak documentation2:\nThe build command is responsible for producing an immutable and optimized server image, which is similar to building a container image. In addition to persisting any build option you have set, this command also performs a series of optimizations to deliver the best runtime when starting and running the server. As a result, a lot of processing that would usually happen when starting and running the server is no longer necessary and the server can start and run faster.\nThere is no way to get away from executing build. We can use the start –auto-build flag (also documented in 2), but one way or another, we have to build it. The –auto-build option takes some time to execute on every start and building a custom image helps us shave some time off on subsequent starts.\nI guessed that I wanted an optimized image so I opted in for building my own Docker image.\nBuilding a custom Docker image is documented here3.\nMy Dockerfile (.docker/keycloak/Dockerfile) looks like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 FROM quay.io/keycloak/keycloak:17.0.0 as builder ENV KC_METRICS_ENABLED=true ENV KC_FEATURES=token-exchange ENV KC_DB=postgres RUN /opt/keycloak/bin/kc.sh build FROM quay.io/keycloak/keycloak:17.0.0 COPY --from=builder /opt/keycloak/lib/quarkus/ /opt/keycloak/lib/quarkus/ WORKDIR /opt/keycloak ENV KEYCLOAK_ADMIN=admin ENV KEYCLOAK_ADMIN_PASSWORD=admin # change these values to point to a running postgres instance ENV KC_DB_URL=jdbc:postgresql://postgres:5432/keycloak?ssl=allow ENV KC_DB_USERNAME=keycloak ENV KC_DB_PASSWORD=keycloak ENV KC_HOSTNAME=idp-dev.gruchalski.com ENV KC_HOSTNAME_STRICT=false ENV KC_HTTP_ENABLED=true ENTRYPOINT [\"/opt/keycloak/bin/kc.sh\", \"start\"] Let’s go through some of its most interesting aspects.\nThe builder stage configures the database provider, here Postgres, and executes kc.sh build. This is where the optimized build is created.\nThe second stage uses the builder stage and simply copies the build output into the final container. If you have looked at the original Keycloak documentation3, you have probably noticed that I do not generate a certificate here. I am putting Keycloak behind Envoy, which will terminate TLS for me. I do not need a certificate.\nThe new thing in 17.0.0 is the use of KEYCLOAK_ADMIN and KEYCLOAK_ADMIN_PASSWORD environment variables. Prior to 17.0.0, to create an initial administrator account, we had to execute the /opt/jboss/keycloak/bin/add-user-keycloak.sh. script and pass -u user -p pass arguments. If you read some of my previous article, you maybe remember this command:\n1 2 3 4 5 docker exec dev_keycloak \\ /opt/jboss/keycloak/bin/add-user-keycloak.sh \\ -u admin \\ -p admin \\ \u0026\u0026 docker restart dev_keycloak A note of caution: in a real deployment, you’d not store those credentials directly in the Dockerfile. You’d pass them from outside. I kept them here for brevity only.\nThree other relevant notes here:\nKC_HOSTNAME: configures the hostname on which Keycloak is intended to be running, KC_HOSTNAME_STRICT: because I am putting Keycloak behind Envoy, I set this to false; in the Docker Compose setup, Envoy will be communicating with Keycloak using the keycloak hostname, this setting disables Keycloak hostname verification, KC_HTTP_ENABLED: is set to true because Envoy is terminating TLS, I don’t need TLS termination directly on Keycloak. Save the contents of the Dockerfile in .docker/keycloak/Dockerfile and build the Docker image:\n1 2 3 cd .docker/keycloak docker build -t local/keycloak:17.0.0 . cd - envoy configuration The etc/envoy/envoy-keycloak.yaml file is exactly the same as in my previous blog post4, except of the domain name.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 static_resources: listeners: - address: socket_address: address: 0.0.0.0 port_value: 443 listener_filters: - name: \"envoy.filters.listener.tls_inspector\" filter_chains: - filter_chain_match: server_names: - idp-dev.gruchalski.com filters: - name: envoy.filters.network.http_connection_manager typed_config: \"@type\": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager codec_type: AUTO stat_prefix: ingress_http route_config: name: local_route virtual_hosts: - name: keycloak domains: - \"*\" routes: - match: prefix: \"/\" route: cluster: proxy-domain1 http_filters: - name: envoy.filters.http.router transport_socket: name: envoy.transport_sockets.tls typed_config: \"@type\": type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.DownstreamTlsContext common_tls_context: tls_certificates: - certificate_chain: filename: /etc/envoy/certificates/idp-dev.gruchalski.com.crt private_key: filename: /etc/envoy/certificates/idp-dev.gruchalski.com.key clusters: - name: proxy-domain1 type: STRICT_DNS lb_policy: ROUND_ROBIN connect_timeout: 10s load_assignment: cluster_name: proxy-domain1 endpoints: - lb_endpoints: - endpoint: address: socket_address: address: keycloak port_value: 8080 This configuration will match any request to https://idp-dev.gruchalski.com, terminate TLS using a certificate and key from /etc/envoy/certificates/* and forward to proxy-domain1 cluster, which will forward the request to keycloak:8080, where keycloak is Keycloak’s hostname in the Docker Compose configuration.\ncertificates Keycloak is configured to run on idp-dev.gruchalski.com. As in, once running, I will be able to use Keycloak by going to https://idp-dev.gruchalski.com. In order to do so, I’m adding the following line to my /etc/hosts file:\n127.0.0.1 idp-dev.gruchalski.com I need certificates. Because I am using a regular browser, I preferably want to have certificates that are by default trusted by my operating system. The obvious choice is Let’s Encrypt. I have written before about using Let’s Encrypt certificates for local development5. Read that article to find out more.\nLong story short, my DNS is managed in Route 53 and I can use Route 53 API to automate the dns-01 challenge to obtain certificates for my local deployment. To do so, I’m using the LEGO client:\n1 2 3 4 5 6 7 8 9 10 11 12 cd etc/envoy docker run --rm \\ -v $(pwd):/lego \\ -v ${HOME}/.aws/credentials:/root/.aws/credentials \\ -e AWS_PROFILE=lego \\ -ti goacme/lego \\ --accept-tos \\ --domains=idp-dev.gruchalski.com \\ --server=https://acme-v02.api.letsencrypt.org/directory \\ --email=radek@gruchalski.com \\ --path=/lego \\ --dns=route53 run When the command finishes, the directory structure looks like this:\n[rad] keycloak-compose (keycloak-17) $ tree -a . . ├── .docker │ └── keycloak │ └── Dockerfile ├── compose.yml └── etc └── envoy ├── certificates │ ├── idp-dev.gruchalski.com.crt │ ├── idp-dev.gruchalski.com.issuer.crt │ ├── idp-dev.gruchalski.com.json │ └── idp-dev.gruchalski.com.key └── envoy-keycloak.yaml I’m ready to start the Compose setup.\ndocker compose My compose.yml looks like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 version: '3.9' networks: keycloak-internal: name: keycloak-internal keycloak-public: name: keycloak-public services: envoy: image: envoyproxy/envoy:v1.21.0 restart: unless-stopped command: /usr/local/bin/envoy -c /etc/envoy/envoy-keycloak.yaml -l debug ports: - 443:443 - 8001:8001 volumes: - type: bind source: ./etc/envoy target: /etc/envoy networks: - keycloak-internal - keycloak-public postgres: image: postgres:13.2 command: -c ssl=off restart: unless-stopped environment: POSTGRES_DB: keycloak POSTGRES_USER: keycloak POSTGRES_PASSWORD: keycloak networks: - keycloak-internal keycloak: depends_on: - postgres container_name: dev_keycloak image: local/keycloak:17.0.0 restart: unless-stopped networks: - keycloak-internal You can see that:\nI bind mount the etc/envoy directory in the Envoy container, this directory contains the envoy-keycloak.yaml file and the certificates I got from Let’s Encrypt, the Postgres database, username, and password match the values from the Dockerfile. Start everything with:\n1 docker compose -f compose.yml up that’s it Keycloak 17.0.0 running locally in Docker Compose, with TLS, behind Envoy proxy.\nNext step is to bring SPIs back into this setup!\nKeycloak 17.0.0 released ↩︎\nConfiguring Keycloak ↩︎ ↩︎\nRunning Keycloak in a container ↩︎ ↩︎\nKeycloak with TLS in Docker compose behind Envoy proxy ↩︎\nLet’s Encrypt certificates for local development ↩︎\n","description":"","tags":["keycloak","keycloak17","tls","envoy"],"title":"Keycloak 17.0.0 with TLS in Docker compose behind Envoy proxy","uri":"/posts/2022-02-20-keycloak-1700-with-tls-behind-envoy/"},{"content":"","description":"","tags":null,"title":"keycloak17","uri":"/tags/keycloak17/"},{"content":"","description":"","tags":null,"title":"protobuf","uri":"/tags/protobuf/"},{"content":"","description":"","tags":null,"title":"rpc","uri":"/tags/rpc/"},{"content":"The YugabyteDB RPC API isn’t an official API, and that’s what makes it interesting. The whole distributed aspect of YugabyteDB is based on that API. Digging through it is a perfect way to understand the internals of the database. The RPC API can also be used to automate various aspects of the database. I’ll come back to this subject in near future.\nAnother reason for investigating the RPC API was to have a lightweight version of yb-admin tools. I run everything in containers. When I want to connect to a cluster, I start another container with a client and connect with it to the cluster. The YugabyteDB Docker image isn’t tiny, in fact, 2.11.2.0-b89 is 1.77GB.\nOne point seventy seven gigabyte!\nI’m kinda not a fan of pulling in an image of such size just to execute list_all_masters. Neither I am a fan of doing docker exec in a production setting. I wanted to have a tool comparable to yb-admin but of a smaller footprint. And I wanted a library, an embeddable library to help automate the hell out of it.\nAnd that’s why some months ago I’ve written the very first version of the YugabyteDB client library for Go. Today, I’ve published the 0.0.2-beta.1 release. Here’s a brief overview of what it can do:\ngiven a list of master addresses, find the leader master and use that for further communication with the cluster handle leader change, auto-discovery of a new leader, handles redelivery auto-reconnect on connection loss, handles redelivery wraps API error types and exposes them as Go errors utilities to deal with various ID types and hybrid time automatically discovers available messages and services, generic execution API is completely type-driven client discovers the service and operation based on the request type configurable logging sink, via hclog.InterceptLogger configurable metrics sink The only feature unsupported feature are side cars but they will come.\nI’ve written more about side cars in my previous article.\nWhat’s the best way to sharpen your appetite? Code, obviously! So here’s how to start with this client:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 package main import ( \"encoding/json\" \"fmt\" \"github.com/hashicorp/go-hclog\" \"github.com/radekg/yugabyte-db-go-client/client\" \"github.com/radekg/yugabyte-db-go-client/configs\" clientErrors \"github.com/radekg/yugabyte-db-go-client/errors\" ybApi \"github.com/radekg/yugabyte-db-go-proto/v2/yb/api\" ) func main() { logger := hclog.Default() clientConfig := \u0026configs.YBClientConfig{ MasterHostPort: []string{\"127.0.0.1:7100\", \"127.0.0.2:7100\", \"127.0.0.3:7100\"}, } ybdbClient := client.NewYBClient(clientConfig).WithLogger(logger) if err := ybdbClient.Connect(); err != nil { logger.Error(\"could not initialize api client\", \"reason\", err) panic(err) } defer ybdbClient.Close() request := \u0026ybApi.ListMastersRequestPB{} response := \u0026ybApi.ListMastersResponsePB{} if err := ybdbClient.Execute(request, response); err != nil { if terr, ok := err.(*clientErrors.ServiceRPCError); ok { logger.Error(\"service RPC error\", \"reason\", terr.Error()) panic(err) } logger.Error(\"could not execute RPC call\", \"reason\", err.Error()) panic(err) } if err := clientErrors.NewMasterError(response.Error); err != nil { logger.Error(\"YugabyteDB master error\", \"reason\", err.Error()) panic(err) } jsonBytes, jsonErr := json.MarshalIndent(response, \"\", \" \") if jsonErr != nil { logger.Error(\"failed serializing response to JSON\", \"reason\", jsonErr.Error()) panic(jsonErr) } logger.Info(\"YugabyteDB masters\") fmt.Println(string(jsonBytes)) } Maybe you need to adjust the master addresses list in line 20.\nThe client comes with github.com/radekg/yugabyte-db-go-client/testutils/master and github.com/radekg/yugabyte-db-go-client/testutils/tserver packages. Those contain a bunch of dockertest based utilities for writing tests using containers. Simply write your integration test and have a cluster right there, when running go test (use some higher timeout, these need some time for the cluster to spin up).\nFor reference:\ntestutils/master/master_test.go: shows how to spin up a single master and run some tests against it testutils/tserver/tserver_test.go: starts a mini-cluster and goes as far as executing an SQL command against it. That’ll get you going.\nthere’s … more Being able to execute calls is one thing, knowing what to execute is a totally different beast.\nThere’s this little repository here: radekg/yugabyte-db-go-client-api. It contains a naive implementation of some of the yb-admin commands. That code is currently written against v0.0.1-beta.X version of the client (so 2.11.0.0 and 2.11.1.0 YugabyteDB) but it might be a good source of inspiration.\nThere’s a rewrite planned for that repository so best is not to invest too much time integrating that as a dependency in your own code.\nprotobuf The client depends on the radekg/yugabyte-db-go-proto library. That’s where the extracted *.proto files and generated Go code live. Have a look at the readme in that repository to find out more on how to generate your own code from YugabyteDB sources.\nThe radekg/yugabyte-db-go-proto library versioning follows the YugabyteDB versioning.\na couple of closing notes Client versions:\n0.0.1-beta.4 is the last version for YugabyteDB 2.11.0.0 and 2.11.1.0 0.0.2-beta.1 is targeted for YugabyteDB 2.11.2+, there are some protobuf changes which might not be exactly compatible with previous database versions, I don’t really know yet, haven’t tried it myself, but I doubt it’ll work Would I recommend using this in production? Yeah, why not. It’s used in production. But please keep in mind, there’s no warranty!\nIf you find a bug, feel free to submit a pull request.\ncommercial plug If your company is looking for someone who can help you migrate your database to the cloud and/or your company needs a solid partner who knows a thing or two about YugabyteDB, feel free to reach out to me at radek.gruchalski@klarrio.com.\n","description":"automate all the database things","tags":["yugabytedb","rpc","protobuf","golang"],"title":"YugabyteDB Go RPC client","uri":"/posts/2022-02-16-yugabytedb-go-rpc-client/"},{"content":"YugabyteDB is a horizontally scalable 100% open source distributed SQL database providing Cassandra (YCQL) and PostgreSQL (YSQL) compatible APIs. For PostgreSQL API, YugabyteDB uses the actual PostgreSQL 11.2 engine and builds on top of it with a RAFT consensus layer.\nThere are two components in the YugabyteDB architecture: master servers and tablet servers. Master servers know everything about the state of the cluster, they are like an address book of all objects and data stored in the database. Tablet servers store the data. Each tablet server runs an instance of PostgreSQL.\nThat was a very brief, very top level, simplified view of the architecture, but it should give a good image of what’s going on.\nMasters and tablet servers communicate using a protobuf (Google Protocol Buffers) API.\nLet’s locate all *.proto files (without test data):\n1 2 3 4 5 cd /tmp git clone https://github.com/yugabyte/yugabyte-db.git cd yugabyte-db/ git checkout v2.11.1 find . -name *.proto | grep -v test | sort ./ent/src/yb/master/master_backup.proto ./src/yb/cdc/cdc_consumer.proto ./src/yb/cdc/cdc_service.proto ./src/yb/common/common.proto ./src/yb/common/pgsql_protocol.proto ./src/yb/common/ql_protocol.proto ./src/yb/common/redis_protocol.proto ./src/yb/common/wire_protocol.proto ./src/yb/consensus/consensus.proto ./src/yb/consensus/log.proto ./src/yb/consensus/metadata.proto ./src/yb/docdb/docdb.proto ./src/yb/fs/fs.proto ./src/yb/master/master.proto ./src/yb/master/master_types.proto ./src/yb/rocksdb/db/version_edit.proto ./src/yb/rpc/rpc_header.proto ./src/yb/rpc/rpc_introspection.proto ./src/yb/server/server_base.proto ./src/yb/tablet/metadata.proto ./src/yb/tablet/tablet.proto ./src/yb/tserver/backup.proto ./src/yb/tserver/pg_client.proto ./src/yb/tserver/remote_bootstrap.proto ./src/yb/tserver/tserver.proto ./src/yb/tserver/tserver_admin.proto ./src/yb/tserver/tserver_forward_service.proto ./src/yb/tserver/tserver_service.proto ./src/yb/util/encryption.proto ./src/yb/util/histogram.proto ./src/yb/util/opid.proto ./src/yb/util/pb_util.proto ./src/yb/util/version_info.proto ./src/yb/yql/cql/cqlserver/cql_service.proto ./src/yb/yql/redis/redisserver/redis_service.proto From the above list it’s easy to roughly figure out YugabyteDB components:\nthe enterprise master backup service: it’s enterprise just in the name, this is fully open source and available under the Apache 2 license, just like any other YugabyteDB component, spoiler alert - this service provides snapshot features, change data capture (CDC) service, the consensus service, DocDB (YugabyteDB storage layer), filesystem service, master server service, tablet server service, CQL and Redis services, common bits and bobs, here we can find PSQL and Redis protocols, query layer protocol, and wire protocol, base RPC definitions and various utilities. Let’s peek at one of the files, for brevity I’m going to skip the output.\n1 cat ./src/yb/master/master.proto The output tells us the following:\nthe protocol is proto2 based, there are no non-standard protobuf features in use, they’re all regular messages and service definitions. the protocol What’s not clear from those proto files: YugabyteDB doesn’t use gRPC.\nInstead, YugabyteDB uses an RPC protocol coming from HBase. The choice is somewhat clear if we consider that the founder of Yugabyte, the company behind YugabyteDB, used to be the technical lead for HBase at Facebook.\nTo understand the wire format, we have to look at HBase RPC protocol. The HBase RPC protocol description can be found here1.\nThe protocol is pretty easy to underatand. It follows the regular request/response paradigm. The client initializes a TCP connection, then sends requests over an established connection. To every request, the client receives a response. The connection can be established over TLS.\nAfter opening a connection, the client sends a preamble and a connection header. In case of YugabyteDB, this is simply YB1. The server does not respond on successful connection setup. Once the connection is established, the client sends requests and receives responses.\nthe request Every request consists of a header and the paylod. The header is a protobuf message defined in rpc_header.proto:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 message RemoteMethodPB { // Service name for the RPC layer. // The client created a proxy with this service name. // Example: yb.rpc_test.CalculatorService required string service_name = 1; // Name of the RPC method. required string method_name = 2; }; // The header for the RPC request frame. message RequestHeader { // A sequence number that is sent back in the Response. Hadoop specifies a uint32 and // casts it to a signed int. That is counterintuitive, so we use an int32 instead. // Allowed values (inherited from Hadoop): // 0 through INT32_MAX: Regular RPC call IDs. // -2: Invalid call ID. // -3: Connection context call ID. // -33: SASL negotiation call ID. optional int32 call_id = 1; // RPC method being invoked. // Not used for \"connection setup\" calls. optional RemoteMethodPB remote_method = 2; // Propagate the timeout as specified by the user. Note that, since there is some // transit time between the client and server, if you wait exactly this amount of // time and then respond, you are likely to cause a timeout on the client. optional uint32 timeout_millis = 3; } To send the request, the client serializes the protobuf header and exactly one protobuf request messages. The client calculates the byte length of the header and a payload, and aggregates the total length. The following is sent over the socket:\nint32 total length, uint32 header length, protobuf serialized header, uint32 message length, protobuf serialized message. We can look up the relevant method in the YugabyteDB Java client, the code is here2. By looking at the Java code, we can imply that YugabyteDB never takes more than one message, it’s always one header and one request message.\nthe response The server replies with a response. The response might be arriving in chunks. The client is expected to read all chunks until receiving a EOF marker which is indicated by a byte array of 4 0 bytes.\nThe format of the response:\nint32 total data length, uint32 response header length, protobuf serialized response header, uint32 payload length, protobuf serialized response message. Before proceeding to read the assumed, expected response message, the client needs to look at the response header is_error property.\nLet’s look up the ResponseHeader definition in the rcp_header.proto:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 message ResponseHeader { required int32 call_id = 1; // If this is set, then this is an error response and the // response message will be of type ErrorStatusPB instead of // the expected response type. optional bool is_error = 2 [ default = false ]; // Byte offsets for side cars in the main body of the response message. // These offsets are counted AFTER the message header, i.e., offset 0 // is the first byte after the bytes for this protobuf. repeated uint32 sidecar_offsets = 3; } If the is_error flag is true, the response is an error response and the client must parse it as the ErrorStatusPB, defined also in rcp_header.proto.\nOtherwise, the response is the expected protobuf response object, as defined by the RPC service.\nFor example, the rpc ListMasters(ListMastersRequestPB) returns (ListMastersResponsePB); operation of the MasterService servide defined in the master.proto file takes:\na header a ListMastersRequestPB message and expects:\nListMastersResponsePB as a response If the RPC service was to return an error, instead of the ListMastersResponsePB message, it would return the ErrorStatusPB message.\nside cars In selected cases the response consists of so called side cars. We can identify which calls reply with side cars by searching for set_rows_data_sidecar in YugabyteDB code3. Currently only the Read and Write operations of the tserver_service.proto use them.\nEach side car is a separate protobuf message. We can find out how to read them by looking at this Java client code4.\nEvery request follows exactly the same semantics. Each new request implies a new request header. The request header contains the int32 call_id field. This is a sequence number of executed requests and it is returned in the response as int32 call_id.\nservices A request header contains the optional RemoteMethodPB remote_method field. This field indicates the target service type for which the request is destined. For example, the MasterService is yb.master.MasterService. This full name is the result of combing the package value (yb.master) and the service name (MasterService).\nIn versions later than 2.11.1, the service name can be optionally declared with a yb.rpc.custom_service_name service option. Here’s an example:\n1 2 3 service MasterCluster { option (yb.rpc.custom_service_name) = \"yb.master.MasterService\"; ... locating the leader master In most cases, we are communicating with the MasterService. When running YugabyteDB with replication factor greater than 1, at any given time, there is only one leader master, other masters are most often acting as followers. In this case, we have to send our requests to the leader master. If we don’t, the master will reply with a NOT_THE_LEADER error.\nFollower masters do not forward requests to the leader master. It’s the task of the client to locate and track the leader master.\nTo find the leader master, the client opens a connection to each master and sends a GetMasterRegistrationRequestPB message and receives a GetMasterRegistrationResponsePB message.\nThis message, found in master.proto, looks like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 message GetMasterRegistrationResponsePB { // Node instance information is always set. required NodeInstancePB instance_id = 1; // These fields are optional, as they won't be set if there's an // error retrieving the host/port information. optional ServerRegistrationPB registration = 2; // This server's role in the consensus configuration. optional consensus.RaftPeerPB.Role role = 3; // Set if there an error retrieving the registration information. optional MasterErrorPB error = 4; } The consensus.RaftPeerPB.Role field contains the role of the master. It is defined in the consensus/metadata.proto, under the RaftPeerPB message:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 // The possible roles for peers. enum Role { // Indicates this node is a follower in the configuration, i.e. that it participates // in majorities and accepts Consensus::Update() calls. FOLLOWER = 0; // Indicates this node is the current leader of the configuration, i.e. that it // participates in majorities and accepts Consensus::Append() calls. LEADER = 1; // New peers joining a quorum will be in this role for both PRE_VOTER and PRE_OBSERVER // while the tablet data is being remote bootstrapped. The peer does not participate // in starting elections or majorities. LEARNER = 2; // Indicates that this node is not a participant of the configuration, i.e. does // not accept Consensus::Update() or Consensus::Update() and cannot // participate in elections or majorities. This is usually the role of a node // that leaves the configuration. NON_PARTICIPANT = 3; // This peer is a read (async) replica and gets informed of the quorum write // activity and provides time-line consistent reads. READ_REPLICA = 4; UNKNOWN_ROLE = 7; }; After locating the leader master, the client can close connections to any non-leader master and continue communicating with a leader master only.\nYugabyteDB master leader can change at any time of the cluster lifecycle. This might be due to:\nthe operator requesting a leader change, due to an underlying infrastructure failure forcing the leader election. When this happens, a message sent to what we think is the current leader will either:\nnot go through (if leader is offline), will be rejected with a NOT_THE_LEADER error. The client should simply follow the same leader discovery procedure and retry the request.\nit’s undocumented and unofficial The RPC API isn’t documented and it’s not an official YugabyteDB API. It’s very powerful, there are very interesting things one can build with it but be advised:\nyou’re on your own, there really isn’t any documentation, figuring out the proper way to navigate through it means diving deep in the YugabyteDB source code, there are undocumented nuances to what can be called when and how, stepping out of the paved path can lead to trouble; hence: do not test random stuff on a live system with important data, you can break your cluster. A good reference on how to start working with can be found in the yb-admin tool sources:\nenterprise extensions regular operations Happy hacking!\nThe HBase RPC protocol description ↩︎\nYugabyteDB Java client payload handling ↩︎\nSourcegraph: usage of side cars in YugabyteDB code ↩︎\nReading side cars in YugabyteDB Java code ↩︎\n","description":"with great power comes great responsibility","tags":["yugabytedb","rpc","protobuf"],"title":"A brief look at YugabyteDB RPC API","uri":"/posts/2022-02-12-a-brief-look-at-yugabytedb-rpc-api/"},{"content":"","description":"","tags":null,"title":"writing","uri":"/tags/writing/"},{"content":"","description":"","tags":null,"title":"yugabyte","uri":"/tags/yugabyte/"},{"content":"I’ve been deep in YugabyteDB trenches for the last 6 months. At Klarrio, we are building a Database as a Service solution for the Data Services Hub. This is a self-managed, multi-tenant solution designed to run on top of Apache Mesos. We have a small team doing all kinds of integration work. I am focusing on the core database rollout.\nWorking with new technology, for me, often means going all in, deep into the darkness of the source code. I like to know how things work. It’s no different with YugbyteDB. I’m not the kind of guy who takes an upstream component and just runs it in production without understanding how it breaks. The best way to figure out how things work is to take them apart and put back together.\nIn the last 6 months, I’ve been doing all sorts of crazy things with YugabyteDB. To some, it might look like procrastinating but I truly believe that running a complex system in production without understanding how that system works is a very bad idea. And so, I’ve been doing things like:\nBuilding a number of Docker1, Mesos and Kubernetes based clusters, Running them at different sizes and in different configurations, Building my own Docker images with all sorts of bells and whistles included, Digging deep in the source code of YugabyteDB and PostgreSQL, First time ever writing PostgreSQL extensions and figuring out how to build YugabyteDB targeted extensions or soft PostgreSQL multi-tenancy, Contributed a couple of features: Foreign Data Wrapper2 shared_preload_libraries3 Building my own bench for working with YugabyetDB source code4 and built it from sources so many times, in my dreams, I sometimes hear all 44 cores of my lab server blasting out at full speed at 3am, Writing a bunch of tools5 for YugabyteDB, looked into monitoring, observability, logging, and operation of a cluster, Speaking at the Distributed SQL Summit 20216. I cannot thank the Yugabyte team enough for the help they have provided me along the way. They’ve been patient, supportive, professional. The depth of technical answers to my questions is outstanding.\nCredit also has to go to the client and my employer, who supported me on this journey.\nIt’s fair to say that I’ve learned a lot. But I am in no way an expert. You see, the nature of my work is to deliver up to spec projects to clients. I do R\u0026D, architectural work, implementation, full delivery: functional tests, operations and en user documentation, sometimes day two operations for a bit.\nBut once the project is finished, I move to another one. What I learn never goes to waste. To a certain extent what I learn will always be relevant in future projects. I often find myself building all sorts of crazy stuff with technology from previous projects. A false premise, building stuff I dream about becoming products but it always lacks focus and I am not really in a position to build a product today. I very much enjoy doing all that crazy lab work but let’s face it: that’s actually procrastinating.\nI learn a lot, I love it, it keeps me honest. But it is procrastinating. All that lab stuff ends up behind closed doors and rots away.\nWhile working on this huge YugabyteDB rollout, I could observe those same emotions again. What can I do differently this time? Around August an idea started growing in my head that maybe instead of building imaginary products, I should write a book.\nHave you ever seen the book from Jacek Laskowski on Spark internals7? This book describes each individual Spark internal component so one can really get to know how Spark works.\nSidetrack: back in 2015 I’ve spent good six months bending Spark in all sorts of ways. I made it work in Mesos with Docker bridge networking with the patched Akka build8 before it was all hip.\nSuddenly I was toying with the idea of writing something similar about YugabyteDB. And then, about a month ago, Jonathan from Apress reached out on LinkedIn asking if I would be interested in writing a book on YugabyteDB.\nThat gave it focus. The answer is: yes. Someone I used to work with, who also wrote a book, said:\nWriting a book takes you out of your comfort zone and forces you to look at every corner of the product, things you’d never touch in your daily work.\nSounds like a fantastic way to learn YugabyteDB inside out. And yes, I know there’s no money in writing technical books but money isn’t the driver for me here. For once to have continuity and contribute in a meaningful way—those are the drivers.\nAs I learned later, Jonathan also reached out to Franck Pachot—Developer Advocate at Yugabyte.\nSo I’ve spoken to Franck and we agreed to co-author a book. It’s all very early stages, the proposal is being written, priorities have to be taken care of, everybody has be aligned, everyone has to feel comfortable with choices being made. It can still go in every direction.\nStay tuned.\nYugabyteDB multi-tenant-paas ↩︎\n[YSQL] Foreign Data Wrapper support ↩︎\n[YSQL] Merge user provided shared_preload_libraries to enable custom PSQL extensions ↩︎\nYugabyteDB build infrastructure ↩︎\nYugabyteDB client for go ↩︎\nWorkload isolation in a multi-tenant container based architecture - DSS21 ↩︎\nThe Internals of Apache Spark 3.2.0 ↩︎\nApache Spark on Mesos with Docker bridge networking ↩︎\n","description":"I know there’s no money in writing technical books but money isn’t the driver for me here","tags":["yugabytedb","yugabyte","writing"],"title":"YugabyteDB: the book","uri":"/posts/2021-11-11-yugabytedb-the-book/"},{"content":"Mmmm, nearly missed it.\nYugabyteDB 2.9.1.0 was released on the 29th of October.\nSo here’s the thing. Back in August 2021, I contributed foreign data wrapper support to YugabyteDB, and 2.9.1.0 is the first beta release with this feature included. What I’m trying to say: postgres_fdw extension can be used in YugabyteDB starting with version 2.9.1.0.\nThe [YSQL] Foreign Data Wrapper support pull request contains all the interesting details but the bottom line is:\ncreate / alter / drop: foreign data wrapper create / alter / drop: server create / alter / drop: user mapping create / alter / drop: foreign table (with the caveat that not supported alter table features of YugabyteDB will also not work here) import foreign schema: this statement uses collate under the hood and requires YugabyteDB with ICU support to work out of the box Let’s give it a go.\nthe environment We will need a proper YugabyteDB cluster. This setup will have 3 masters and 9 TServers. The design of this cluster uses 3 always-on TServers and 2 groups of TServers, each with 3 TServers per tenant. In effect, we have:\na shared set of masters region: base1a: 3 TServers region: tenant1a: 3 TServers region: tenant2a: 3 TServers each TServer region has its own Envoy proxy in front I am going to use my reference Docker compose setup which is available here.\nDocker image Build the referenced Docker image:\n1 2 3 cd .docker/yugabyte-db/ docker build -t local/yugabyte:2.9.1.0-b140 . cd - And start the cluster:\n1 2 3 4 5 docker-compose --env-file \"$(pwd)/.env\" \\ -f compose-masters.yml \\ -f compose-tservers-shared.yml \\ -f compose-tservers-tenant1.yml \\ -f compose-tservers-tenant2.yml up This will most likely take some time to settle. On my lab server, this takes about 15 seconds.\nMind you, this cluster needs about 25GB RAM to operate rather reasonably. There are 12 containers, each reserving 2GB RAM and some Envoy proxies.\nwhat’s the plan The plan of action goes like this:\nas yugabyte user: configure two tenant databases, each database is owned by a respective tenant user, each tenant user has a tablespace assigned, as tenant2: create a table, as yugabyte: configure the foreign data wrapper for tenant1, as tenant1, create a foreign table and run some queries on it. setting things up All tooling for this setup is already within the repository. All commands should be executed from the directory where compose files live.\nsetup tenants The password for the yugabyte user is yugabyte (default). Passwords for those new accounts are the same as usernames: tenant1 and tenant2 respectively.\nExtension related errors can be ignored.\n1 2 3 4 5 docker run --rm \\ --net=yb-dbnet \\ -v \"$(pwd)/sql-init-tenant1.sql:/init.sql\" \\ -ti postgres:11.2 \\ bash -c 'psql \"host=envoy-yb-shared port=35432 user=yugabyte dbname=yugabyte\" -f /init.sql' Password for user yugabyte: CREATE ROLE CREATE DATABASE CREATE TABLESPACE REVOKE REVOKE GRANT ALTER ROLE ALTER ROLE You are now connected to database \"tenant1db\" as user \"yugabyte\". psql:/init.sql:12: ERROR: could not open extension control file ... 1 2 3 4 5 docker run --rm \\ --net=yb-dbnet \\ -v \"$(pwd)/sql-init-tenant2.sql:/init.sql\" \\ -ti postgres:11.2 \\ bash -c 'psql \"host=envoy-yb-shared port=35432 user=yugabyte dbname=yugabyte\" -f /init.sql' Password for user yugabyte: CREATE ROLE CREATE DATABASE CREATE TABLESPACE REVOKE REVOKE GRANT ALTER ROLE ALTER ROLE You are now connected to database \"tenant1db\" as user \"yugabyte\". psql:/init.sql:12: ERROR: could not open extension control file ... create tenant2 table Connect as tenant2:\n1 2 3 4 docker run --rm \\ --net=yb-dbnet \\ -ti postgres:11.2 \\ bash -c 'psql \"host=envoy-yb-tenant2 port=35432 user=tenant2 dbname=tenant2db\"' Password for user tenant2: psql (11.2 (Debian 11.2-1.pgdg90+1)) Type \"help\" for help. tenant2db=\u003e And create a table:\n1 create table sharedtableexample (rowid int, rowval text) split into 3 tablets; CREATE TABLE Close the connection:\n1 \\q I’ve noticed that all-lower-case names are the easiest to work with.\nconfigure tenant1 foreign data wrapper Technically, it does not matter which Envoy proxy is used for this operation, as long as it is executed as the yugabyte user:\n1 2 3 4 docker run --rm \\ --net=yb-dbnet \\ -ti postgres:11.2 \\ bash -c 'psql \"host=envoy-yb-shared port=35432 user=yugabyte dbname=tenant1db\"' Password for user yugabyte: psql (11.2 (Debian 11.2-1.pgdg90+1)) Type \"help\" for help. tenant1db=# Run these commands to create the extension, create a server and setup use mapping with permissions:\n1 2 3 4 5 6 7 8 9 10 create extension postgres_fdw; create server fdw_tenant2shares foreign data wrapper postgres_fdw options ( host 'envoy-yb-tenant2', port '35432', dbname 'tenant2db'); create user mapping for tenant1 server fdw_tenant2shares options ( user 'tenant2', password 'tenant2'); grant usage on foreign server fdw_tenant2shares to tenant1; \\q postgres_fdw extension needs to be created within the tenant database so we have connected directly to the target database. Alternatively, I could have connected to the yugabyte database and used the \\connect tenant1db command.\ncreate foreign table as tenant1 As tenant1:\n1 2 3 4 docker run --rm \\ --net=yb-dbnet \\ -ti postgres:11.2 \\ bash -c 'psql \"host=envoy-yb-tenant1 port=35432 user=tenant1 dbname=tenant1db\"' Password for user tenant1: psql (11.2 (Debian 11.2-1.pgdg90+1)) Type \"help\" for help. tenant1db=\u003e List existing foreign servers and create the foreign table:\n1 2 3 4 5 6 \\des+ create foreign table sharedtableexample_foreign ( rowid integer options (column_name 'rowid'), rowval text options (column_name 'rowval') ) server fdw_tenant2shares options ( schema_name 'public', table_name 'sharedtableexample'); run some queries tenant1 can now insert data to the foreign table:\n1 2 3 4 insert into sharedtableexample_foreign (rowid, rowval) values (1, 'hello, world!'); insert into sharedtableexample_foreign (rowid, rowval) values (2, 'hello, world!'); insert into sharedtableexample_foreign (rowid, rowval) values (3, 'hello, world!'); \\q verify Connect back to tenant2db as tenant2:\n1 2 3 4 docker run --rm \\ --net=yb-dbnet \\ -ti postgres:11.2 \\ bash -c 'psql \"host=envoy-yb-tenant2 port=35432 user=tenant2 dbname=tenant2db\"' Password for user tenant2: psql (11.2 (Debian 11.2-1.pgdg90+1)) Type \"help\" for help. tenant2db=\u003e And select the data from the original table:\n1 select * from sharedtableexample order by rowid; All records are in:\nrowid | rowval -------+--------------- 1 | hello, world! 2 | hello, world! 3 | hello, world! (3 rows) voila! The beauty of YugabyteDB. Because it is PostgreSQL under the hood, postgres_fdw is simply working.\n","description":"a brief intro to postgres_fdw with YugabyteDB","tags":["yugabytedb","postgres"],"title":"YugabyteDB: Postgres foreign data wrapper","uri":"/posts/2021-11-08-yugabytedb-postgres-foreign-data-wrapper/"},{"content":"Yes, it’s perfectly fine to run databases in containers. The only challenge is to make sure that the data stored by the database does not reside within the file system of the container. Otherwise, after removing the container, the data will be gone, too.\nbasic docker Let’s have a look at the most basic example from the Postgres Docker Hub:\n1 2 3 4 docker run \\ --name some-postgres \\ -e POSTGRES_PASSWORD=mysecretpassword \\ -d postgres The problem here is, once we stop working with this container, if we do docker stop some-postgres \u0026\u0026 docker rm some-postgres, the data stored in the database will be gone because the default Postgres data directory resides within the container file system.\nTo prevent this from happening, the Postgres container can be configured with a volume mapped to the host directory. For example, to store the data in the /tmp/postgres-data directory of the host, the container can be started like this:\n1 2 3 4 5 6 docker run -d \\ --name some-postgres \\ -e POSTGRES_PASSWORD=mysecretpassword \\ -e PGDATA=/var/lib/postgresql/data/pgdata \\ -v /tmp/postgres-data:/var/lib/postgresql/data \\ postgres The -v, or long –volume, option has the form of [host path]:[container path]. The PGDATA environment variable tells the docker-entrypoint.sh to configure the pgdata directory with its value. At the same time, the container starts with that directory mapped from the host.\nThe -v option can be specified multiple times to map different directories or files. The option is documented here.\ndocker compose When working with Docker Compose, the best option is to use a bind volume mount. The simplest example would be:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 version: '3.3' services: postgres: image: postgres:13.2 restart: unless-stopped environment: POSTGRES_DB: mydb POSTGRES_USER: myuser POSTGRES_PASSWORD: mysecretpassword PGDATA: /var/lib/postgresql/data/pgdata ports: - \"5432:5432\" volumes: - type: bind source: /tmp/postgres-data target: /var/lib/postgresql/data networks: - reference networks: reference: running in the cloud When running containers in a cloud environment, say on an EC2 instance in AWS, the preferred way is to use an EBS like volume (block storage) as the host volume for the container host path location. This will ensure data survivability across container instances and VM instances, as long as the EBS volume with the host directory is attached to the expected host running the Docker container.\n","description":"keep the postgres data across container restarts","tags":["docker","postgres"],"title":"Postgres in Docker with persistent storage","uri":"/posts/2021-07-12-postgres-in-docker-with-persistent-storage/"},{"content":"The default YugabyteDB Docker image from Docker Hub runs the database as a root user.\nI need to run it as a non-root user and there is no release Docker image Dockerfile available in YugabyteDB repositories.\nSo I’ve created my own and here it is.\nTo build the image, run this command:\n1 2 3 4 curl --silent https://gist.githubusercontent.com/radekg/3f749cba86e91a8c88eb0e88c8b8754c/raw \u003e Dockerfile docker build -t yb-test:latest . ... =\u003e =\u003e naming to docker.io/library/yb-test:latest Start the container:\n1 docker run --rm -ti yb-test:latest bash Run this in the the container to start the cluster:\n1 [myybuser@0fb8cfa7c3d0 /]$ yugabyted start The output:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 Starting yugabyted... ✅ System checks +--------------------------------------------------------------------------------------------------+ | yugabyted | +--------------------------------------------------------------------------------------------------+ | Status : Running. Leader Master is present | | Web console : http://127.0.0.1:7000 | | JDBC : jdbc:postgresql://127.0.0.1:5433/yugabyte?user=yugabyte\u0026password=yugabyte | | YSQL : bin/ysqlsh -U yugabyte -d yugabyte | | YCQL : bin/ycqlsh -u cassandra | | Data Dir : /home/myybuser/var/data | | Log Dir : /home/myybuser/var/logs | | Universe UUID : c9e704e3-ff8b-46df-8921-6c5bbd6de2f8 | +--------------------------------------------------------------------------------------------------+ 🚀 yugabyted started successfully! To load a sample dataset, try 'yugabyted demo'. 🎉 Join us on Slack at https://www.yugabyte.com/slack 👕 Claim your free t-shirt at https://www.yugabyte.com/community-rewards/ That’s about it.\n","description":"create a YugabyteDB Docker image with custom uid / gid","tags":["yugabytedb"],"title":"YugabyteDB Docker image","uri":"/posts/2021-06-15-yugabytedb-docker-image/"},{"content":"After some insightful weeks of diving into the Ory platform, I am reverting back to Keycloak to investigate some other of its interesting features. The last few weeks spent in the Ory-land were enlightening.\nOne of my previous post, Introduction to Keycloak Authorization Services1, gets about 20 daily reads but authorization services isn’t the only awesome thing about Keycloak.\nservice provider interfaces Keycloak’s extensibility is what absolutely blows my mind. Keycloak defines a number of service provider interfaces (SPI)2 which allow the developer to tap into and add completely new functionality. Authorization is a bit like CRM or ERP, every organization has their own quirks.\nSPI implementations are written in Java—any language targeting the JVM, really—and deployed to the Keycloak instance by placing them in a predefined directory.\nThe SPI deployment supports hot reloading, SPIs can be updated while the server is running.\ntesting the waters I’m going to start this little series by looking at a required action provider. A required action provider hooks into the registration and login process and executes an action which allows amending the flow and conditionally allow or reject the login attempt.\nThe question: how difficult would it be to implement the following scenario:\nThe user registers via self service. The user is not allowed to log in via self service until a specific attribute is set; this attribute would be set by an administrative operator; effectively requires a registration approval. If the user account does not have the required attribute, the user is forwarded to an information page outside of Keycloak. I’m not implementing the actual approval process because this is way out of scope of this article.\nthe Java code Any Keycloak provider requires, at minimum, two implementation classes:\nA provider factory: a class implementing a specific SPI factory interface. An implementation of the SPI initialized by the previously written factory. The required action provider is one of the simplest SPIs available, but it does allow for some pretty wild implementations. For example, the out of the box WebAuthn support is one of those.\nHere, the User must be approved factory, is pretty straightforward:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 package com.gruchalski.idp.spi.actions; import org.keycloak.Config; import org.keycloak.authentication.RequiredActionFactory; import org.keycloak.authentication.RequiredActionProvider; import org.keycloak.models.KeycloakSession; import org.keycloak.models.KeycloakSessionFactory; public class UserMustBeApprovedActionFactory implements RequiredActionFactory { private static final UserMustBeApprovedAction SINGLETON = new UserMustBeApprovedAction(); @Override public RequiredActionProvider create(KeycloakSession session) { return SINGLETON; } @Override public void init(Config.Scope scope) {} @Override public void postInit(KeycloakSessionFactory keycloakSessionFactory) {} @Override public void close() {} @Override public String getId() { return UserMustBeApprovedAction.PROVIDER_ID; } @Override public String getDisplayText() { return \"User must be approved\"; } } This class implements the org.keycloak.authentication.RequiredActionFactory and creates a static action instance. The create, init and postInit methods are pretty neat because they allow us to configure the action based on whatever the Keycloak status is. They enable full integration with Keycloak runtime. This pattern exists across all of the Keycloak SPIs.\nThe actual action looks like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 package com.gruchalski.idp.spi.actions; import org.keycloak.authentication.RequiredActionContext; import org.keycloak.authentication.RequiredActionProvider; public class UserMustBeApprovedAction implements RequiredActionProvider { public static String PROVIDER_ID = \"USER_MUST_BE_APPROVED\"; @Override public void evaluateTriggers(RequiredActionContext requiredActionContext) {} @Override public void requiredActionChallenge(RequiredActionContext requiredActionContext) { if (requiredActionContext .getUser() .getAttributes() .containsKey(\"x-approved\")) { requiredActionContext.success(); } else { requiredActionContext .getAuthenticationSession() .setRedirectUri(\"https://account.gruchalski.com/errors/approval-required/\"); requiredActionContext.failure(); } } @Override public void processAction(RequiredActionContext requiredActionContext) {} @Override public void close() {} } The requiredActionChallenge(RequiredActionContext) method is where the action happens. This method is called when the user enters the User must be approved login step; usually after submitting the login form. The requiredActionContext argument provides a number of interesting methods. For example, we can create forms with arbitrary fields asking the user for additional input. If we did that, we could process that input in the processAction(RequiredActionContext) method.\nThis action, however, is a binary decision - the user either has or does not have the attribute assigned. We can inspect the logging in user by looking up various details using the requiredActionContext.getUser() method. Here, the program checks whether the user account has the x-approved. If yes, the context is approved using the success() method. Otherwise, the request is redirected to an arbitrary URI which could provide detailed explanation of the reason.\nTo have this action available in Keycloak, a third file is required. The file is called org.keycloak.authentication.RequiredActionFactory and must be placed in resources/META-INF/services directory of the Java project. The content is simply:\ncom.gruchalski.idp.spi.actions.UserMustBeApprovedActionFactory deployment Regardless on your Java packaging tool of choice, the outcome is a jar file containing the compiled classes and the resources directory. The jar file must be copied to the Keycloak /opt/jboss/keycloak/standalone/deployments directory. Give Keycloak a couple of seconds to load the classes, the status will be printed in the logs.\nenabling and testing Sign in to Keycloak and navigate to the realm Authentication (left menu) / Required Actions tab. Click the Register button in top left table corner and select the User must be approved action from the list. Click Ok. The action will be added to the list.\nIf you want to enforce this action for every user in the realm, tick the Default action checkbox. Be careful, if you do this, make sure your current user you are logged in as, has the x-approved attribute assigned - otherwise you will lock yourself out! If in doubt, test on a temporary realm or on a Keycloak instance you can easily wipe.\nThe action can be enforced for individual users only. To do so, go to realm Users, find the account for which the action should be enforced and select it in the Required User Actions. Save the changes.\nThat’s it. Now, when the user tries to sign in and there is no required attribute, they will be redirected to the information page. This action also takes effect when using the direct password grant. Without the attribute, the response is:\n1 2 3 4 { \"error\": \"invalid_grant\", \"error_description\": \"Account is not fully set up\" } In one of the future posts, I am going to look at adding custom forms to the action.\nIntroduction to Keycloak Authorization Services ↩︎\nKeycloak Service Provider Interfaces ↩︎\n","description":"","tags":["keycloak"],"title":"Extending Keycloak—required actions: user must be approved","uri":"/posts/2021-06-06-extending-keycloak-required-action-providers/"},{"content":" 20th of February, 2022:\nI have published a version of this article adapted for Keycloak 17: Keycloak 17.0.0 with TLS in Docker compose behind Envoy proxy.\nThe 24 hours of Nürburgring race was just red flagged for the remainder of the night due to the fog. That’s a perfect opportunity to add TLS to my Keycloak Docker Compose setup described previously here1.\nThere are multiple ways of setting up TLS for Keycloak, one of them being the native Java JKS key store / trust store gymnastics.\nWell, that’s certainly a way to go. If you prefer that path, feel free to do so, details are here2 but that’s a lot of work. I like my life simple so I choose to use a proxy to terminate TLS instead.\nLet’s have a look at the original compose.yml file, it’s short:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 version: '3.9' services: postgres: image: postgres:13.2 restart: unless-stopped environment: POSTGRES_DB: ${POSTGRESQL_DB} POSTGRES_USER: ${POSTGRESQL_USER} POSTGRES_PASSWORD: ${POSTGRESQL_PASS} networks: - local-keycloak keycloak: depends_on: - postgres container_name: local_keycloak environment: DB_VENDOR: postgres DB_ADDR: postgres DB_DATABASE: ${POSTGRESQL_DB} DB_USER: ${POSTGRESQL_USER} DB_PASSWORD: ${POSTGRESQL_PASS} image: jboss/keycloak:${KEYCLOAK_VERSION} ports: - \"28080:8080\" restart: unless-stopped networks: - local-keycloak networks: local-keycloak: To enable the proxy with TLS support, let’s modify the yaml file to this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 version: '3.9' services: envoy: image: envoyproxy/envoy:v1.18.2 restart: unless-stopped command: /usr/local/bin/envoy -c /etc/envoy/envoy-keycloak.yaml -l debug ports: - 443:443 - 8001:8001 volumes: - type: bind source: ./etc/envoy target: /etc/envoy networks: - local-keycloak postgres: image: postgres:13.2 restart: unless-stopped environment: POSTGRES_DB: ${POSTGRESQL_DB} POSTGRES_USER: ${POSTGRESQL_USER} POSTGRES_PASSWORD: ${POSTGRESQL_PASS} networks: - local-keycloak keycloak: depends_on: - envoy - postgres container_name: local_keycloak environment: DB_VENDOR: postgres DB_ADDR: postgres DB_DATABASE: ${POSTGRESQL_DB} DB_USER: ${POSTGRESQL_USER} DB_PASSWORD: ${POSTGRESQL_PASS} PROXY_ADDRESS_FORWARDING: \"true\" image: jboss/keycloak:${KEYCLOAK_VERSION} restart: unless-stopped networks: - local-keycloak networks: local-keycloak: There are four differences in the new file:\nthere is a new envoy service the keycloak service additionally depends on the envoy service the keycloak service no longer exposes the 28080 port on the host there is a new environment variable defined for the keycloak service: PROXY_ADDRESS_FORWARDING: \"true\" envoy configuration Looking closely at the envoy service, we can spot the host ./etc/envoy to container /etc/envoy volume bind. The proxy command references the /etc/envoy/envoy-keycloak.yaml configuration file. The file must have the yaml extension, yml is not going to work. The content is:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 static_resources: listeners: - address: socket_address: address: 0.0.0.0 port_value: 443 listener_filters: - name: \"envoy.filters.listener.tls_inspector\" filter_chains: - filter_chain_match: server_names: - idp.gruchalski.com filters: - name: envoy.filters.network.http_connection_manager typed_config: \"@type\": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager codec_type: AUTO stat_prefix: ingress_http route_config: name: local_route virtual_hosts: - name: keycloak domains: - \"*\" routes: - match: prefix: \"/\" route: cluster: proxy-domain1 http_filters: - name: envoy.filters.http.router transport_socket: name: envoy.transport_sockets.tls typed_config: \"@type\": type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.DownstreamTlsContext common_tls_context: tls_certificates: - certificate_chain: filename: /etc/envoy/certificates/idp.gruchalski.com.crt private_key: filename: /etc/envoy/certificates/idp.gruchalski.com.key clusters: - name: proxy-domain1 type: STRICT_DNS lb_policy: ROUND_ROBIN connect_timeout: 10s load_assignment: cluster_name: proxy-domain1 endpoints: - lb_endpoints: - endpoint: address: socket_address: address: keycloak port_value: 8080 The directory structure looks like this:\n. ├── compose.yml └── etc └── envoy └── envoy-keycloak.yaml You might ask what is this config file doing so let’s look at it from top to bottom.\nFirst, we define a listener bound to 0.0.0.0 on port 443 - standard HTTPS stuff. Next, we create a filter chain matching the idp.gruchalski.com domain name - this is the TLS SNI matching. The TLS SNI implies that our service, here Keycloak, will be accessed over HTTPS only and hostname advertised during the TLS handshake is used to find the upstream (cluster) target to forward the traffic to. As you can probably already imagine, I will be accessing Keycloak via https://idp.gruchalski.com. The connections matching the filtered domain will be forwarded to the proxy-domain1 cluster via the http_filter. The cluster forwards the requests to the load balancer endpoints, in this, we have one at keycloak:8080. This is the name of the container on Docker network used for this setup. The part I’ve glossed over is the transport_socket.common_tls_context.tls_certificates. It points at the TLS certificate and key used for the filter for the domain.\nOkay, a couple of caveats:\nwe don’t have the certificate yet how do we access Keycloak using the domain name when it is running in local compose the domain name Easy, modify the /etc/hosts file by adding:\n127.0.0.1 idp.gruchalski.com certificates This isn’t a rocket science either. In your case, you probably already have a domain name you want to use instead of idp.gruchalski.com so replace all occurences with your own domain in configs above and commands below.\nBecause Keycloak is used in the browser, we want real TLS certificates from one of the public trusted certificate authorities. Let’s Encrypt is for sure an awesome choice. We can get the LE certficites in multiple ways but at the core, we either need the control over the DNS for the dns-01 challenge or we need a http/https server reachable via the domain names for which the certificates should be issued. More about LE challenge types3.\nLong story short, as I am requesting the certifciates for the local compose setup, the http-01 and tls-alpn-01 challenges are not an option because Let’s Encrypt will not be able to call back to a server running on my local machine.\nThe dns-01 challenge is the way to go but it requires having an administrative control over the DNS server so the required TXT records can be created to complete the LE challenge. I have that, I use AWS Route 53 as my DNS of choice.\nA couple of days ago, I have written about the LEGO client which I used for obtaining the certificates4. Here, I’d use the following command:\n1 2 3 4 5 6 7 8 9 10 11 12 cd etc/envoy docker run --rm \\ -v $(pwd):/lego \\ -v ${HOME}/.aws/credentials:/root/.aws/credentials \\ -e AWS_PROFILE=lego \\ -ti goacme/lego \\ --accept-tos \\ --domains=idp.gruchalski.com \\ --server=https://acme-v02.api.letsencrypt.org/directory \\ --email=radek@gruchalski.com \\ --path=/lego \\ --dns=route53 run As a result, my file structure now looks like this:\n. ├── compose.yml └── etc └── envoy ├── accounts │ └── acme-v02.api.letsencrypt.org │ └── radek@gruchalski.com │ ├── account.json │ └── keys │ └── radek@gruchalski.com.key ├── certificates │ ├── idp.gruchalski.com.crt │ ├── idp.gruchalski.com.issuer.crt │ ├── idp.gruchalski.com.json │ └── idp.gruchalski.com.key └── envoy-keycloak.yaml The configuration is now complete.\nAfter starting the setup with docker compose -f compose.yml up, I can access my Keycloak by entering https://idp.gruchalski.com in the browser address bar. The TLS request is terminated at Envoy and Envoy finds the cluster based on the hostname advertised during the TLS handshake. The request is then forwarded to Keycloak on port 8080.\nInstalltion finalization is exactly the same as in the previous article.\nKeycloak with Docker Compose ↩︎\nKeycloak documentation: setting up SSL ↩︎\nLet’s Encrypt challenge types ↩︎\nLet’s Encrypt certificates for local development ↩︎\n","description":"","tags":["keycloak","tls","envoy"],"title":"Keycloak with TLS in Docker compose behind Envoy proxy","uri":"/posts/2021-06-06-keycloak-with-tls-behind-envoy/"},{"content":"While building a couple of browser based prototypes, I’ve hit an interesting problem. Basically, I am trying to replicate a full remote setup with a reverse proxy and TLS SNI while running everything on localhost. Getting the DNS functioning is pretty easy—I just add the required hosts to the /etc/hosts file and I’m done with it.\nHowever, I still need actual certificates trusted by the browser. In 2021, Let’s Encrypt is the way to go. Turns out, this is also pretty easy to do. There’s a Let’s Encrypt client and ACME library written in Go1 which can be used via Docker on any operating system.\nHere’s how to use it with AWS Route53:\n1 2 3 4 5 6 7 8 9 10 11 12 13 mkdir -p /tmp/certs \u0026\u0026 cd /tmp/certs docker run --rm \\ -v $(pwd):/lego \\ -v ${HOME}/.aws/credentials:/root/.aws/credentials \\ -e AWS_PROFILE=lego \\ -ti goacme/lego \\ --accept-tos \\ --domains=subdomain1.example.com \\ --domains=subdomain2.example.com \\ --server=https://acme-staging-v02.api.letsencrypt.org/directory \\ --email=info@example.com \\ --path=/lego \\ --dns=route53 run This command starts a Docker container using thr latest LEGO Docker image and requests a certificate for two subdomains. It mounts two volumes:\ncurrent working directory as /lego in the container, this is where the retrieved data will land ${HOME}/.aws/credentials as /root/.aws/credentials The command assumes that AWS profiles are used and there is a profile named lego. The profile must have the following IAM permissions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"\", \"Effect\": \"Allow\", \"Action\": [ \"route53:GetChange\", \"route53:ChangeResourceRecordSets\", \"route53:ListResourceRecordSets\" ], \"Resource\": [ \"arn:aws:route53:::hostedzone/*\", \"arn:aws:route53:::change/*\" ] }, { \"Sid\": \"\", \"Effect\": \"Allow\", \"Action\": \"route53:ListHostedZonesByName\", \"Resource\": \"*\" } ] } As I have not specified the hosted zone ID in the command, LEGO client will figure it out from the --domains flags. The output directory is configured with the --path flag pointing to the first volume target.\nTo fetch the real production certificate, use https://acme-v02.api.letsencrypt.org/directory as the --server value.\nPretty neat. I now have LE real certificates without messing with remote HTTP servers.\nFull documentation is available here2.\nuse as a library The LEGO client can be used as a library in any go program. Here’s a repository3 wrapping the library in a more high level client. Very decent sample can be found in the examples folder.\nLet’s Encrypt client and ACME library written in Go ↩︎\nLEGO documentation ↩︎\neggsampler/acme ↩︎\n","description":"LEGO client with Docker and AWS Route53 DNS","tags":["letsencrypt","tls"],"title":"Let’s Encrypt certificates for local development","uri":"/posts/2021-06-04-letsencrypt-certificates-for-local-development/"},{"content":"Oh boy, 20 years of software engineering definitely makes you tune your BS radar when reading claims about how awesome and mature a relatively new technology is. Every new project claims to solve all the problems for the current itch on hand. Do you know that feeling? You spend a week reading the documentation, prepare the test deployment, find out that documentation doesn’t match reality, look at GitHub and find dozens open issues. Finally, you throw it out in anger and curse at yourself because you just wasted a week of time.\nRecently, I’ve been on a hunt for a PostgreSQL (optionally distributed) compatible database for one of the work tasks. It’s an awesome task because it forced me to look deep into the current database ecosystem. Other than occasional Postgres Docker image pull, I haven’t touched a traditional SQL database since 2012. This exercise is a good opportunity to figure out what’s new in the relational databases world since my last experiences with Microsoft SQL Server 2008, which I enjoyed very much!\nLooking at just PostgreSQL was much fun. PostgreSQL is simply amazing. Fast, just works, JSON operations included and the grant system is outstanding. Permissions can be managed on a database, tablespace, schema, column and row level. With tablespaces, I can tell Postgres where the particular bit of data resides on disk. It’s a solid, ubiquitous database available in every cloud and very easy to deploy on premise. The tooling is mature, the operations are pretty well understood. Some claim that setting up replication and automatic failover can be a pita. I don’t know, I haven’t had a chance to go that far yet. However, when I was contracting for Saywell International back in 2010, we were running running an active-active MSSQL setup with log shipping over Dropbox. Crazy idea but it worked and was a lot of fun, ha! 2021 Postgres cannot be more difficult than that.\nPostgreSQL is an ACID1 (atomic, consistent, isolated, durable) RDBMS (relational database management system). There’s a catch with RDBMS. Data must fit on a single server. If the data outgrows a server, it can be manually shareded (partitioned) or moved to a bigger server. Well, we can can get pretty big servers today, for sure. 256BG RAM? 44 cores? 12TB disks? Sure. But when that bad boy has a hiccup, things go bad fast. The cost of these servers, even in the cloud, is mind-boggling. Triple that when you’re in need of replicas.\nA database article without mentioning the CAP theorem2 is not a database article. The CAP theorem describes the three major properties of a database system: consistency, availability and partition tolerance. The CAP theorem also says that it is not possible to have all three properties fulfilled by one system, choose any of the two properties but all three is not possible. An RDMBS satisfies the C and A properties: data is consistent and available but not partition tolerant.\nThe last few years, and especially the last couple of years, has seen some really interesting new developments in the database space. New SQL databases are popping up left and right and many of them attempt to crack the hard problem of distributed querying, distributed transactions and data replication. This is an interesting development, most likely, rooted in NoSQL (not only SQL). Databases like Cassandra or Riak brought the concept of Amazon’s Dynamo to the masses. These new databases took advantage of and introduced many new concepts. Consistent hashing3 and distributed hash tables4 (DHT) for data storage being probably most crucial ones.\nDHTs made the automatic partitioning possible. Consistent hashing makes it possible to identify the location of the partition the data belongs to within the cluster with O(1) complexity. These new databases replicate the data within the cluster. Data is available, a loss of a node does not imply service interruption. These databases fulfill the A and P properties of CAP.\nOn a hand-wavy, high conceptual level, the storage mechanism wasn’t different than RDBMS sharding with manual replication configuration. However, with regular RDBMS, it isn’t possible to treat a sharded table as one. Querying across shards is not straightforward, transactions across shards are not possible.\nSolving one problem often implies introducing another. These new databases are in fact key/value stores with an SQL layer on top. There are no transactions, there are no joins, distributed consistent counters are difficult. Due to the nature of replication, the replicas are eventually consistent5; it takes time for the replica to catch up with the main partition (hence no C property). Using databases like Cassandra or Riak implied plenty of write overhead. For performance reasons, it’s often better to store the data in the format intended for reading. It’s a common pattern to write the data multiple multiple times to different tables, depending on the context it would be used when querying. Capacity planning is difficult, what sits where in the database is difficult to track by just looking at the schema. There are no foreign keys, constraints are not not always possible. Plenty of the database logic is shifted to the application layer.\nAs the world was figuring out NoSQL, on the other side of the spectrum, the smart folks at Google and Amazon had a crack at the number one RDBMS problem. How to make a global, scalable, distributed, highly available ACID database a reality. The result of that work is—respectively—Google Spanner and Amazon Aurora. Both are available, consistent, isolated and durable global scale, distributed SQL databases featuring distributed transactions, joins and majority of the niceties of an RDBMS. Both can be used with existing RDBMS tools and often are a drop-in replacement. Things don’t always work like in the real implementation because of the design trade-offs of the distributed implementation, but things are often close enough. The problem with those? They’re proprietary and prohibitively expensive when using from the outside of the respective cloud provider.\nAs it usually happens with those clever whitepapers from the big players, other smart people pick them up and go on to build interesting products. It’s no different with Spanner / Aurora thing. The two most interesting products currently available are CockroachDB and YugabyteDB. They are roughly similar in what problems they tackle. Both are CP databases but both achieve high availability. Both are ACID and both are PostgreSQL compatible. Both have a roughly similar architecture. CockroachDB is written in go and reimplements the Postgres protocol from scratch, YugabyteDB is C++ and lifts the complete original C++ PostgreSQL engine.\nIt’s like PostgreSQL with storage ripped out and new RAFT based storage layer added.\nI have briefly looked at CockroachDB but rejected it almost immediately due to the licensing constraints. It’s not really clear what becomes Apache 2 licensed when and it explicitly disallows as-a-service use.\nYugabyteDB6 is Apache 2 licensed with no strings attached. The database core has no use restrictions. The cloud control plane is not Apache 2 licensed but YugabyteDB can be deployed without it.\nYugabyteDB Here’s the description:\nYugabyteDB is a free and open-source, distributed, relational, NewSQL database management system designed to handle large amounts of data spanning across multiple availability zones and geographic regions while providing single-digit latency, high availability, and no single point of failure.\nBasically, the magic pixie dust. I was very reluctant giving it a try. My BS radar was fully tuned.\nBut… wow…\nThis was different than anxiously anticipated! It does deliver everything what it claims it does.\nDatabases, schemas, tables, views, triggers, foreign data wrapper, and tablespaces, it’s all in there. Permission management comes from PostgreSQL and works exactly as expected. Roles can be restricted to databases, schemas, tablesspaces. Access to tables, views, functions, triggers, even individual columns and rows, can be controlled exactly like in Postgres—it’s all in there and working.\nThere’s more. YugabyteDB is a distributed system for a reason. The underlying DocDB layer automatically replicates and shards the data for high availability while leaving full control over those aspects to the operator. This is great because the replication is the first backup layer, a loss of a node does not mean data loss. Of course, take your backups…\nYugabyteDB uses RAFT replication per table. Different tables can have different replication factor. Different tables can be tied to different regions. Wow, even specific rows of specific tables can be placed in exact geographical locations, for example for legal compliance. There are distributed queries, joins, triggers, transactions, everything.\nMy first Postgres compatibility tests used a single YB master and YB TServer setup. The next day, I had a 30 node cluster with latency on par with PostgreSQL.\nMaybe I’m still in the honeymoon period with YugabyteDB. Whatever, so far I love the product and the execution. It behaves like PostgreSQL. For the tooling, it’s basically PostgreSQL with a lot of muscle behind. Like the new tablespaces implementation taking the Postgres concept one step further to allow geo-aware placement.\nWhile the Postgres layer is amazing in itself, YugabyteDB is not only about Postgres. There’s also the YCQL layer which is basically Cassandra on steroids and there’s the Yedis layer—Redis on steroids. There’s change data capture, point in time restore, Kafka ingest integration and many more features adding up to a very solid all round database solution ready for the cloud-native XXI century.\nIt’s clear that the team behind the product understands the problem space and is focused on solving their clients problems in open source rather than chasing an ever longer feature list. There does not seem to be a secondary agenda. There are two commercial products next to the open source version: the enterprise fully managed solution and the self-managed licensed solution. The company behind clearly knowns what, how and for whom.\nIf you enjoy using PostgreSQL, you’ll love YugabyteDB.\nACID ↩︎\nCAP theorem ↩︎\nConsistent hashing ↩︎\nDistributed hash table ↩︎\nEventual consistency ↩︎\nYugabyteDB ↩︎\n","description":"It’s like PostgreSQL with the storage part ripped out and RAFT plugged in","tags":["yugabytedb","postgres"],"title":"On YugabyteDB","uri":"/posts/2021-05-30-on-yugabytedb/"},{"content":"S3, Azure Blob, Google Storage and Minio, they’re all a K/V storage at the core. Yes, of course, they provide much, much more functionality beyond that but—at the core—object storage systems, S3, GCS, Minio and the likes, are K/V stores.\nEach one provides a HTTP interface. Putting the data in object storage is done via HTTP PUT or POST requests, fetching is available via GET. To check if an object under the key exists, that’s a HEAD request. Deleting is a single HTTP DELETE away.\nIt is somewhat interesting that we usually don’t think about S3 or GCS as key/value systems. They’re hidden behind a HTTP layer and our perception tells us they’re some sort of directories and files. They’re certainly not file systems.\nIf we start treating object storage as K/V, we can quickly find resemblance to other K/V systems. For example Redis or Cassandra. Or a PostgreSQL table with a primary key.\nLike S3 or Azure Blob, these dedicated systems provide functionality beyond just K/V. If the item to be fetched is identifiable by a well known ID and that fetch is one hop away, there’s no need to filter or join over anything else, that’s definitely K/V like!\nOry Hydra as an example The lifecycle of an OAuth token is not very complex. Once a token is generated, it lives for some period of time. Maybe 10 minutes, maybe a month. They’re handed over to an external application which holds on to them until a new token is needed. A token serves a couple of major purposes:\nit assures the holder that the data in the token comes from the issuer; this fact can be proven by validating the token’s signature, it can be sent to another application which can also—in turn—itself validate that the middleman has not tampered with the original data. There are two major types of tokens:\nJWT tokens: contains readable content, anybody can decode them, the information is readable, opaque tokens: these are not meant to be read, only the issuer understands what’s inside. Either type is issued in response to some event requiring assurance of a successful action on the issuer side. Most often, that’s an authentication or authorization. Tokens are idempotent. Once issued, they do not change.\nMost often, the following actions are performed on the tokens:\nsignature validation: token does not need to be sent to the issuer, the signature can be validated on the consumer side by loading public keys from issuer’s JWKS verification: is the token active? usually validated by checking the expiry timestamp and verifying that the server has not invalidated the token yet invalidation: the token should not be recognized anymore refreshing a token: in return for a valid refresh token, a new access token is issued Even if the token is forwarded to a third party and used for some fancy application specific logic, the third-party will essentially do one of the four of actions listed above.\nthe big question So the big question this write up asks: is the database system even needed to store them?\nThe signature validation does not require a look up. Even the issuer does not need to look anything up. Verification, in case of a database system, usually implies checking if the row for the token exists. Eventually, if there is an invalidation row in another table. Invalidation usually means removing the token from the table or creating the invalidation row in another table.\nThese operations are the same for a refresh token but a new token is generated, if refresh token is still valid.\nThis looks awfully close to K/V.\nSo what’s the point of having a database for that at all? Why not using globally distributed object storage instead?\nmental exercise Testing for expired token can be done in two complementary ways:\nissue a HEAD request against the key with the token object, 404 Not Found means the token is not valid, eventually, a HEAD request can be issued to test if there is an invalidation object for the respective token. Maybe these operations can be reversed depending on how probable the invalidation of a token is. Additionally, all of the object storage systems provide object expiration so the cleanup of old tokens comes for free, as in.\nInvalidation is an operation fulfilling the criteria for the conditions above.\nsome numbers Let’s go through some numbers based on a semi-real example. A client with roughly 1000 accounts, each account logging in once a week or so. That’s about 4000 tokens a month with further, say, 600K token verifications a month. If we wanted to run a cheapest version of HA database in the public cloud, options are (among other, of course):\nCloud SQL: $0.0966 per vCPU / GB RAM, that’s $69.552 / month based on 30 days AWS RDS Multi-zone: $0.36 for 2 vCPU with 1GB RAM burstable db.t3.micro instances: $25.92 / month It’s definitely not easy to run a HA database system with reasonable performance for less than $25 / month, even if we consider the likes of Hetzner.\nNeither of these could be recommended for a production public facing system. The prices above are compute only. There is no:\ndata transfer cost, maintenance cost, backup / restore, storage and snapshots. My point is, neither of these prices reflects reality and the actual cost will be definitely higher.\nLet’s compare this with some back-of-the-envelope calculations for object storage operations.\nAWS S3 storage cost, for a few thousand tokens, is going to be negligible. We are taking about data in megabytes, not gigabytes. Azure, GCS would be the same. Object storage operations are by far the most expensive. S3 charges $0.005 / 1000 PUT requests and $0.004 / 1000 GET and other (including HEAD) requests.\nGCS divides the operation in 2 classes. storage.object.put belongs to class A, these are $0.05 / 10000 operations, storage.object.get is a class B operation and these are priced at $0.004 per 10000 operations, effectively 10 times cheaper than S3.\nIn terms of number of operations:\nissuing a token is 1 PUT request, token verification is at most 2 HEAD requests token refresh is at most 2 HEAD requests and 1 PUT request. That’s respectively:\nS3 Standard: $0.000005 for issuing a token $0.000008 for a token verification worst case scenario and $0.000004 for best case $0.000008+$0.000005 per token refresh worst case scenario and $0.000004+$0.000005 for best case GCS: $0.000005 for issuing a token (price the same as S3) $0.0000008 for a token verification worst case scenario and $0.0000004 for best case (10 times cheaper than S3) $0.0000008+$0.000005 per token refresh worst case scenario and $0.0000004+$0.000005 for best case (PUT has the same price as S3 but GET is 10 times cheaper) If there is no need to support invalidation and expiry can be checked at the edge, there is no need to touch storage at all. If invalidation check is opportunistic, not every token will be checked for invalidation. However, for the most pessimistic usage, this client could run their token storage for:\nS3 Standard: $0.000005 * 4000 tokens + $0.000008 * 600K verifications = $4.82 / month GCS: $0.000005 * 4000 tokens + $0.0000008 * 600K verifications = $0.50 / month what about compute Aha, I’m glad this question came up. Turns out that running Ory Hydra on AWS Lambda using a Docker container results in an end to end request latency of ~100ms / request and it fits easily within the minimum 128MB execution runtime. That’s a mere $0.000000021 per request. The ~600K requests a month would cost that client a whopping $0.0126.\nCold start takes around 1.5 second but with 600K verifications, there is a request roughly every ~4 seconds so the cold start would not be frequent. They can be be minimized by trimming Hydra down, if there was no need to initialize ORM for example.\nIt’s fair to say that maybe $30 or $70 / month is not a lot of money. But consider that there are places where that might not be the case. Or maybe it’s a side project in an evaluation phase and that $70 becomes $840 / year. Finally, simply compare the number of requests per month and imagine that your $70 / month is used for a mostly idle resource. Why paying for a mostly idle resource at all?\nThere’s definitely a clear cut off point where a database becomes a reasonable choice worth paying for.\nMmm, how far away is it, though.\nno infrastructure The side effect here is: it is possible to run all this authentication and authorization stuff without any permanent compute resources. The same principle applies to Keto and Oathkepeer. All the decision related data can be easily put in a container, if it needs to be.\nThere is no need for a database in any of these systems. Kratos is the only difficult case.\n","description":"S3, Azure Blob, Google Storage and Minio, they’re all a K/V storage at the core","tags":["ory","hydra","kratos","keto","oathkeeper","serverless"],"title":"Do you really need a database for that Ory stack?","uri":"/posts/2021-05-23-do-you-really-need-a-database-for-that-ory-stack/"},{"content":"","description":"","tags":null,"title":"hydra","uri":"/tags/hydra/"},{"content":"","description":"","tags":null,"title":"kratos","uri":"/tags/kratos/"},{"content":"","description":"","tags":null,"title":"oathkeeper","uri":"/tags/oathkeeper/"},{"content":"","description":"","tags":null,"title":"Ory platform","uri":"/categories/ory-platform/"},{"content":"","description":"","tags":null,"title":"serverless","uri":"/tags/serverless/"},{"content":"I must admin—I struggled understanding Oathkeeper. Looking back, I think the reason was, I compared it one for one to things like Traefik or Envoy. Turns out, Oathkeeper does not necessarily intend replacing a reverse proxy, although many people probably use it as such.\nI was glossing over it for a long time and recently decided to come back to it. While reading the Oathkeeper Docs1 Introduction section, the part about using Oathkeeper as a decision API for Envoy, Ambassador and Nginx started working.\nDigging through issues and pull requests2, I found some recent commits mentioning Traefik ForwardAuth support. Little bit more digging and I found this little gem:\nGET /decisions HTTP/1.1 Accept: application/json with the following description:\nThis endpoint mirrors the proxy capability of ORY Oathkeeper’s proxy functionality but instead of forwarding the request to the upstream server, returns 200 (request should be allowed), 401 (unauthorized), or 403 (forbidden) status codes.\nIt’s in the clear, in the REST API documentation! Yes, that’s it.\nwhat is Oathkeeper Oathkeeper authorizes HTTP requests by matching a request to a rule from a set of defined rules, applying some guarding logic and allowing or denying a request. Oathkeeper has two modes of work:\na proxy: if a request is allowed, it is forwarded to the upstream an arbiter validating a request and returning HTTP success status or an error In both cases a request originating from the HTTP client must, at least, touch Oathkeeper. In the proxy mode, it flows completely through it.\nOathkeeper applies a maximum of four internal steps to each request:\nfirst, it finds if there is any rule matching the request URL and eventually a HTTP method, the matches can be either regexp or glob if there are no matching rules, the request is denied otherwise, the request passes through a pipeline of maximum three types of, let’s call them filters, for a lack of better word: authenticators: responsible for validating credentials authorizers: permissions the subject, basically: is the user behind the request allowed to execute this request mutators: transforms input credentials into upstream credentials rules The rules are always defined as references to JSON or YAML files in the main Oathkeeper YAML configuration. For example /etc/config/ok/oathkeeper.yaml (furhter always referred to as global configuration):\n1 2 3 4 access_rules: repositories: - file:///etc/config/ok/rules.json - https://example.com/ok.yaml More than one repository can be defined, rules can be loaded from Azure Blob Storage, Google Storage, S3, HTTPS, local files and more.\nSadly, there is no support for ETCD or Consul.\nAn example might look like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 [ { \"id\": \"some-id\", \"version\": \"v0.36.0-beta.4\", \"upstream\": { \"url\": \"http://my-backend-service\", \"preserve_host\": true, \"strip_path\": \"/api/v1\" }, \"match\": { \"url\": \"http://my-app/some-route/\u003c.*\u003e\", \"methods\": [\"GET\", \"POST\"] }, \"authenticators\": [{ \"handler\": \"noop\" }], \"authorizer\": { \"handler\": \"allow\" }, \"mutators\": [{ \"handler\": \"noop\" }], \"errors\": [{ \"handler\": \"json\" }] } ] Rules have unique IDs, can be optionally tagged with a supported Oathekeeper version and are matched according to the match section of the rule.\nIn the case above, any GET or POST request to any URL starting with http://my-app/some-route/ would be matched and authenticators, the authorizer and mutators would be applied. In this case, the request would be automatically allowed because there is only one noop authenticator. This rule appears to be used as a proxy rule because the upstream is defined.\nThe decision to allow or deny the request is made based on the evaluation of authenticators and optionally the authorizer. Mutators allow amending the credentials for the authenticator. For example, it could read a bearer token out of a cookie and place it in an Authorization header instead.\nIn order to use an authenticator, authorizer or a mutator in a rule, the respective item must be enabled in the main Oathkeeper YAML configuration.\nIf the respective item type requires additional configuration, the minimum configuration must be specified in the global configuration but can be overridden in individual rules.\nauthenticators An authenticator inspects the HTTP request and returns a boolean decision based on the implementation logic. At the time of writing, there are following authenticators available:\nnoop: bypass authentication, authorization and mutation, forward or allow the request further downstream outright unauthorized: outright reject the request anonymous: if there is no Authorization header, set the subject to anonymous (subject can be configured) cookie_session: forwards the request headers, path and method to a session store, basically: validate session based on headers (cookies, these are headers after all…) bearer_token: similar to cookie_session but allows configuring the source of the token: cookie, header or query parameter oauth2_client_credentials: uses the Authorization: Basic to perform OAuth 2.0 credentials grant to check if the credentials are valid, with a bit of reverse proxy trickery, if could potentially facilitate Hydra with credentials grant oauth2_introspection: uses the token introspection endpoint to validate the token and required scopes jwt: requires an Authorization: Bearer and assumes a JWT token, validates the signature of the token Every authenticator type has its dedicated configuration parameters. The configuration can be specified in the rule. As mentioned earlier, to be able to use an authenticator in the rule, the authenticator must be enabled globally.\nFor example, to use noop:\n1 2 3 authenticators: noop: enabled: true authorizers An authorizer ensures that the subject (the entity behind the request) has sufficient permissions to issue the request. There are currently five different authorizers with one of them being a Keto 0.5 specific legacy authorizer:\nallow: permits outright deny: denies outright remote: this one issues a POST request to the configured remote authorization endpoint and sends the original request body, if the remote returns 200 OK, the request is allowed, if the endpoint returns 403 Forbidden, the request is denied remote_json: this one issues a POST request to the configured remote authorization endpoint and sends configured JSON payload in the POST body, if the remote returns 200 OK, the request is allowed, if the endpoint returns 403 Forbidden, the request is denied keto_engine_acp_ory: is the Keto 0.5 specific authorizer, unless you are already using Keto 0.5, you’ll never use this one As with authenticators, the authorizers have to be explicitly enabled in the global configuration.\nFor example:\n1 2 3 authorizers: noop: enabled: true The remote_json authorizer can be used to authorize the request with Keto 0.6 Zanzibar fanciness:\n1 2 3 4 5 6 7 8 9 10 11 authorizer: handler: remote_json config: remote: http://keto:4466/check payload: | { \"namespace\": \"default-namespace\", \"subject\": \"{{ print .Subject }}\", \"object\": \"reports\", \"relation\": \"edit\" } The example shows that it is possible to use templating to populate the JSON payload from an AuthenticationSession object, where the respective golang code is:\n1 2 3 4 5 6 7 8 9 10 11 12 13 type AuthenticationSession struct { Subject string `json:\"subject\"` Extra map[string]interface{} `json:\"extra\"` Header http.Header `json:\"header\"` MatchContext MatchContext `json:\"match_context\"` } type MatchContext struct { RegexpCaptureGroups []string `json:\"regexp_capture_groups\"` URL *url.URL `json:\"url\"` Method string `json:\"method\"` Header http.Header `json:\"header\"` } Frankly, this bit lacks proper documentation because it is totally not clear where is this constructed, based on what data, how exactly is the subject extracted and when the rest of the data is available.\nAt least in the context of a JWT bearer token, the subject appears to be the JWT token subject.\nmutators Mutators transform an incoming credential into an outgoing credential. Following mutators are available:\nnoop: no mutation, forward headers as they came in id_token: converts the subject into a signed ID Token, the back end service can verify this token using the public key of Oathkeeper JWKS header: allows constructing additional headers from the HTTP request cookie: similar to header but constructs named cookies hydrator: allows fetching additional data from an external API and populates the mysterious AuthenticationSession object mentioned earlier, hmmm… maybe this can somehow be used to dynamically populate object and relation in the remote_json authorizer? error handlers Optionally, as the last step of a rule, a method of handling the authorization error can be specified using the errors rule section. Following handlers are available:\njson: returns the error as a JSON payload with application/json content type redirect: redirect the request to the location using HTTP 301 or 302 status, configurable www_authenticate: responds with HTTP 401 status and the WWW-Authenticate header The default handler is json. The order of the default handlers can be changed in the global configuration, for example:\n1 2 3 4 errors: fallback: - redirect - json As with any other object type, the error handler type has to be explicitly enabled in the global config, for example:\n1 2 3 4 errors: handlers: json: enabled: true Error handlers can be conditionally matched using when clauses:\n1 2 3 4 5 6 7 8 9 10 errors: redirect: enabled: true config: to: https://bad.robot when: - request: header: accept: - application/json two modes of operation I have briefly mentioned that Oathkeeper has two modes of operation: the proxy and the pure decision API.\nthe proxy The flow goes roughly like this:\nThe request is validated via Oathkeeper. When found okay, it is proxied upstream. It’s possible to instruct Oathkeeper to keep the original request host and strip a path prefix. These request go via the proxy server. The proxy server is configured in the global configuration, for example:\n1 2 3 4 serve: proxy: host: 0.0.0.0 port: 4455 The proxy server does not define any custom routes, it serves as a catch all router, as you’d expect from, well, a proxy.\nthe decision API This is the interesting one. In the simplest case, it works like this:\nIn this mode, Oathkeeper never sends the requests to upstream. In fact, a rule used with the decision mode does not need to define the upstream section. This would work perfectly fine:\n1 2 3 4 5 6 7 8 9 10 11 12 13 [ { \"id\": \"some-decision-id\", \"match\": { \"url\": \"http://my-app/some-route/\u003c.*\u003e\", \"methods\": [\"GET\", \"POST\"] }, \"authenticators\": [{ \"handler\": \"noop\" }], \"authorizer\": { \"handler\": \"allow\" }, \"mutators\": [{ \"handler\": \"noop\" }], \"errors\": [{ \"handler\": \"json\" }] } ] Assuming that the client wants to authorize a GET /some-route/abc request, it would send a GET /decisions/some-route/abc request to Oathkeeper in API mode. Oathkeeper would then look up the /some-route/abc rule for GET method and run its regular pipeline on it.\nIf the request was authorized, a HTTP 200 OK would be returned, otherwise the result would be HTTP 403 Forbidden.\nIt’s exactly the same as proxy mode but there is no proxying going on.\nThe API server is configured in the global configuration:\n1 2 3 4 serve: api: host: 0.0.0.0 port: 4456 and would normally be placed behind a reverse proxy. For example, the following Nginx configuration could take advantage of this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 http { #... server { #... location /private/ { auth_request /auth; auth_request_set $auth_status $upstream_status; } location = /auth { internal; proxy_pass http://oathkeeper:4456/decisions$request_uri; #... } } } summary This was a quick, 10 minute-like, introduction into Oathkeeper. A brain dump of sorts.\nI’m not sure if I’d ever use it as a pure proxy but the decision API looks very neat. Putting it behind Traefik would be an awesome feat but first investigations imply that a custom ForwardAuth plugin would be required.\nTraefik would be an awesome choice because of automatic ACME and the ability to use a single ForwardAuth to serve multiple sites via single Oathkeeper.\nThe reason why a custom ForwardAuth might be required, is that Traefik forwards the original request data in headers. The custom ForwardAuth would have to issue the Oathkeeper request constructed from those headers.\nWell, it’s not so complicated after all…\nOathkeeper Docs ↩︎\nIntegrate with Traefik, Nginx, Ambassador, Envoy on GitHub ↩︎\n","description":"","tags":["ory","oathkeeper","traefik","iap"],"title":"Figuring out Ory Oathkeeper","uri":"/posts/2021-05-20-figuring-out-oathkeeper/"},{"content":"","description":"","tags":null,"title":"iap","uri":"/tags/iap/"},{"content":"","description":"","tags":null,"title":"traefik","uri":"/tags/traefik/"},{"content":"There was an interesting question coming up related to the previous article on RBAC with Ory Keto1.\nThe question was:\nhow do I list the roles of a user\nAt the end of the previous article, the solution allowed finding out if the user is allowed to access the resources. But, indeed, what I have not discussed was how to get the roles the user is assigned to.\nThe final tuples for Fry and Bender, after Fry was demoted, looked like this:\ndev-director#member@Fry dev-director#member@Bender These relations tell that dev-director contains Fry and Bender. We can test this out with the following query:\n1 curl --silent 'http://localhost:4466/expand?namespace=default-namespace\u0026object=dev-director\u0026relation=member\u0026max-depth=2' | jq '.' 1 2 3 4 5 6 7 8 9 10 11 12 13 14 { \"type\": \"union\", \"subject\": \"default-namespace:dev-director#member\", \"children\": [ { \"type\": \"leaf\", \"subject\": \"Bender\" }, { \"type\": \"leaf\", \"subject\": \"Fry\" } ] } What if we want to know if the roles Fry belongs to? We could naively ask using the subject:\n1 curl --silent 'http://localhost:4466/expand?namespace=default-namespace\u0026subject=Fry\u0026relation=member\u0026max-depth=2' | jq '.' What we find out is, the result is completely wrong:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 { \"type\": \"union\", \"subject\": \"default-namespace:#member\", \"children\": [ { \"type\": \"leaf\", \"subject\": \"Bender\" }, { \"type\": \"leaf\", \"subject\": \"Fry\" }, { \"type\": \"leaf\", \"subject\": \"Hermes\" }, { \"type\": \"leaf\", \"subject\": \"default-namespace:it-director#member\" }, { \"type\": \"leaf\", \"subject\": \"default-namespace:it-director#member\" }, { \"type\": \"leaf\", \"subject\": \"default-namespace:dev-director#member\" }, { \"type\": \"leaf\", \"subject\": \"default-namespace:it-director#member\" } ] } In a real-world application, this would not leak PII, because you’d use UUIDs instead of names. However, this path is a no go.\nreverse user to role binding The new additional tuple looks like this:\nFry#is-member@dev-director 1 2 3 4 5 6 curl -XPUT --data '{ \"namespace\": \"default-namespace\", \"object\": \"Fry\", \"relation\": \"is-member\", \"subject\": \"dev-director\" }' http://localhost:4467/relation-tuples Now, we can ask:\n1 curl --silent 'http://localhost:4466/expand?namespace=default-namespace\u0026object=Fry\u0026relation=is-member\u0026max-depth=2' | jq '.' 1 2 3 4 5 6 7 8 9 10 { \"type\": \"union\", \"subject\": \"default-namespace:Fry#is-member\", \"children\": [ { \"type\": \"leaf\", \"subject\": \"dev-director\" } ] } For each user / role mapping, there are two tuples:\nrole#member@user user#is-member@role but there’s more We could go one step further. In the previous article, the following was established:\nat the time of permission evaluation, both the object for which the permission is being validated, and the subject, are known\nWhen Fry has got his opportunity, we ended up with the following configuration (plus the new reverse binding):\nproduction-viewer#member@default-namespace:dev-director#member production-creator#member@default-namespace:fast-dev-director#member production-deleter#member@default-namespace:fast-dev-director#member dev-director#member@Fry fast-dev-director#member@Fry Fry#is-member@dev-director Fry#is-member@fast-dev-director What if we had this instead:\nproduction-viewer#member@dev-director # \u003c--- this is added production-creator#member@fast-dev-director # \u003c--- this is added production-deleter#member@fast-dev-director # \u003c--- this is added production-viewer#member@default-namespace:dev-director#member production-viewer#member@default-namespace:fast-dev-director#member dev-director#member@Fry fast-dev-director#member@Fry Fry#is-member@dev-director Fry#is-member@fast-dev-director where the new tuples are:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 curl -XPUT --data '{ \"namespace\": \"default-namespace\", \"object\": \"Fry\", \"relation\": \"is-member\", \"subject\": \"fast-dev-director\" }' http://localhost:4467/relation-tuples curl -XPUT --data '{ \"namespace\": \"default-namespace\", \"object\": \"production-creator\", \"relation\": \"member\", \"subject\": \"fast-dev-director\" }' http://localhost:4467/relation-tuples curl -XPUT --data '{ \"namespace\": \"default-namespace\", \"object\": \"production-deleter\", \"relation\": \"member\", \"subject\": \"fast-dev-director\" }' http://localhost:4467/relation-tuples and all tuples together mean:\nAnybody who is dev-director or fast-dev-director, a member of dev-director or a member of fast-dev-director, can act as production-viewer. Fry is a member of both using the bi-directional mapping and also a member of production-creator and production-creator via fast-dev-director.\nWe have all the benefits of the original solution with an extra superpower.\nWhen we ask if create can be performed by Fry, what we really mean is is create allowed for any role Fry holds.\nThus, after asking for Fry’s membership, we would iterate over every role and ask Keto directly for the role permission:\n1 2 3 4 5 6 7 8 9 for role in $(curl --silent 'http://localhost:4466/expand?namespace=default-namespace\u0026object=Fry\u0026relation=is-member\u0026max-depth=2' | jq '.children[].subject' -r) do echo \"$role:\" $(curl --silent -XPOST --data '{ \"namespace\": \"default-namespace\", \"object\": \"production\", \"relation\": \"create\", \"subject\": \"'$role'\" }' http://localhost:4466/check) done dev-director: {\"allowed\":false} fast-dev-director: {\"allowed\":true} This is interesting because it allows implementing Keycloak style Authorization Services Decision Strategy2.\nWhen associating policies with a permission, you can also define a decision strategy to specify how to evaluate the outcome of the associated policies to determine access.\nUnanimous: The default strategy if none is provided. In this case, all policies must evaluate to a positive decision for the final decision to be also positive. Affirmative: In this case, at least one policy must evaluate to a positive decision for the final decision to be also positive. Consensus: In this case, the number of positive decisions must be greater than the number of negative decisions. If the number of positive and negative decisions is equal, the final decision will be negative. The result above could be the Affirmative strategy.\nclosing words The former solution, with less tuples, is definitely easier to reason about. Ha, but let’s be honest. Once UUIDs are used instead of literals, no sane mind can follow!\nThe latter solution opens the door for extra features. If I was rooting for a high level RBAC solution, I’d go for the more complex one. The complexity is an implementation detail.\nGetting the consistency right might be tricky with Keto REST / gRPC API only. This would probably require some sort of stateful state machine, possibly on top of etcd but it’s definitely doable. The extra benefit of the decision strategy is a nice side effect.\nFood for thought.\nRBAC with Ory Keto ↩︎\nKeycloak Authorization Services Decision Strategy ↩︎\n","description":"More thoughts on RBAC with Keto 0.6","tags":["ory","keto","iam","rbac","zanzibar","keycloak"],"title":"Keto RBAC - listing roles of a user","uri":"/posts/2021-05-17-keto-rbac-listing-roles-of-a-user/"},{"content":"","description":"","tags":null,"title":"rbac","uri":"/tags/rbac/"},{"content":"Role-base Access Control is an access control method whereby the entity roles define the level of access. Usually when talking about RBAC, the entity is a person and the object is a resource or a task (function) granted to a person. The usual example goes like this:\nIn an organization, the job functions define the roles of employees. Only employees in specific roles are allowed to execute certain tasks. The employees are given permissions to execute these tasks. Sometimes the employees may gain additional permissions to execute more tasks, sometimes certain permissions are taken away and the employees cannot execute selected tasks anymore.\nUpdate, 17th of May 2021: When publishing this article yesterday, I have incorrectly assumed that usersets aren’t implemented by Keto. The usersets are implemented, as explained by Patrik from the Ory team here1. What is not yet implemented is the rewrites functionality. I have updated the article to reflect this new fact.\ntheory RBAC The English Wikipedia has a very good, comprehensive entry on RBAC2. We can find a number of relevant RBAC related definitions, mainly:\nRole assignment: A subject can exercise a permission only if the subject has selected or been assigned a role. Role authorization: A subject’s active role must be authorized for the subject. With rule 1 above, this rule ensures that users can take on only roles for which they are authorized. Permission authorization: A subject can exercise a permission only if the permission is authorized for the subject’s active role. With rules 1 and 2, this rule ensures that users can exercise only permissions for which they are authorized. These can be a subject to additional organizational hierarchy where higher-level roles incorporate the ones of the subordinates. With this in mind, we find the conventions useful:\nS = Subject = A person or automated agent R = Role = Job function or title which defines an authority level P = Permissions = An approval of a mode of access to a resource SE = Session = A mapping involving S, R and/or P SA = Subject Assignment PA = Permission Assignment RH = Partially ordered Role Hierarchy. RH can also be written: ≥ (The notation: x ≥ y means that x inherits the permissions of y.) A subject can have multiple roles. A role can have multiple subjects. A role can have many permissions. A permission can be assigned to many roles. An operation can be assigned to many permissions. A permission can be assigned to many operations. Further, Wikipedia suggests the following set theory notation:\nPA ⊆ P × R SA ⊆ S × R RH ⊆ R × R Grokking these is straightforward, in order:\npermission assignment is a subset of permissions multiplied by roles subject assignment is a subset of subjects multiplied by roles role hierarchy is a subset of roles multiplied by other roles In other words:\nthe subject (a user) is allowed (has the permission) to execute certain action (on an object) when they have certain roles; the roles can inherit permissions of other roles\nThe second part is important because it suggests that the permissions of certain roles in the hierarchy can change.\nWhat’s crucial, we can parse the first statement in reverse:\nan action can be performed on an object by a subject holding specific roles\nLet’s hold on to that thought and proceed.\nthe…ory keto Keto is an implementation of the Zanzibar whitepaper3 from Google. Zanzibar, thus Keto, is used to implement Access Control Lists (ACL). RBAC differs from ACL in that RBAC assigns permissions to operations instead of objects. Keto (Zanzibar) does not have any knowledge of the system it controls the access on behalf of. There are four major terms in the Zanzibar whitepaper:\na relation tuple an object a relation a subject where the relation tuple is a result of an object, permission and a subject. Straight from the whitepaper:\n〈tuple〉::=〈object〉‘#’〈relation〉‘@’〈user〉 〈object〉::=〈namespace〉‘:’〈objectid〉 〈user〉::=〈userid〉|〈userset〉 〈userset〉::=〈object〉‘#’〈relation〉 This is the notation you can see in the Keto examples on the internet. For now, I will ignore the namespace part and use a single namespace for the example further down.\nIn the relation tuple, the user and relation are pretty obvious. The object part leaves room for improvisation. A little bit higher up, we have discussed that RBAC assigns permissions to operations instead of objects. Before moving on, we need to establish a couple of facts for the purpose of this article:\nthe Zanzibar object implies an RBAC operation, the Zanzibar subject is the RBAC object, the relation defines the permission. so, RBAC with ACL? I hear you say. But why not. Zanzibar is the outcome of Google’s work on the global access control system powering things like Calendar, Cloud, Drive and so on.\nTheir Cloud products alone contain dozens of sub-products, each defining own roles. Each of those roles carries object permissions (general service actions or individual object actions) the user can be granted.\nhow to proceed From the previous theory, two statements are the strongest signals of how to move forward:\nRBAC differs from ACL in that RBAC assigns permissions to operations instead of objects. An action can be performed on an object by a subject holding specific roles. The first one is a constraint. Actions, say granular read or write, should not be assigned to individual objects.\nThis makes sense because if a certain action should no longer be allowed for a certain role, there should be no need to iterate over every object to remove a permission. Instead, a permission should be revoked from a role and the users holding these roles should no longer be allowed to perform the revoked action.\nThe second statement is more of a clue. At the time of the decision making, two criteria are known:\nthe object on which the action is to be performed the user (subject) for which the decision has to be reached It is safe to assume that knowing the user implies knowing the roles the user belongs to. The second statement stems from the first one and shows that asking is the user allowed to execute this action is the wrong thing to do.\nThe correct question to ask is: can an action be performed on this object by someone holding these roles.\nthe scenario To visualize a PoC RBAC with Keto, let’s consider the following imaginary scenario.\nThere is a SaaS business offering compute services. Compute resources can be created, viewed and deleted. The Company is the client of SaaS and has a compute environment called production. The IT director, Hermes, can create, view and delete compute resources. The Company employs two development directors: Fry and Bender. Both of them can only view the resources in production. As the time passes, the Company asks Fry to speed up the deployment to production and gives him the permissions to create and delete resources in production. The SaaS and the Company will not be included in the implementation, they’re irrelevant to this example.\nimplementation Let’s start by launching the Ory stack locally:\n1 2 3 git clone https://github.com/radekg/ory-reference-compose.git cd ory-reference-compose/compose docker-compose -f compose.yml up psql keto-migrate keto Keto read API runs on localhost:4466 and the write API is reachable via localhost:4467. The compose stack creates a single namespace called default-namespace.\nLet’s start with defining the production platform by creating the following relation tuples:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 curl -XPUT --data '{ \"namespace\": \"default-namespace\", \"object\": \"production\", \"relation\": \"create\", \"subject\": \"default-namespace:production-creator#member\" }' http://localhost:4467/relation-tuples curl -XPUT --data '{ \"namespace\": \"default-namespace\", \"object\": \"production\", \"relation\": \"view\", \"subject\": \"default-namespace:production-viewer#member\" }' http://localhost:4467/relation-tuples curl -XPUT --data '{ \"namespace\": \"default-namespace\", \"object\": \"production\", \"relation\": \"delete\", \"subject\": \"default-namespace:production-deleter#member\" }' http://localhost:4467/relation-tuples In the Zanzibar notation, these are:\nproduction#create@(default-namespace:production-creator#member): roughly translates to: production allows create by anybody bound as a member of production-creator production#view@(default-namespace:production-viewer#member): production allows view by anybody bound as a member of production-viewer production#delete@(default-namespace:production-deleter#member): production allows delete by anybody bound as a member of production-deleter These relations can be read as properties (or capabilities) of the production platform. The production-[creator|viewer|deleter] is a handle for a referencing object, these referencing objects, called production-[creator|viewer|deleter] respectively, can be seen as the bindings.\nLet’s bind the IT director first. The following relation tuples together define a role called it-director.\nproduction-creator#member@default-namespace:it-director#member production-viewer#member@default-namespace:it-director#member production-deleter#member@default-namespace:it-director#member What we are saying here is that any person who is a member of the it-director role will be treated as a member of the respective production-X role, which we have already bound to respective create, view and delete.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 curl -XPUT --data '{ \"namespace\": \"default-namespace\", \"object\": \"production-creator\", \"relation\": \"member\", \"subject\": \"default-namespace:it-director#member\" }' http://localhost:4467/relation-tuples curl -XPUT --data '{ \"namespace\": \"default-namespace\", \"object\": \"production-viewer\", \"relation\": \"member\", \"subject\": \"default-namespace:it-director#member\" }' http://localhost:4467/relation-tuples curl -XPUT --data '{ \"namespace\": \"default-namespace\", \"object\": \"production-deleter\", \"relation\": \"member\", \"subject\": \"default-namespace:it-director#member\" }' http://localhost:4467/relation-tuples If there was a higher level system responsible for actual role management, the tool could present a role object called IT Director with these granular tuples hidden from view. The role management operator would be working with that abstraction instead of these granular items.\nNext, we can bind the development director. Originally, this role should only be allowed the view of the production resources:\nproduction-viewer#member@default-namespace:dev-director#member 1 2 3 4 5 6 curl -XPUT --data '{ \"namespace\": \"default-namespace\", \"object\": \"production-viewer\", \"relation\": \"member\", \"subject\": \"default-namespace:dev-director#member\" }' http://localhost:4467/relation-tuples Let’s put the people in their actual positions:\nit-director#member@Hermes dev-director#member@Fry dev-director#member@Bender 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 curl -XPUT --data '{ \"namespace\": \"default-namespace\", \"object\": \"it-director\", \"relation\": \"member\", \"subject\": \"Hermes\" }' http://localhost:4467/relation-tuples curl -XPUT --data '{ \"namespace\": \"default-namespace\", \"object\": \"dev-director\", \"relation\": \"member\", \"subject\": \"Fry\" }' http://localhost:4467/relation-tuples curl -XPUT --data '{ \"namespace\": \"default-namespace\", \"object\": \"dev-director\", \"relation\": \"member\", \"subject\": \"Bender\" }' http://localhost:4467/relation-tuples We can now ask Keto if the respective people can perform their tasks, for example, can Hermes create resources in production?\n1 2 3 4 5 6 curl -XPOST --data '{ \"namespace\": \"default-namespace\", \"object\": \"production\", \"relation\": \"create\", \"subject\": \"Hermes\" }' http://localhost:4466/check 1 {\"allowed\":true} What about Fry?\n1 2 3 4 5 6 curl -XPOST --data '{ \"namespace\": \"default-namespace\", \"object\": \"production\", \"relation\": \"delete\", \"subject\": \"Fry\" }' http://localhost:4466/check 1 {\"allowed\":false} But, both him and Bender, should be able to view:\n1 2 3 4 5 6 curl -XPOST --data '{ \"namespace\": \"default-namespace\", \"object\": \"production\", \"relation\": \"view\", \"subject\": \"Bender\" }' http://localhost:4466/check 1 {\"allowed\":true} So far, so good.\nFry’s opportunity The Company has finally decided to give Fry the opportunity and improve the production delivery speed.\nA new role has been carved out just for Fry:\n1 2 3 4 5 6 curl -XPUT --data '{ \"namespace\": \"default-namespace\", \"object\": \"fast-dev-director\", \"relation\": \"member\", \"subject\": \"Fry\" }' http://localhost:4467/relation-tuples He can’t yet create or delete:\n1 2 3 4 5 6 curl -XPOST --data '{ \"namespace\": \"default-namespace\", \"object\": \"production\", \"relation\": \"create\", \"subject\": \"Fry\" }' http://localhost:4466/check 1 {\"allowed\":false} For that, the fast-dev-director#member must be explicitly allowed by the respective production-X role.\n1 2 3 4 5 6 7 8 9 10 11 12 curl -XPUT --data '{ \"namespace\": \"default-namespace\", \"object\": \"production-creator\", \"relation\": \"member\", \"subject\": \"default-namespace:fast-dev-director#member\" }' http://localhost:4467/relation-tuples curl -XPUT --data '{ \"namespace\": \"default-namespace\", \"object\": \"production-deleter\", \"relation\": \"member\", \"subject\": \"default-namespace:fast-dev-director#member\" }' http://localhost:4467/relation-tuples Can he now create and delete?\n1 2 3 4 5 6 curl -XPOST --data '{ \"namespace\": \"default-namespace\", \"object\": \"production\", \"relation\": \"create\", \"subject\": \"Fry\" }' http://localhost:4466/check 1 {\"allowed\":true} 1 2 3 4 5 6 curl -XPOST --data '{ \"namespace\": \"default-namespace\", \"object\": \"production\", \"relation\": \"delete\", \"subject\": \"Fry\" }' http://localhost:4466/check 1 {\"allowed\":true} He should still be able to view through the dev-director role:\n1 2 3 4 5 6 curl -XPOST --data '{ \"namespace\": \"default-namespace\", \"object\": \"production\", \"relation\": \"view\", \"subject\": \"Fry\" }' http://localhost:4466/check 1 {\"allowed\":true} He can, indeed. What about Bender?\n1 2 3 4 5 6 curl -XPOST --data '{ \"namespace\": \"default-namespace\", \"object\": \"production\", \"relation\": \"delete\", \"subject\": \"Bender\" }' http://localhost:4466/check 1 {\"allowed\":false} This works as expected.\nFry has fried the production As Fry isn’t very smart, he did fry the production system and the Company was forced to remove his access. It was done like this:\n1 curl --silent -X DELETE 'http://localhost:4467/relation-tuples?namespace=default-namespace\u0026object=fast-dev-director\u0026relation=member\u0026subject=Fry' And Fry could no longer create and delete in production.\nAlternatively, the Company should have been able to remove the role binding, like this:\n1 2 curl --silent -X DELETE 'http://localhost:4467/relation-tuples?namespace=default-namespace\u0026object=production-creator\u0026relation=member\u0026subject=default-namespace:fast-dev-director#member' curl --silent -X DELETE 'http://localhost:4467/relation-tuples?namespace=default-namespace\u0026object=production-deleter\u0026relation=member\u0026subject=default-namespace:fast-dev-director#member' Which would leave Fry the fast-dev-director role assignment but the missing binding would no longer allow him to create and delete resources. However, these two delete statements do not seem to be removing the actual binding.\nsummary How does this example compare to the RBAC theory from the beginning of the article?\nRole assignment: A subject can exercise a permission only if the subject has selected or been assigned a role: check, Fry had to be explicitly assigned using the member relation to the fast-dev-director role. Role authorization: A subject’s active role must be authorized for the subject. With rule 1 above, this rule ensures that users can take on only roles for which they are authorized: check, respective production-X explicitly allows fast-dev-director#member Permission authorization: A subject can exercise a permission only if the permission is authorized for the subject’s active role. With rules 1 and 2, this rule ensures that users can exercise only permissions for which they are authorized: check, we have verified that even though Fry was given the fast-dev-director role, he could not create nor delete before the relevant production-X binding was established; the non-functional curl -XDELETE throws a spanner in the works but it’s unrelated to the fact that Keto does provide this capability and the core issue can be fixed wrapping up This simple example shows that a simple RBAC is doable with Ory Keto. Without the rewrites, the configuration can be quite verbose but the example shows that different relation tuples can be chained together to form a more complex decision tree.\nThe example with deletes not removing the expected tuples indicate that Keto might still not be totally bulletproof but things can only get better.\nWorking directly with tuples can probably lead to a big headache. An efficient way of working would definitely require a tool managing and verifying the relation tuples based on some higher level role and membership abstraction.\nusersets vs rewrites clarification ↩︎\nRole-based Access Control ↩︎\nZanzibar: Consistent, Global Authorization System ↩︎\n","description":"Building bare bones RBAC with Keto 0.6","tags":["ory","keto","iam","rbac","zanzibar"],"title":"RBAC with Ory Keto","uri":"/posts/2021-05-15-rbac-with-ory-keto/"},{"content":"Software almost never runs in isolation. Today’s systems integrate with a vast number of external services. Ensuring reliability is difficult because the external dependencies, be it a database or an authentication system, adds an element of unpredictability which is difficult to emulate in isolation. A reliable system should account for the behavior of its dependencies. What does it help that an API is up and running when the underlying service it talks to hasn’t been accounted for a specific edge case and is causing an unexpected latency to my clients under certain conditions?\nEmulating input and output is easy with regular unit tests. Many people rely on the so called mocks to emulate external systems. Mocks are the pieces of code emulating external dependencies and behaving almost like them. Almost being the key word.\nAn example is go httptest.Server. It gives a fully functional, easy to configure HTTP server but the handlers do not necessarily behave like the dependency. These mocks only get us so far. Often the output depends on the quirks of the input, or the configuration of the system we integrate with. These edge cases may be very difficult to cover because they require reimplementing said quirks.\nThat’s exactly what I personally dislike about mocking. Reinventing this once or twice is maybe okay. Reinventing this for one, maybe two simple dependencies is maybe okay. When we intend to iterate our systems for years, we want to focus on our business problem. Our dependencies will mature, change scope, reimplement features and grow in complexity. We will add more dependencies and maybe replace existing ones with alternatives.\nMaintaining such mocked services adds unnecessary overhead. When a new version of the dependency is released, not only we have to adapt our services. Our testing infrastructure must be adapted to cover the changes.\nYes, it does give us the opportunity to understand how our dependencies work. This is always good know but this knowledge should come via means other than rewriting the logic of the dependency. If you have to use a hash map, you don’t write your own. You use a library. Why would an OAuth server be any different?\nA library is directly called in the path of our code. When the library dependency is upgraded, there’s an instant feedback loop. If the interfaces have changed or the behavior differs, the code will not compile or existing tests will fail immediately. With an external system, things are not often so obvious. Just because the API hasn’t changed doesn’t mean the dependency is doing the same thing. What would be great is to treat an external system in tests like a library.\nSome organizations have enough resources to supply the development team with a shared integration environment to run the tests against. This is great because there is no need to maintain these mocking services anymore. Tests run against a real database, areal authentication system, a real queue, and so on.\nThe cost is the most obvious downside. That infrastructure costs money and someone has to maintain it. If you’re a solo developer or a small team, you either may not have the financial resources to simply make it happen, or simply do not have the time to maintain all of that yourself.\nThere’s also the element of inflexibility. There’s a central system configured once, sharing the state between multiple instances of tests. This can be improved on if the developers have access to good equipment and can run parts of the infrastructure locally. The elephant in the room is that this leads to the duplication in environment setup. There’s a production system and another copy of the system which needs (sometimes can’t) be configured as close to the production as possible.\ntesting with containers Containers are the third available option. Writing a system in go and need to integrate with Kafka? Writing a Ruby app and need to talk to Postgres? This is easy with Docker. Just start Kafka in Docker, connect to it in the test and run real code paths right in tests. Need Redis? No problem, start a container. Postgres? Why not a container. etcd, MySQL, Minio, Hydra and Vault together for a complex integration? Containers can do this, regardless if your system is PHP, Ruby, Rust, go or ColdFusion!\nThe ultimate method is to spin up the containers right in tests. There are libraries in virtually every programming languages enabling this.\ngo and ory/dockertest Here, I’d like to focus on go. I’ve spent some time over more than two years evaluating the Ory platform and the dockertest library from Ory became an invaluable asset.\ndockertest is a very nice abstraction layer on top of the go-dockerclient making it so much easier to configure and execute containers with the focus on short lived tests.\nEvaluating the Ory platform meant setting up Hydra, Keto and Kratos, in many different configurations. Reducing the time of every iteration, even as simple as changing the underlying configuration, was crucial. What’s the better way than spinning up a fresh setup in every test and configuring it in-test?\nI haven’t found any better method than running containers. So what does it look like to run a container with dockertest? This is an example from its readme:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 postgres, err := pool.RunWithOptions(\u0026dockertest.RunOptions{ Repository: \"postgres\", Tag: \"11\", Env: []string{ \"POSTGRES_USER=test\", \"POSTGRES_PASSWORD=test\", \"listen_addresses = '*'\", }, }, func(config *docker.HostConfig) { // set AutoRemove to true so that stopped container goes away by itself config.AutoRemove = true config.RestartPolicy = docker.RestartPolicy{ Name: \"no\", } }) This gives a running Postgres database server. It’s so easy that there is no reason not to do it.\napp-kit-orytest A couple of weeks ago I have open sourced the app-kit-orytest library. This library provides preconfigured Hydra, Keto and Kratos components running against Postgres database and is available on GitHub.\nWith app-kit-orytest, the result is a full IAM / IdP environment right in the test.\nWhat does it take to have Hydra, Kratos and Keto in the test? It’s only a few lines of code - here’s an example:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 package main import ( \"github.com/radekg/app-kit-orytest/common\" \"github.com/radekg/app-kit-orytest/hydra\" \"github.com/radekg/app-kit-orytest/keto\" \"github.com/radekg/app-kit-orytest/kratos\" \"github.com/radekg/app-kit-orytest/mailslurper\" \"github.com/radekg/app-kit-orytest/postgres\" ketoModels \"github.com/ory/keto-client-go/models\" \"github.com/stretchr/testify/assert\" ) func Test(t *testimng.T) { // Any component requires Postgres: postgresCtx := postgres.SetupTestPostgres(t) // Start Hydra: hydraCtx := hydra.SetupTestHydra(t, postgresCtx) defer hydraCtx.Cleanup() // Start Keto: ketoCtx := keto.SetupTestKeto(t, postgresCtx, \"default-namespace\") defer ketoCtx.Cleanup() // Kratos requires mailslurper SMTP server: mailslurperCtx := mailslurper.SetupTestMailslurper(t) // Start Kratos: kratosSelfService := kratos.DefaultKratosTestSelfService(t) defer kratosSelfService.Close() kratosCtx := kratos.SetupTestKratos(t, postgresCtx, mailslurperCtx, kratosSelfService) defer kratosCtx.Cleanup() // The contexts provide the tests with clients, for example: _, err := ketoCtx.WriteClient().Write.CreateRelationTuple(ketoWrite. NewCreateRelationTupleParams(). WithPayload(\u0026etoModels.InternalRelationTuple{ Namespace: common.StringP(\"default-namespace\"), Object: common.StringP(\"company-a\"), Relation: common.StringP(\"employs\"), Subject: (*ketoModels.Subject)(common.StringP(\"director\")), })) assert.Nil(t, err) } The code above starts Hydra, Keto and Kratos. It ensures the migrations are executed, the configuration files are written, the volumes are mounted and services listen on random ports making it easy to run multiple tests in parallel, if needed. With this library, I was able to very quickly test different Ory configuration.\npotential downsides? Testing using external CI/CD might get complicated because third-party CI/CD platforms very often do not allow controlling of the Docker daemon by the unit under test but integration tests have often different testing pipelines anyway.\nWriting tests using containers requires testing code hygiene. To cover potential dependency change, not too much of the test setup logic should live outside of the library. To what extent this is a downside, one has to answer themself.\nclosing words With Docker and dockertest, I was able to achieve quicker turn around when testing different configurations and approaches. That’s not only proving to be a great method for maintaining a reliable system but also enables quick exploration. A test can be also a scratch pad.\n","description":"Testing software against real system is the ultimate testing","tags":["testing","ory","docker"],"title":"On software testing with dockertest","uri":"/posts/2021-04-24-on-software-testing-with-dockertest/"},{"content":"","description":"","tags":null,"title":"testing","uri":"/tags/testing/"},{"content":"","description":"","tags":null,"title":"consul","uri":"/tags/consul/"},{"content":"","description":"","tags":null,"title":"firecracker","uri":"/tags/firecracker/"},{"content":"Some two months ago, when I started the Firecracker journey, I set myself a goal to run en etcd cluster in Firecracker microVMs. Many lines of code later, after tackling the problem the hard way, there’s an outcome.\nOkay, it’s not etcd but rather HashiCorp Consul.\nHere’s how a 3 node Consul cluster is launched with firebuild:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 # Start Consul 1 with IP 192.168.127.10: sudo firebuild run \\ --profile=standard \\ --from=combust-labs/consul:1.9.4 \\ --cni-network-name=machines \\ --vmlinux-id=vmlinux-v5.8 \\ --ip-address=192.168.127.10 \\ --name=consul1 \\ --daemonize \\ -- agent -server -client 0.0.0.0 -bootstrap-expect 3 -data-dir /consul/data -retry-join 192.168.127.11 -retry-join 192.168.127.12 -node consul1 # Start Consul 2 with IP 192.168.127.11: sudo firebuild run \\ --profile=standard \\ --from=combust-labs/consul:1.9.4 \\ --cni-network-name=machines \\ --vmlinux-id=vmlinux-v5.8 \\ --ip-address=192.168.127.11 \\ --name=consul2 \\ --daemonize \\ -- agent -server -client 0.0.0.0 -bootstrap-expect 3 -data-dir /consul/data -retry-join 192.168.127.10 -retry-join 192.168.127.12 -node consul2 # Start Consul 3 with IP 192.168.127.10: sudo firebuild run \\ --profile=standard \\ --from=combust-labs/consul:1.9.4 \\ --cni-network-name=machines \\ --vmlinux-id=vmlinux-v5.8 \\ --ip-address=192.168.127.12 \\ --name=consul3 \\ --daemonize \\ -- agent -server -client 0.0.0.0 -bootstrap-expect 3 -data-dir /consul/data -retry-join 192.168.127.10 -retry-join 192.168.127.11 -node consul3 After a couple of seconds required to boot the VMs, start the services and reconcile the cluster, the cluster can be queries for status:\n1 $ curl http://192.168.127.12:8500/v1/status/leader The output will be similar to:\n1 \"192.168.127.10:8300\" What about the DNS? Consul nodes were launched with with -node flags. The cluster uses the default 8600 DNS port and default data center name of dc1.\nHence, we can query for a specific node:\n1 $ dig @192.168.127.10 -p 8600 consul1.node.dc1.consul The response looks similar to:\n; \u003c\u003c\u003e\u003e DiG 9.11.3-1ubuntu1.14-Ubuntu \u003c\u003c\u003e\u003e @192.168.127.10 -p 8600 consul1.node.dc1.consul ; (1 server found) ;; global options: +cmd ;; Got answer: ;; -\u003e\u003eHEADER\u003c\u003c- opcode: QUERY, status: NOERROR, id: 13511 ;; flags: qr aa rd; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 2 ;; WARNING: recursion requested but not available ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 4096 ;; QUESTION SECTION: ;consul1.node.dc1.consul.\tIN\tA ;; ANSWER SECTION: consul1.node.dc1.consul. 0\tIN\tA\t192.168.127.10 ;; ADDITIONAL SECTION: consul1.node.dc1.consul. 0\tIN\tTXT\t\"consul-network-segment=\" ;; Query time: 1 msec ;; SERVER: 192.168.127.10#8600(192.168.127.10) ;; WHEN: Wed Apr 14 21:31:00 UTC 2021 ;; MSG SIZE rcvd: 104 in other news documentation is online There is a new firebuild documentation website online. It’s not complete but it’s a step forward from maintaining a huge readme file in a GitHub repository. The documentation is available here1.\nDocker image based builds Apart from that, there was a bit more work down in the trenches. From the very beginning, it was possible to create a root file system from a Dockerfile. But sometimes a Dockerfile is not sufficient, especially in case of a multi-stage build where the actual build happens outside of a Dockerfile.\nAn example of this is the Jaeger tracing Dockerfile with binary artifacts built in a separate make step.\nIt is now possible to build a root file system directly from a Docker image. Here’s an example of building Jaeger 1.22:\n1 2 3 4 5 6 7 8 sudo firebuild rootfs \\ --profile=standard \\ --docker-image=jaegertracing/all-in-one:1.22 \\ --docker-image-base=alpine:3.13 \\ --cni-network-name=machine-builds \\ --vmlinux-id=vmlinux-v5.8 \\ --mem=512 \\ --tag=combust-labs/jaeger-all-in-one:1.22 Right now, more details only in the huge readme2. Documentation will be updated in the following weeks.\nvminit rootfs bootstrap now with MMDS The next big update is the vminit rootfs MMDS based bootstrap. Until now, the rootfs was build via an SSH connection. This has been replaced with MMDS based bootstrap and the SSH code has been removed from firebuild. More about that in this article3.\nvms can be named As seen in the Consul cluster example at the top, the VMs can be named so it is much easier to refer to them later. No need to hunt the random VM ID anymore. Just start the VM with --name=unique1 and this is possible:\n1 2 3 VMIP=$(sudo firebuild inspect \\ --profile=standard \\ --vmm-id=unique1 | jq '.NetworkInterfaces[0].StaticConfiguration.IPConfiguration.IP' -r) By the way, the metadata JSON uses camel case for easier integration with tools like jq.\nThere are caveats related to names. The name can be maximum 20 characters long and only alphanumeric characters are allowed.\ntest coverage There have been many tests written and the coverage, especially for builds and resource discovery, has been increased significantly.\nwhat’s coming The next big research areas over the coming weeks, in no particular order:\nexpose guest ports via command line flags on the host through iptables integration run command to have support for adding files to the VM on boot additional volumes for long lived data persistence service discovery integration; first step is to integrate with Consul service catalog and DNS - this will open the door for launching more complex infrastructures with firebuild That’s it for today, thank you for reading!\nfirebuild documentation ↩︎\nfirebuild readme: build directly from a Docker image ↩︎\nfirebuild rootfs - gRPC with mTLS ↩︎\n","description":"The goal has been reached, I have a Consul cluster running","tags":["firecracker","microvm","firebuild","consul"],"title":"Launching Consul cluster with firebuild and other news","uri":"/posts/2021-04-14-launching-consul-cluster-with-firebuild/"},{"content":"","description":"","tags":null,"title":"microvm","uri":"/tags/microvm/"},{"content":"Permissions management is an interesting topic. Modern applications are often complex beasts. It doesn’t take much time to hit the point where certain functionality must be allowed only to the selected users or access to a resource should be granted only under certain conditions.\nBuilding a flexible permissions management system is not easy. They tend to be tightly coupled with the business logic and executed every time a decision whether to grant or deny access is required. There are as many requirements as software systems out there. A permissions management systems must also perform. If decisions are to be made often, the latency to make a decision must be minimal.\nORY Keto is one of the ORY platform components and a few days ago it has seen a major upgrade. Versions 0.6.0-alpha.1 is a complete reimplementation of Keto and is marketed as the first open source implementation of Zanzibar: Google’s Consistent, Global Authorization System1.\nFrom the Zanzibar abstract:\nDetermining whether online users are authorized to access digital objects is central to preserving privacy. This paper presents the design, implementation, and deployment of Zanzibar, a global system for storing and evaluating access control lists. Zanzibar provides a uniform data model and configuration language for expressing a wide range of access control policies from hundreds of client services at Google, including Calendar, Cloud, Drive, Maps, Photos, and YouTube. Its authorization decisions respect causal ordering of user actions and thus provide external consistency amid changes to access control lists and object contents. Zanzibar scales to trillions of access control lists and millions of authorization requests per second to support services used by billions of people. It has maintained 95th-percentile latency of less than 10 milliseconds and availability of greater than 99.999% over 3 years of production use.\nI have been using previous Keto versions for some R\u0026D work. The new version has sparked my interest because the firebuild system I am working on is going to be in need of a permissions system at some point in time.\nTo better understand the new Keto, I wanted a simple, easy to follow scenario so a judgement can be formed quickly.\nIn one short sentence: it was very easy and it’s impressive!!\nthe scenario The scenario I was evaluating:\na company employs a director and IT staff the director contracts a consultant the IT staff subscribes to external services Find out what the company pays for directly and indirectly.\nHere’s the rough diagram:\nORY Keto in Docker Compose I have previously written about my reference ORY Docker Compose2 and that is what I’m using further. To start the local installation, assuming the containers are already built, as explained in the repository readme3:\n1 2 3 git clone https://github.com/radekg/ory-reference-compose.git cd ory-reference-compose/compose docker-compose -f compose.yml up The new Keto version exposes two API servers:\nthe write API: default port is 4467 the read API: default port is 4466 Before we can start querying Keto for a decision, we have to create a few relation tuples4.\nKeto does not try understanding our data, it infers the decision by inspecting and traversing tuples it knows.\nRelation tuples can be created using cURL via the write API. The Compose setup publishes both Keto APIs so we can start like this:\n1 2 3 4 5 6 curl -XPUT --data '{ \"namespace\": \"default-namespace\", \"object\": \"company-a\", \"relation\": \"employs\", \"subject\": \"director\" }' http://localhost:4467/relation-tuples 1 2 3 4 5 6 curl -XPUT --data '{ \"namespace\": \"default-namespace\", \"object\": \"company-a\", \"relation\": \"employs\", \"subject\": \"it-staff\" }' http://localhost:4467/relation-tuples The associations above imply that the company pays for the director and the IT staff. We could model it like this:\n1 2 3 4 5 6 curl -XPUT --data '{ \"namespace\": \"default-namespace\", \"object\": \"company-a\", \"relation\": \"pays\", \"subject\": \"default-namespace:company-a#employs\" }' http://localhost:4467/relation-tuples That subject means: the company pays for anybody it employs. Let’s see this in action by executing this request against the read API:\n1 curl --silent 'http://localhost:4466/expand?namespace=default-namespace\u0026object=company-a\u0026relation=pays\u0026max-depth=10' | jq '.' The output is:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 { \"type\": \"union\", \"subject\": \"default-namespace:company-a#pays\", \"children\": [ { \"type\": \"union\", \"subject\": \"default-namespace:company-a#employs\", \"children\": [ { \"type\": \"leaf\", \"subject\": \"director\" }, { \"type\": \"leaf\", \"subject\": \"it-staff\" } ] } ] } Now, the IT staff subscribes to AWS, Dropbox and GCP. These relations could be modelled like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 curl -XPUT --data '{ \"namespace\": \"default-namespace\", \"object\": \"it-staff\", \"relation\": \"subscribes\", \"subject\": \"aws\" }' http://localhost:4467/relation-tuples curl -XPUT --data '{ \"namespace\": \"default-namespace\", \"object\": \"it-staff\", \"relation\": \"subscribes\", \"subject\": \"dropbox\" }' http://localhost:4467/relation-tuples curl -XPUT --data '{ \"namespace\": \"default-namespace\", \"object\": \"it-staff\", \"relation\": \"subscribes\", \"subject\": \"gcp\" }' http://localhost:4467/relation-tuples As these will bear the cost to the company, the JSON report should list them and can be done with this new relation tuple:\n1 2 3 4 5 6 curl -XPUT --data '{ \"namespace\": \"default-namespace\", \"object\": \"company-a\", \"relation\": \"pays\", \"subject\": \"default-namespace:it-staff#subscribes\" }' http://localhost:4467/relation-tuples That means: the company pays for everything the IT staff subscribes to. If we execute the /expand call again, the output will be:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 { \"type\": \"union\", \"subject\": \"default-namespace:company-a#pays\", \"children\": [ { \"type\": \"union\", \"subject\": \"default-namespace:company-a#employs\", \"children\": [ { \"type\": \"leaf\", \"subject\": \"director\" }, { \"type\": \"leaf\", \"subject\": \"it-staff\" } ] }, { \"type\": \"union\", \"subject\": \"default-namespace:it-staff#subscribes\", \"children\": [ { \"type\": \"leaf\", \"subject\": \"aws\" }, { \"type\": \"leaf\", \"subject\": \"dropbox\" }, { \"type\": \"leaf\", \"subject\": \"gcp\" } ] } ] } Pretty cool, regardless of how many services the IT staff would subscribe to in the future, they will get listed in the output!\nKnowing about the cost of a consultant contracted by the director could be modelled in the following way:\n1 2 3 4 5 6 curl -XPUT --data '{ \"namespace\": \"default-namespace\", \"object\": \"director\", \"relation\": \"contracts\", \"subject\": \"consultant\" }' http://localhost:4467/relation-tuples 1 2 3 4 5 6 curl -XPUT --data '{ \"namespace\": \"default-namespace\", \"object\": \"company-a\", \"relation\": \"pays\", \"subject\": \"default-namespace:director#contracts\" }' http://localhost:4467/relation-tuples 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 { \"type\": \"union\", \"subject\": \"default-namespace:company-a#pays\", \"children\": [ { \"type\": \"union\", \"subject\": \"default-namespace:company-a#employs\", \"children\": [ { \"type\": \"leaf\", \"subject\": \"director\" }, { \"type\": \"leaf\", \"subject\": \"it-staff\" } ] }, { \"type\": \"union\", \"subject\": \"default-namespace:director#contracts\", \"children\": [ { \"type\": \"leaf\", \"subject\": \"consultant\" } ] }, { \"type\": \"union\", \"subject\": \"default-namespace:it-staff#subscribes\", \"children\": [ { \"type\": \"leaf\", \"subject\": \"aws\" }, { \"type\": \"leaf\", \"subject\": \"dropbox\" }, { \"type\": \"leaf\", \"subject\": \"gcp\" } ] } ] } And if the director decides to contract a solicitor?\n1 2 3 4 5 6 curl -XPUT --data '{ \"namespace\": \"default-namespace\", \"object\": \"director\", \"relation\": \"contracts\", \"subject\": \"solicitor\" }' http://localhost:4467/relation-tuples the existing relation already covers that:\n1 curl --silent 'http://localhost:4466/expand?namespace=default-namespace\u0026object=company-a\u0026relation=pays\u0026max-depth=10' | jq '.' 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ... { \"type\": \"union\", \"subject\": \"default-namespace:director#contracts\", \"children\": [ { \"type\": \"leaf\", \"subject\": \"consultant\" }, { \"type\": \"leaf\", \"subject\": \"solicitor\" } ] }, ... Very neat. At some point, we might want to have an answer to the following direct question:\ndoes the company pay for the consultant?\n1 2 3 4 5 6 curl -XPOST --data '{ \"namespace\": \"default-namespace\", \"object\": \"company-a\", \"relation\": \"pays\", \"subject\": \"consultant\" }' http://localhost:4466/check To which the answer is:\n1 2 3 { \"allowed\": true } If we ever had to understand what relation causes the company to pay for the consultant, we would use the expand call with sufficient depth.\nnotes on the examples The examples use literal object identifiers for readability purposes. Keto documentation suggests using UUIDv4 or SHA-1 hashes instead. The concept is better explained on this page5.\nconclusion This very simple example shows in just few steps how awesome the new version of Keto is.\nThe API is very simple to use and understand. The flexibility and applicability is endless.\nThis is something what will greatly benefit firebuild and I am sure we will see Keto being used all over the place.\nThe ORY team deserves huge applause for making this technology so easily approachable.\nZanzibar: Google’s Consistent, Global Authorization System ↩︎\nORY reference Docker Compose and thoughts on the platform ↩︎\nReference ORY Docker Compose setup readme ↩︎\nORY Keto concepts: relation tuples ↩︎\nORY Keto concepts: objects ↩︎\n","description":"First look at ORY Keto","tags":["ory","keto","zanzibar"],"title":"Looking at Zanzibar through ORY Keto","uri":"/posts/2021-04-11-looking-at-zanzibar-through-ory-keto/"},{"content":"obligatory Keycloak mention Keycloak is awesome because it provides almost everything an organization of almost any size might ever need when it comes to topics like OpenID, SSO, federation or authorization services.\nBut Keycloak is a monolith, it doesn’t do things the cloud native way1. There’s quite a few running modes one needs to fully understand how to operate: standalone, standalone clustered, domain clustered mode, cross data center replication. All very powerful but to many, pretty unapproachable. XML files, JBoss, WildFly, Java - heavyweight technologies associated with slow startup times and difficulty in scaling.\nKeycloak doesn’t provide OpenAPI specification. There are many client libraries but a lot of them are incomplete and do not cover every aspect.\nIntegration can be frustrating, especially when using more obscure Keycloak features. Upgrades can be a pain.\nIt’s not surprising that alternatives appear left and right.\nory One of them is ORY2. Unlike Keycloak, ORY is a bunch of building blocks. In contrast to Keycloak, ORY is written in golang and the pieces are relatively small footprint, zero dependency binaries.\nThey are designed to run in containers and start very fast. This makes them a perfect fit for cloud native applications. There are four major components, each providing and OpenAPI 2.0 specification allowing for relatively easy post deployment operations:\nHydra: OpenID Certified OAuth 2.0 Server and OpenID Connect Provider Keto: is the open source implementation of Zanzibar: Google’s Consistent, Global Authorization System3; basically an access management system Kratos: Identity and User Management System Oathkeeper: an Identity \u0026 Access Proxy (IAP) and Access Control Decision API ORY is not a one to one alternative to Keycloak. A number of examples:\nKratos does not provide any account searching functionality there is no way to attach user account metadata in Kratos there are no groups and roles in Kratos in theory this can be built out on Keto but there is no standard method Keto does not provide user managed access Oathkeeper is okay for simple authentication but complex authentication flows get hairy pretty fast a lot of JSON rule files to manage multi-tenant deployment with ad-hoc OpenID clients would be much better served using Traefik with ForwardAuth plugins Oathkeeper appears to have some aspirations to be a reverse proxy too but it’s not exactly Traefik gluing Kratos, Hydra and Keto together is a lot of one-off work there are no plugins in ORY adding 2FA is a custom job creating custom authentication flows is a one-off job using Keycloak mappers? there’s is nothing like this in ORY custom claims? only by using Hydra administrative Login Accept API no SAML, no LDAP support scalability through the database The individual components are pretty easy to get going. Using all four is actually a lot of work. Integration will require writing a lot of code.\nLike with Keycloak, starting with ORY is overwhelming. Individual components are documented rather okay but diving deep in Dockerfiles and examples on GitHub will be necessary.\nthe Docker Compose ORY reference setup I’ve been maintaining my own reference setup for more than a year.\nIt is available on GitHub today4. The reference integrates all four components and provides a couple of simple scenarios visualizing the interactions. It starts with building Docker images from sources and goes through the configuration of individual components, sometimes on purpose the hard way. The reference uses the most recent components at the time of writing:\nHydra v1.10.1 Keto v0.6.0-alpha.1 Kratos v0.5.5-alpha.1 Oathkeeper v0.38.9-beta.1; v0.38.10-beta.1 does not build from sources The reference is kept up to date on a best effort basis.\nCloud native computing ↩︎\nORY ↩︎\nZanzibar: Google’s Consistent, Global Authorization System ↩︎\nReference ORY Docker Compose setup on GitHub ↩︎\n","description":"Thought on the ORY platform and a reference Docker compose","tags":["sso","iam","ory","hydra","keto","kratos","oathkeeper","keycloak"],"title":"ORY reference Docker Compose and thoughts on the platform","uri":"/posts/2021-04-10-ory-reference-docker-compose-and-thoughts-on-the-platform/"},{"content":"","description":"","tags":null,"title":"sso","uri":"/tags/sso/"},{"content":"Updated on 10th of April 2021: The decision to move Apache Mesos to Attic, has been reversed.\nAn end of an era. What’s the better way to summarize that the maintainers of Apache Mesos are now voting on moving the project to Apache Attic. Attic is a place where Apache projects go when they reach end of life.\nIn other words: it’s end of the road for Apache Mesos.\nI was first introduced to Mesos in 2015, back at Technicolor Virdata. A colleague, Nathan, has been going on about for a long time. I was tasked to help him move our Virdata platform to Mesos. It was the first time I was exposed to distributed scheduling. If I recall correctly, it took us about a couple of weeks to get our heads around it. We had to work out the installation and configuration process, figure out how to deploy Marathon. DC/OS did not exist yet.\nTook us another two weeks to have the platform migrated. I have fond memories of nights spent trying to run Apache Spark on Mesos with Docker bridge networking…\nIt was great, such a big improvement over thousands of lines of Chef which we could start deprecating.\nSo yeah, end of an era. Mesos was great but I do feel it was a bit mismanaged. Not by the community though, the community put an awesome effort into maintaining it for such a long time.\nIt’s now between Kubernetes and HashiCorp Nomad to figure it out. What’s positive, they’re solving the same problem but serve different audiences.\nGreat to have a choice.\n","description":"An end of an era","tags":["mesos","kubernetes","k8s","nomad"],"title":"Apache Mesos reaches end of life","uri":"/posts/2021-04-06-apache-mesos-reaches-end-of-life/"},{"content":"","description":"","tags":null,"title":"mesos","uri":"/tags/mesos/"},{"content":"","description":"","tags":null,"title":"nomad","uri":"/tags/nomad/"},{"content":"If you are using golang, there’s a pretty high chance you have used the os.Expand(s string, mapping func(string) string) function in your code already. Or maybe it’s derivative, os.ExpandEnv(s string).\nThe former takes an input string and expands the shell variable-like occurrences with actual shell variable values. For example:\n1 2 3 os.Setenv(\"VARIABLE\", \"hello\") fmt.Println(os.ExpandEnv(\"${VARIABLE}, world!\")) // prints \"hello, world!\" It uses os.Lookup(s string) as the mapping argument to os.Expand.\nPretty often, that may be what is needed and os.ExpandEnv is one of little gems of the golang standard library.\nThe problem with os.ExpandEnv is, if the variable referenced in the string does not exist, it’s replaced with an empty string.\nHowever, consider the following command:\n1 2 3 4 5 6 7 8 9 const command = `cd /tmp/build \u0026\u0026 \\ apkArch=\"$(apk --print-arch)\" \u0026\u0026 \\ case \"${apkArch}\" in \\ aarch64) consulArch='arm64' ;; \\ armhf) consulArch='armhfv6' ;; \\ x86) consulArch='386' ;; \\ x86_64) consulArch='amd64' ;; \\ *) echo \u003e\u00262 \"error: unsupported architecture: ${apkArch} (see ${HASHICORP_RELEASES}/consul/${CONSUL_VERSION}/)\" \u0026\u0026 exit 1 ;; \\ esac` Assuming that the values of HASHICORP_RELEASES and CONSUL_VERSION are passed as environment variables:\n1 2 3 os.Setenv(\"HASHICORP_RELEASES\", \"https://releases.hashicorp.com\") os.Setenv(\"CONSUL_VERSION\", \"1.9.4\") fmt.Println(os.ExpandEnv(command)) would give the following output:\ncd /tmp/build \u0026\u0026 \\ apkArch=\"$(apk --print-arch)\" \u0026\u0026 \\ case \"\" in \\ aarch64) consulArch='arm64' ;; \\ armhf) consulArch='armhfv6' ;; \\ x86) consulArch='386' ;; \\ x86_64) consulArch='amd64' ;; \\ *) echo \u003e\u00262 \"error: unsupported architecture: (see https://releases.hashicorp.com/consul/1.9.4/)\" \u0026\u0026 exit 1 ;; \\ esac The ${apkArch} part was obliterated from the output. This command would never work.\nFortunately, os.Expand comes to the rescue!\n1 2 3 4 5 6 7 8 lookupFunc := func(placeholderName string) string { if value, ok := os.Lookup(placeholderName); ok { return value } // fallback: return fmt.Sprintf(\"$%s\", placeholderName) } fmt.Println(os.Expand(command, lookupFunc)) Aha, now it looks better:\ncd /tmp/build \u0026\u0026 \\ apkArch=\"$(apk --print-arch)\" \u0026\u0026 \\ case \"$apkArch\" in \\ aarch64) consulArch='arm64' ;; \\ armhf) consulArch='armhfv6' ;; \\ x86) consulArch='386' ;; \\ x86_64) consulArch='amd64' ;; \\ *) echo \u003e\u00262 \"error: unsupported architecture: $apkArch (see https://releases.hashicorp.com/consul/1.9.4/)\" \u0026\u0026 exit 1 ;; \\ esac This output would definitely work. But shell strings can be much more complicated than this.\nThe problem with the lookupFunc is that one has to make an upfront decision to surround the fallback with {}.\nAnd there are cases when neither is the right choice.\nConsider the following input, a slightly modified real example coming from the official Postgres 13 Dockerfile:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 const command = `RUN set -eux; \\ export GNUPGHOME=${GNUPGHOME:=$(mktemp -d)}; \\ savedAptMark=\"$(apt-mark showmanual)\"; \\ apt-get update; \\ apt-get install -y --no-install-recommends ca-certificates wget; \\ rm -rf /var/lib/apt/lists/*; \\ dpkgArch=\"$(dpkg --print-architecture | awk -F- '{ print $NF }')\"; \\ wget -O /usr/local/bin/gosu \"https://github.com/tianon/gosu/releases/download/$GOSU_VERSION/gosu-$dpkgArch\"; \\ wget -O /usr/local/bin/gosu.asc \"https://github.com/tianon/gosu/releases/download/$GOSU_VERSION/gosu-$dpkgArch.asc\"; \\ gpg --batch --keyserver hkps://keys.openpgp.org --recv-keys B42F6819007F00F88E364FD4036A9C25BF357DD4; \\ gpg --batch --verify /usr/local/bin/gosu.asc /usr/local/bin/gosu; \\ gpgconf --kill all; \\ rm -rf \"$GNUPGHOME\" /usr/local/bin/gosu.asc; \\ apt-mark auto '.*' \u003e /dev/null; \\ [ -z \"$savedAptMark\" ] || apt-mark manual $savedAptMark \u003e /dev/null; \\ apt-get purge -y --auto-remove -o APT::AutoRemove::RecommendsImportant=false; \\ chmod +x /usr/local/bin/gosu; \\ gosu --version; \\ gosu nobody true ` There are two conflicting cases in this input: export GNUPGHOME=${GNUPGHOME:=$(mktemp -d)}; and | awk -F- '{ print $NF }'.\nIn case of the export command, the surrounding {} must be preserved. The lookupFunc could return the fallback of fmt.Sprintf(\"${%s}\", placeholderName).\nBut in case of | awk -F- '{ print $NF }', surrounding $NF with {} results in an error. The mapper argument of os.Expand fails to tell what the raw input was. Can this be fixed?\nThe answer is to look at the source of os.Expand standard library function. It looks like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 func Expand(s string, mapping func(string) string) string { var buf []byte // ${} is all ASCII, so bytes are fine for this operation. i := 0 for j := 0; j \u003c len(s); j++ { if s[j] == '$' \u0026\u0026 j+1 \u003c len(s) { if buf == nil { buf = make([]byte, 0, 2*len(s)) } buf = append(buf, s[i:j]...) name, w := getShellName(s[j+1:]) if name == \"\" \u0026\u0026 w \u003e 0 { // Encountered invalid syntax; eat the // characters. } else if name == \"\" { // Valid syntax, but $ was not followed by a // name. Leave the dollar character untouched. buf = append(buf, s[j]) } else { buf = append(buf, mapping(name)...) } j += w i = j + 1 } } if buf == nil { return s } return string(buf) + s[i:] } The case we are interested in is the final else. It says:\nif it was a valid shell variable name, replace the value with the value from the mapper\nIf we replaced this code with:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 func Expand(s string, mapping func(string) (string, bool)) string { var buf []byte // ${} is all ASCII, so bytes are fine for this operation. i := 0 for j := 0; j \u003c len(s); j++ { if s[j] == '$' \u0026\u0026 j+1 \u003c len(s) { if buf == nil { buf = make([]byte, 0, 2*len(s)) } buf = append(buf, s[i:j]...) shellNameInput := s[j+1:] name, w := getShellName(shellNameInput) if name == \"\" \u0026\u0026 w \u003e 0 { // Encountered invalid syntax; eat the // characters. } else if name == \"\" { // Valid syntax, but $ was not followed by a // name. Leave the dollar character untouched. buf = append(buf, s[j]) } else { replacement, ok := mapping(name) if ok { buf = append(buf, replacement...) } else { // preserve enclosing {} if shellNameInput[0] == '{' { buf = append(buf, fmt.Sprintf(\"${%s}\", name)...) } else { buf = append(buf, fmt.Sprintf(\"$%s\", name)...) } } } j += w i = j + 1 } } if buf == nil { return s } return string(buf) + s[i:] } We would get the fully correct behavior. We have added:\n1 2 3 buf = append(buf, s[i:j]...) shellNameInput := s[j+1:] // \u003c----- this line name, w := getShellName(shellNameInput) and changed the final else statement to:\n1 2 3 4 5 6 7 8 9 10 11 replacement, ok := mapping(name) if ok { buf = append(buf, replacement...) } else { // preserve enclosing {} if shellNameInput[0] == '{' { buf = append(buf, fmt.Sprintf(\"${%s}\", name)...) } else { buf = append(buf, fmt.Sprintf(\"$%s\", name)...) } } This bit reads as follows:\nif the mapper found the value, use it; otherwise fall back to the original value but preserve surrounding braces\nThe result of the custom Expand:\n1 2 os.Setenv(\"GOSU_VERSION\", \"1.12\") fmt.Println(Expand(command, os.Lookup)) is correct:\nRUN set -eux; \\ export GNUPGHOME=${GNUPGHOME:=$(mktemp -d)}; \\ savedAptMark=\"$(apt-mark showmanual)\"; \\ apt-get update; \\ apt-get install -y --no-install-recommends ca-certificates wget; \\ rm -rf /var/lib/apt/lists/*; \\ dpkgArch=\"$(dpkg --print-architecture | awk -F- '{ print $NF }')\"; \\ wget -O /usr/local/bin/gosu \"https://github.com/tianon/gosu/releases/download/1.12/gosu-$dpkgArch\"; \\ wget -O /usr/local/bin/gosu.asc \"https://github.com/tianon/gosu/releases/download/1.12/gosu-$dpkgArch.asc\"; \\ gpg --batch --keyserver hkps://keys.openpgp.org --recv-keys B42F6819007F00F88E364FD4036A9C25BF357DD4; \\ gpg --batch --verify /usr/local/bin/gosu.asc /usr/local/bin/gosu; \\ gpgconf --kill all; \\ rm -rf \"$GNUPGHOME\" /usr/local/bin/gosu.asc; \\ apt-mark auto '.*' \u003e /dev/null; \\ [ -z \"$savedAptMark\" ] || apt-mark manual $savedAptMark \u003e /dev/null; \\ apt-get purge -y --auto-remove -o APT::AutoRemove::RecommendsImportant=false; \\ chmod +x /usr/local/bin/gosu; \\ gosu --version; \\ gosu nobody true A full implementation is here.\n","description":"They are awesome but could be better","tags":["golang"],"title":"On golang os.Expand and os.ExpandEnv","uri":"/posts/2021-04-06-on-golang-osexpand-and-osexpandenv/"},{"content":"Well, sort of. But bear with me.\nbackground A couple of days ago, Confluent announced a ZooKeeper free Kafka 2.8 RC0 available for testing. A fantastic effort, great achievement by all the contributors who made it happen.\nIn the typical Hacker News fashion, a post about Kafka always triggers an inevitable “Puslar vs Kafka” discussion. These always remind me of one of my main gripes related to Kafka: no infinite retention. I’ve written about it close to five years ago1.\nSo, apparently there is a KIP for open source Kafka tiered storage2 which would enable this kind of behavior and take it even further. There is definitely a paid feature on the Confluent platform enabling tiered storage3.\nHowever, as a user of the open source Kafka, I can’t use it.\nafter 5 years of waiting I kinda hacked it myself in. Here’s how I’ve done it.\nI have forked Kafka from https://github.com/apache/kafka and checked out the 2.8 branch, the one with no ZooKeeper. To be exact, the 08849bc3909d4fabda965c8ca7f78b0feb5473d2 commit.\nI then applied this diff:\nbuild Kafka from sources 1 ./gradlewAll releaseTarGz Now, I can start my new Kafka like this:\n1 2 3 4 5 6 AWS_SHARED_CREDENTIALS_FILE=~/.aws/credentials \\ AWS_PROFILE=a-profile-with-bucket-access \\ KAFKA_AWS_S3_UPLOADS=1 \\ KAFKA_AWS_S3_REGION=eu-central-1 \\ KAFKA_AWS_S3_BUCKET=my-kafka-logs-bucket \\ ./kafka_2.13-2.8.0-SNAPSHOT/bin/kafka-server-start.sh ./kafka_2.13-2.8.0-SNAPSHOT/config/kraft/server.properties Now, every time Kafka is about to delete a log segment, it will put it in S3 first. Only the log files are stored because there is no need to have an index. Whenever I need data from an older segment, I can download the segment from S3, rebuild the index and read out all data from the segment.\nI can process dozens of segments in parallel, regardless of the total partition size.\nquick and dirty test method Create a topic with a rather small segment:\n~/dev/kafka-2.8.0/kafka_2.13-2.8.0-SNAPSHOT/bin/kafka-topics.sh \\ --create \\ --topic test-topic \\ --partitions 1 \\ --replication-factor 1 \\ --bootstrap-server localhost:9092 \\ --config segment.bytes=524288 Write some data to the topic with a tool of choice. I can see new segments rolling in:\n[2021-04-02 23:32:57,551] INFO [BrokerLifecycleManager id=1] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager) [2021-04-02 23:32:57,552] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(test-topic-0) (kafka.server.ReplicaFetcherManager) [2021-04-02 23:36:38,025] INFO [ProducerStateManager partition=test-topic-0] Writing producer snapshot at offset 3830 (kafka.log.ProducerStateManager) [2021-04-02 23:36:38,028] INFO [Log partition=test-topic-0, dir=/tmp/kraft-combined-logs] Rolled new log segment at offset 3830 in 5 ms. (kafka.log.Log) [2021-04-02 23:36:44,082] INFO [ProducerStateManager partition=test-topic-0] Writing producer snapshot at offset 7646 (kafka.log.ProducerStateManager) [2021-04-02 23:36:44,083] INFO [Log partition=test-topic-0, dir=/tmp/kraft-combined-logs] Rolled new log segment at offset 7646 in 3 ms. (kafka.log.Log) Finally, delete the topic from Kafka:\n[2021-04-02 23:37:52,732] INFO [Controller 1] Removed topic test-topic with ID H6YadkN7SUGXheMu6MJ1uA. (org.apache.kafka.controller.ReplicationControlManager) [2021-04-02 23:37:52,759] INFO [BrokerMetadataListener id=1] Processing deletion of topic test-topic with id H6YadkN7SUGXheMu6MJ1uA (kafka.server.metadata.BrokerMetadataListener) [2021-04-02 23:37:52,761] INFO [GroupCoordinator 1]: Removed 0 offsets associated with deleted partitions: test-topic-0. (kafka.coordinator.group.GroupCoordinator) [2021-04-02 23:37:52,768] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(test-topic-0) (kafka.server.ReplicaFetcherManager) [2021-04-02 23:37:52,768] INFO [ReplicaAlterLogDirsManager on broker 1] Removed fetcher for partitions Set(test-topic-0) (kafka.server.ReplicaAlterLogDirsManager) [2021-04-02 23:37:52,776] INFO Log for partition test-topic-0 is renamed to /tmp/kraft-combined-logs/test-topic-0.fe590aa674124d0c81bb5b3d718a73ab-delete and is scheduled for deletion (kafka.log.LogManager) After about 60 seconds, the segments are uploaded to S3 and deleted from disk:\n[2021-04-02 23:38:52,781] INFO [Log partition=test-topic-0, dir=/tmp/kraft-combined-logs] Deleting segments as the log has been deleted: LogSegment(baseOffset=0, size=524285, lastModifiedTime=1617399398000, largestRecordTimestamp=Some(1617399398018)),LogSegment(baseOffset=3830, size=524270, lastModifiedTime=1617399404000, largestRecordTimestamp=Some(1617399404079)),LogSegment(baseOffset=7646, size=48257, lastModifiedTime=1617399404000, largestRecordTimestamp=Some(1617399404638)) (kafka.log.Log) [2021-04-02 23:38:52,786] INFO [Log partition=test-topic-0, dir=/tmp/kraft-combined-logs] Deleting segment files LogSegment(baseOffset=0, size=524285, lastModifiedTime=1617399398000, largestRecordTimestamp=Some(1617399398018)),LogSegment(baseOffset=3830, size=524270, lastModifiedTime=1617399404000, largestRecordTimestamp=Some(1617399404079)),LogSegment(baseOffset=7646, size=48257, lastModifiedTime=1617399404000, largestRecordTimestamp=Some(1617399404638)) (kafka.log.Log) [2021-04-02 23:38:57,798] INFO Uploading segment noan.local/test-topic-0.fe590aa674124d0c81bb5b3d718a73ab-delete/00000000000000000000.log.deleted with size 524285 prior to delete... (kafka.log.LogSegment) [2021-04-02 23:38:59,210] INFO Uploaded segment prior to delete, S3 ETag: 769bf025a6e85a7402509142de3d149a, took 6417ms (kafka.log.LogSegment) [2021-04-02 23:38:59,212] INFO Deleted log /tmp/kraft-combined-logs/test-topic-0.fe590aa674124d0c81bb5b3d718a73ab-delete/00000000000000000000.log.deleted. (kafka.log.LogSegment) [2021-04-02 23:38:59,219] INFO Deleted offset index /tmp/kraft-combined-logs/test-topic-0.fe590aa674124d0c81bb5b3d718a73ab-delete/00000000000000000000.index.deleted. (kafka.log.LogSegment) [2021-04-02 23:38:59,219] INFO Deleted time index /tmp/kraft-combined-logs/test-topic-0.fe590aa674124d0c81bb5b3d718a73ab-delete/00000000000000000000.timeindex.deleted. (kafka.log.LogSegment) [2021-04-02 23:39:04,223] INFO Uploading segment noan.local/test-topic-0.fe590aa674124d0c81bb5b3d718a73ab-delete/00000000000000003830.log.deleted with size 524270 prior to delete... (kafka.log.LogSegment) [2021-04-02 23:39:04,840] INFO Uploaded segment prior to delete, S3 ETag: 06d987e417031ca3474a4fcf6efc72de, took 5620ms (kafka.log.LogSegment) [2021-04-02 23:39:04,842] INFO Deleted log /tmp/kraft-combined-logs/test-topic-0.fe590aa674124d0c81bb5b3d718a73ab-delete/00000000000000003830.log.deleted. (kafka.log.LogSegment) [2021-04-02 23:39:04,842] INFO Deleted offset index /tmp/kraft-combined-logs/test-topic-0.fe590aa674124d0c81bb5b3d718a73ab-delete/00000000000000003830.index.deleted. (kafka.log.LogSegment) [2021-04-02 23:39:04,843] INFO Deleted time index /tmp/kraft-combined-logs/test-topic-0.fe590aa674124d0c81bb5b3d718a73ab-delete/00000000000000003830.timeindex.deleted. (kafka.log.LogSegment) [2021-04-02 23:39:09,848] INFO Uploading segment noan.local/test-topic-0.fe590aa674124d0c81bb5b3d718a73ab-delete/00000000000000007646.log.deleted with size 48257 prior to delete... (kafka.log.LogSegment) [2021-04-02 23:39:10,103] INFO Uploaded segment prior to delete, S3 ETag: 624d8699a9f53ecb24143e197c4f1ccf, took 5259ms (kafka.log.LogSegment) [2021-04-02 23:39:10,104] INFO Deleted log /tmp/kraft-combined-logs/test-topic-0.fe590aa674124d0c81bb5b3d718a73ab-delete/00000000000000007646.log.deleted. (kafka.log.LogSegment) [2021-04-02 23:39:10,104] INFO Deleted offset index /tmp/kraft-combined-logs/test-topic-0.fe590aa674124d0c81bb5b3d718a73ab-delete/00000000000000007646.index.deleted. (kafka.log.LogSegment) [2021-04-02 23:39:10,105] INFO Deleted time index /tmp/kraft-combined-logs/test-topic-0.fe590aa674124d0c81bb5b3d718a73ab-delete/00000000000000007646.timeindex.deleted. (kafka.log.LogSegment) [2021-04-02 23:39:10,111] INFO Deleted log for partition test-topic-0 in /tmp/kraft-combined-logs/test-topic-0.fe590aa674124d0c81bb5b3d718a73ab-delete. (kafka.log.LogManager) The upload would take less time if I was running my Kafka in EC2. This method works for deleted topics, regular segment rolls and also compacted topics. Basically, every time a segment is about to be removed from disk.\nThe case for Kafka cold storage ↩︎\nKIP-405: Kafka Tiered Storage ↩︎\nConfluent Platform infinite Kafka retention ↩︎\n","description":"Well, sort of…","tags":["kafka"],"title":"I hacked infinite retention into my open source Kafka","uri":"/posts/2021-04-02-kafka-infinite-retention/"},{"content":"","description":"","tags":null,"title":"kafka","uri":"/tags/kafka/"},{"content":"Long time coming but the KIP-5001 has finally landed. It’s official, Apache Kafka does not require ZooKeeper anymore. The KRaft, the Kafka Raft implementation, is not recommended for production yet. Full announcement from Confluent is here2.\nRegardless, this is a fantastic milestone and a kudos to all the contributors for making this happen as the simplification in the operations will be significant.\ntaking it for a test drive First, generate a new cluster ID:\n1 2 $ ~/dev/kafka-2.8/bin/kafka-storage.sh random-uuid HCsQovjcTs-8xhS1DSU5Gw Next, format the storage directory. The default directory is /tmp/kraft-combined-logs and the setting can be found under the log.dirs property of the new config/kraft/server.properties file:\n1 2 $ ~/dev/kafka-2.8/bin/kafka-storage.sh format -t HCsQovjcTs-8xhS1DSU5Gw -c ~/dev/kafka-2.8/config/kraft/server.properties Formatting /tmp/kraft-combined-logs And simply start the broker:\n1 $ ~/dev/kafka-2.8/bin/kafka-server-start.sh ~/dev/kafka-2.8/config/kraft/server.properties Which produces the following output:\n[2021-03-31 23:25:56,097] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$) [2021-03-31 23:25:56,425] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util) [2021-03-31 23:25:56,664] INFO [Log partition=@metadata-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log) [2021-03-31 23:25:56,727] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper) [2021-03-31 23:25:56,914] INFO [RaftManager nodeId=1] Completed transition to Unattached(epoch=0, voters=[1], electionTimeoutMs=1165) (org.apache.kafka.raft.QuorumState) [2021-03-31 23:25:56,917] INFO [RaftManager nodeId=1] Completed transition to Candidate(localId=1, epoch=1, retries=1, electionTimeoutMs=1680) (org.apache.kafka.raft.QuorumState) [2021-03-31 23:25:56,921] INFO [RaftManager nodeId=1] Completed transition to Leader(localId=1, epoch=1, epochStartOffset=0) (org.apache.kafka.raft.QuorumState) [2021-03-31 23:25:57,004] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler) [2021-03-31 23:25:57,009] INFO [kafka-raft-outbound-request-thread]: Starting (kafka.raft.RaftSendThread) ... [2021-03-31 23:25:57,959] INFO Kafka version: 2.8.0-SNAPSHOT (org.apache.kafka.common.utils.AppInfoParser) [2021-03-31 23:25:57,959] INFO Kafka commitId: 08849bc3909d4fab (org.apache.kafka.common.utils.AppInfoParser) [2021-03-31 23:25:57,959] INFO Kafka startTimeMs: 1617225957958 (org.apache.kafka.common.utils.AppInfoParser) [2021-03-31 23:25:57,959] INFO [Controller 1] The request from broker 1 to unfence has been granted because it has caught up with the last committed metadata offset 1. (org.apache.kafka.controller.BrokerHeartbeatManager) [2021-03-31 23:25:57,960] INFO Kafka Server started (kafka.server.KafkaRaftServer) [2021-03-31 23:25:57,963] INFO [Controller 1] Unfenced broker: UnfenceBrokerRecord(id=1, epoch=0) (org.apache.kafka.controller.ClusterControlManager) [2021-03-31 23:25:57,989] INFO [BrokerLifecycleManager id=1] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager) It’s alive. A topic can be created using the usual tooling:\n1 2 3 4 5 $ ~/dev/kafka-2.8/bin/kafka-topics.sh --create \\ --topic test-topic \\ --partitions 1 \\ --replication-factor 1 \\ --bootstrap-server localhost:9092 [2021-03-31 23:28:21,836] INFO [Controller 1] createTopics result(s): CreatableTopic(name='test-topic', numPartitions=1, replicationFactor=1, assignments=[]): SUCCESS (org.apache.kafka.controller.ReplicationControlManager) [2021-03-31 23:28:21,838] INFO [Controller 1] Created topic test-topic with ID H6YadkN7SUGXheMu6MJ1uA. (org.apache.kafka.controller.ReplicationControlManager) [2021-03-31 23:28:21,838] INFO [Controller 1] Created partition H6YadkN7SUGXheMu6MJ1uA:0 with PartitionControlInfo(replicas=[1], isr=[1], removingReplicas=null, addingReplicas=null, leader=1, leaderEpoch=0, partitionEpoch=0). (org.apache.kafka.controller.ReplicationControlManager) [2021-03-31 23:28:21,904] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(test-topic-0) (kafka.server.ReplicaFetcherManager) [2021-03-31 23:28:21,926] INFO [Log partition=test-topic-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log) [2021-03-31 23:28:21,930] INFO Created log for partition test-topic-0 in /tmp/kraft-combined-logs/test-topic-0 with properties {compression.type -\u003e producer, message.downconversion.enable -\u003e true, min.insync.replicas -\u003e 1, segment.jitter.ms -\u003e 0, cleanup.policy -\u003e [delete], flush.ms -\u003e 9223372036854775807, etention.ms -\u003e 604800000, flush.messages -\u003e 9223372036854775807, message.format.version -\u003e 2.8-IV1, file.delete.delay.ms -\u003e 60000, max.compaction.lag.ms -\u003e 9223372036854775807, max.message.bytes -\u003e 1048588, min.compaction.lag.ms -\u003e 0, message.timestamp.type -\u003e CreateTime, preallocate -\u003e false, min.cleanable.dirty.ratio -\u003e 0.5, index.interval.bytes -\u003e 4096, unclean.leader.election.enable -\u003e false, retention.bytes -\u003e -1, delete.retention.ms -\u003e 86400000, segment.ms -\u003e 604800000, message.timestamp.difference.max.ms -\u003e 9223372036854775807, segment.index.bytes -\u003e 10485760}. (kafka.log.LogManager) [2021-03-31 23:28:21,931] INFO [Partition test-topic-0 broker=1] No checkpointed highwatermark is found for partition test-topic-0 (kafka.cluster.Partition) [2021-03-31 23:28:21,932] INFO [Partition test-topic-0 broker=1] Log loaded for partition test-topic-0 with initial high watermark 0 (kafka.cluster.Partition) Impressive.\nKIP-500  ↩︎\nApache Kafka Made Simple: A First Glimpse of a Kafka Without ZooKeeper ↩︎\n","description":"KIP-500 is implemented and Kafka is now completely standalone","tags":["kafka","zookeeper"],"title":"Kafka 2.8 is out in the wild and does not need ZooKeeper anymore","uri":"/posts/2021-03-31-kafka-2.8-does-not-need-zookeeper-anymore/"},{"content":"","description":"","tags":null,"title":"zookeeper","uri":"/tags/zookeeper/"},{"content":"","description":"","tags":null,"title":"ca","uri":"/tags/ca/"},{"content":"the problem Currently, when a rootfs is built, the guest is started with an SSH server and the bootstrap process executes via an SSH connection. I don’t like this and want to replace the SSH method with an MMDS based solution. MMDS is already present in the firebuild run command.\nrun uses the vminit component from firebuild-mmds. When the guest starts, the vminit guest service connects to the MMDS endpoint, downloads the metadata and configures the VM. This is pretty similar to cloud-init but I don’t want cloud-init at this stage. Writing a cloud-init provider in Python is a bit of a head scratcher, can be done but maybe some other time.\nrootfs bootstrap is pretty similar to run in the sense that it also starts a guest VM. There is no reason why it should not work the same way. Bye SSH, welcome MMDS, easy peasy. Not so… The big difference between run and rootfs is:\nrootfs requires access to RUN commands and ADD / COPY resources present in the Docker artifact\nMultiple approaches are possible. Firecracker supports vsock devices. The guest can connect to the host and vice-versa, even without a network interface. Very nice but vsock is pretty low level and since the guest requires at least egress—Dockerfiles are full of package installation and pulling random stuff from the Internet—there was really no point going that way.\nAn alternative is a host service which the guest can connect to and fetch whatever is needed. I originally wanted a HTTP service but since there is a need of bi-directional communication without much protocol overhead, gRPC seems to be a better fit.\nkiss, keep it simply secure I opted for the following:\nfirebuild will start a bootstrap only gRPC server, one per rootfs command run firebuild will put the bootstrap endpoint in MMDS the guest will connect via vminit to MMDS and discover the bootstrap endpoint the guest will connect to the gRPC service via vminit bootstrap, download commands and resources and execute the bootstrap What I wanted was that every connection is always TLS protected, even when the operator would not configure TLS for the bootstrap process. In fact, mutual TLS is preferred so I made a decision to never allow a non-TLS connection or insecure certificates.\nthe CA chain and client certificate will be delivered via MMDS metadata the solution, embedded CA No insecure certificates imply a certificate authority being available. I’ve written about certificate authorities before1. Deploying something like Vault is not really difficult but during testing and development, considering the requirements, adds some friction.\nI don’t like managing development dependency certificate files. I mean, I’ve done it but it’s always a bit messy. It requires extra tools, documentation and there are those pesky extra steps to follow in the readme, or make steps to execute.\nfirebuild is written in Golang which has an awesome first class support for anything TLS/PKI/x509 related. Turns out a mini CA is less than 300 lines of code!\nfirebuild will use an embedded certificate authority. It’s lightweight and does only bare minimum to look like a CA but support a short-lived rootfs build process. If cacert, server cert and server key are not provided, it does the following:\non start, generate the root CA certificate optionally, when configured, generate an intermediate CA generate a server *tls.Config with a newly generated server certificate generate a client *tls.Config with a newly generated client certificate automatically configures the certificate and the client *tls.Config to fulfill the gRPC server name requirement Here’s how to use it:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 package main import ( \"github.com/combust-labs/firebuild-embedded-ca/ca\" \"github.com/hashicorp/go-hclog\" \"google.golang.org/grpc\" \"google.golang.org/grpc/credentials\" ) func main() { logger := hclog.Default() grpcServiceName := \"grpc-service-name\" grpcServerOptions := []grpc.ServerOption{} embeddedCA, embeddedCAErr := ca.NewDefaultEmbeddedCAWithLogger(\u0026ca.EmbeddedCAConfig{ Addresses: []string{grpcServiceName}, KeySize: 4096, }, logger.Named(\"embdedded-ca\")) if embeddedCAErr != nil { panic(embeddedCAErr) } serverTLSConfig, tlsConfigErr := embeddedCA.NewServerCertTLSConfig() if tlsConfigErr != nil { panic(tlsConfigErr) } clientTLSConfig, err := embeddedCA.NewClientCertTLSConfig(grpcServiceName) if err != nil { panic(embeddedCAErr) } grpcServerOptions = append(grpcServerOptions, grpc.Creds(credentials.NewTLS(serverTLSConfig))) listener, listenerErr := net.Listen(\"tcp\", \"127.0.0.1:0\") if listenerErr != nil { panic(listenerErr) } grpcServer = grpc.NewServer(grpcServerOptions...) ///proto.Register...(grpcServer, ...) chanErr := make(chan struct{}) go func() { if err := s.srv.Serve(listener); err != nil { logger.Error(\"failed grpc serve\", \"reason\", err) close(chanErr) } }() grpcConn, _ := grpc.Dial(listener.Addr().String(), grpc.WithTransportCredentials(credentials.NewTLS(clientTLSConfig))) // ... } With key sizes of 2048 bits, it takes a reasonable time to start, the overhead isn’t significant. Considering that rootfs build is not very time sensitive, this seems pretty okay. Of course, there will be an option to use an already deployed CA instead of this one.\nThe embedded CA is available on GitHub2 under an Apache 2 license.\nCertificate Authority is not Voodoo ↩︎\nembedded CA on GitHub ↩︎\n","description":"There’s a certificate authority right in firebuild","tags":["ca","firebuild","go","grpc","pki","tls"],"title":"firebuild rootfs - gRPC with mTLS","uri":"/posts/2021-03-28-firebuild-rootfs-grpc-with-mtls/"},{"content":"","description":"","tags":null,"title":"pki","uri":"/tags/pki/"},{"content":"what is Firecracker Firecracker is a virtualization technology for creating and managing secure, multi-tenant services suited for container-like and serverless scenarios. Firecracker workloads run in virtual machines, not containers. Unlike containers, they benefit from extra isolation properties provided by the hardware virtualization. Similar to containers, Firecracker VMs—microVMs—are lightweight and fast to boot. Like containers, they can be treated like cattle. They combine the flexibility of containers and security of virtual machines. These little things can be started in as little as 125 milliseconds and a single host can manage thousands of them! Firecracker was developed at Amazon Web Services primarily for Lambda and Fargate offerings.\nFirecracker uses Kernel Virtual Machine (KVM) to create and run microVMs. A minimalist design is achieved by removing unnecessary devices and guest-facing functionality. This reduces the memory footprint and attack surface of each individual VM leading to better utilization and increased security. At minimum, a microVM requires a Linux kernel image and a root file system. Networking can be provided by setting up interfaces manually or with container network interface (CNI).\nFirecracker is a couple of years old. Pretty young in the technology world but there are already interesting integrations out there. Kata Containers and WeaveWorks Ignite are the major ones.\nfirebuild There is only so much one can learn by looking at existing tools. The best way is to take something and build another useful thing on top of it. Only this way one can hit roadblocks cleared by others. Only this way one can investigate alternative avenues, possibly not considered before. That is why a few weeks ago I have started working on firebuild. The source code is on GitHub1.\nWith firebuild it is possible to:\nbuild root file systems directly from Dockerfiles tag and version root file systems run and manage microVMs on a single host define run profiles The concept of firebuild is to leverage as much of the existing Docker world as possible. There are thousands of Docker images out there. Docker images are awesome because they encapsulate the software we want to run in our workloads, they also encapsulate dependencies. Dockerfiles are what Docker images are built from. Dockeriles are the blueprints of the modern infrastructure. There are thousands of them for almost anything one can imagine and new ones are very easy to write.\nan image is worth more than a thousand words Ah, but the idea is pretty difficult to visualize with a single image. So, instead, let me walk you though this example of running HashiCorp Consul 1.9.4 on Firecracker. I promise, any questions are answered further.\nBefore going all in, some prerequisites2.\ncreate a firebuild profile 1 2 3 4 5 6 7 8 9 sudo $GOPATH/bin/firebuild profile-create \\ --profile=standard \\ --binary-firecracker=$(readlink /usr/bin/firecracker) \\ --binary-jailer=$(readlink /usr/bin/jailer) \\ --chroot-base=/fc/jail \\ --run-cache=/fc/cache \\ --storage-provider=directory \\ --storage-provider-property-string=\"rootfs-storage-root=/fc/rootfs\" \\ --storage-provider-property-string=\"kernel-storage-root=/fc/vmlinux\" create a base operating system root file system (baseos) firebuild uses the Docker metaphor. An image of an application is built FROM a base. An application image can be built FROM alpine:3.13, for example. Or FROM debian:buster-slim, or FROM registry.access.redhat.com/ubi8/ubi-minimal:8.3 and dozens others.\nIn order to fulfill those semantics, a base operating system image must be built before the application root file system can be created.\n1 2 3 sudo $GOPATH/bin/firebuild baseos \\ --profile=standard \\ --dockerfile $(pwd)/baseos/_/alpine/3.12/Dockerfile create a root file system of the application (rootfs) To run an instance of HashiCorp Consul, firebuild requires the Consul application root file system. To build one:\n1 2 3 4 5 6 7 sudo $GOPATH/bin/firebuild rootfs \\ --profile=standard \\ --dockerfile=git+https://github.com/hashicorp/docker-consul.git:/0.X/Dockerfile \\ --cni-network-name=machine-builds \\ --ssh-user=alpine \\ --vmlinux-id=vmlinux-v5.8 \\ --tag=combust-labs/consul:1.9.4 start the application 1 2 3 4 5 sudo $GOPATH/bin/firebuild run \\ --profile=standard \\ --from=combust-labs/consul:1.9.4 \\ --cni-network-name=machines \\ --vmlinux-id=vmlinux-v5.8 query Consul First, find the VM ID:\n1 2 3 sudo $GOPATH/bin/firebuild ls \\ --profile=standard \\ --log-as-json 2\u003e\u00261 | jq '.id' -r In my case, the value is wcabty1922gloailwrce. I used it to get the IP address of the VM:\n1 2 3 $ sudo $GOPATH/bin/firebuild inspect \\ --profile=standard \\ --vmm-id=wcabty1922gloailwrce | jq '.NetworkInterfaces[0].StaticConfiguration.IPConfiguration.IP' -r The command returned 192.168.127.89. I could query Consul via REST API:\n1 2 curl http://192.168.127.89:8500/v1/status/leader \"127.0.0.1:8300\" what the heck happened I have started by creating a firebuild profile. Technically firebuild does not require one. Common arguments may be provided on every execution. The profile exists for two reasons:\nit makes subsequent operations more concise by moving the tedious arguments away provides extra isolation with different chroots, cache directories, and image / kernel catalogs The directories referenced in the profile must exist before a profile can be created.\nIn the next step, I have built a base operating system root file system. The elephant in the room question is:\nWhy does this tool even require that step?\nTypical Linux in Docker has many parts removed. For example, there is no init system. Further, different base Docker images have often completely different sets of tools available. All that is for a good reason: Docker images supposed to be small, must start fast and limit the potential attack surface by removing what’s unnecessary.\nfirebuild builds Firecracker virtual machines. It does so from Dockerfile blueprints.\nIn order to provide a consistent experience, it requires a more or less functional multi-user Linux installation with components otherwise hidden in the Docker or OCI runtime. These base Linux installations are built from firebuild provided Dockerfiles, the --dockerfile $(pwd)/baseos/_/alpine/3.12/Dockerfile is a base Alpine 3.12. All the commands above were executed from $GOPATH/src/github.com/combust-labs/firebuild directory, hence the use of $(pwd) in the baseos build.\nfirebuild uses Docker to build the base operating root file system by:\nbuilding a Docker image from the provided Dockerfile starting a container from newly built image exporting the root file system of the container to the ext4 file on the host using Docker API exec removing the container and the image persisting the built file in the storage provider and namespacing it, the example above results in the root file system stored in /fc/rootfs/_/alpine/3.12/rootfs persisting the build metadata next to the root file system file, above example gives /fc/rootfs/_/alpine/3.12/metadata.json This custom firebuild provided Dockerfile is based on an upstream alpine:3.12 from Docker Hub.\nThe primary reason for following this path is to enable building Firecracker VMs from upstream Dockerfiles as often as possible. Other tools out there enable converting a Docker container into a rootfs file but to achieve that full VM experience, a Docker container has to be launched from a hand crafted Dockerfile or extra packages have to be installed on the running container before the export. Dockerfiles are fully auditable but these extra steps are not. The steps often differ between containers. It might be difficult to track how the rootfs was built, some benefits of using a blueprint could be lost.\nThe step 2 of the example builds Consul directly from the official HashiCorp Docker images GitHub repository. The application root file system was built using the rootfs command.\nNote: I refer to the application root file system as rootfs. Bit confusing at first because the result of the baseos command is technically also a rootfs. However, to mentally distinguish one from the other, I refer to to the base OS using the term baseos and an application is a rootfs. This may change in the future.\nThe rootfs command does much more work than the baseos command.\nIt starts by fetching a Dockerfile from a source given via the --dockerfile argument. The source can be one of:\na git+http(s):// style URL pointing at a git repository (does not have to be GitHub) a http:// of https:// URL, be careful here: There Will Be Dragons (read more3) a local file an inline Dockerfile standard ssh://, git:// and git+ssh:// URL with a Dockerfile path appended via :/path/to/Dockerfile The most convenient is the local file system build or a git repository. If a git repository is used, firebuild will clone a complete repository to a temporary directory and treat the build further as a local file system build. Once the sources are on disk, firebuild loads and parses the Dockerfile. This part is preliminary and will change in favor of unattended bootstrap without SSH requirement: Next, a build time VM is started, firebuild connects to it via SSH and runs all commands from the Dockerfile against that VM.\nResources referenced with ADD and COPY commands are treated likewise and supported. Remote resources are supported. firebuild does its best to properly reflect any WORKDIR, USER and SHELL conditions. It supports --chown flags for ADD and COPY.\nWhat’s more, firebuild supports multi-stage builds. firebuild will build any stages with FROM ... as as regular Docker images and extract resources from the stage to the main build when COPY --from= is found. For example, it’s perfectly fine to build a Kafka Proxy root file system from:\n1 2 3 4 5 6 7 sudo $GOPATH/bin/firebuild rootfs \\ --profile=standard \\ --dockerfile=git+https://github.com/grepplabs/kafka-proxy.git:/Dockerfile#v0.2.8 \\ --cni-network-name=machine-builds \\ --ssh-user=alpine \\ --vmlinux-id=vmlinux-v5.8 \\ --tag=combust-labs/kafka-proxy:0.2.8 The Dockerfile commands statements which are not supported: ONBUILD, HEALTHCHECK and STOPSIGNAL (although the last one will be supported at a later stage).\nOnce all of that is finished, the build VM will be stopped, cleaned up and the resulting root file system will be persisted in the storage provider. A metadata file is stored next to the root file system. Currently, only the directory based storage provider is available.\nFinally, a resulting application is launched with the run command. The run command uses an unattended, cloud-init like mechanism. The metadata of the baseos and rootfs is combined. A guest facing version is put in MMDS (the Firecracker machine metadata service). MMDS provides a HTTP API available to both: the host and the guest. By default, if the guest was started with --allow-mmds flag, it can reach that API via 169.254.169.254 IP address. firebuild uses MMDS by default for all guests but this can be disabled. The guest facing metadata contains a bunch of information required to bootstrap the VM in a cloud-init style. These are fairly short so let’s look at an example:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 { \"latest\": { \"meta-data\": { \"Drives\": { \"1\": { \"DriveID\": \"1\", \"IsReadOnly\": \"false\", \"IsRootDevice\": \"true\", \"Partuuid\": \"\", \"PathOnHost\": \"rootfs\" } }, \"EntrypointJSON\": \"{\\\"Cmd\\\":[\\\"agent\\\",\\\"-dev\\\",\\\"-client\\\",\\\"0.0.0.0\\\"],\\\"EntryPoint\\\":[\\\"docker-entrypoint.sh\\\"],\\\"Env\\\":{\\\"HASHICORP_RELEASES\\\":\\\"https://releases.hashicorp.com\\\"},\\\"Shell\\\":[\\\"/bin/sh\\\",\\\"-c\\\"],\\\"User\\\":\\\"0:0\\\",\\\"Workdir\\\":\\\"/\\\"}\", \"Env\": {}, \"ImageTag\": \"combust-labs/consul:1.9.4\", \"LocalHostname\": \"sharp-mirzakhani\", \"Machine\": { \"CPU\": \"1\", \"CPUTemplate\": \"\", \"HTEnabled\": \"false\", \"KernelArgs\": \"console=ttyS0 noapic reboot=k panic=1 pci=off nomodules rw\", \"Mem\": \"128\", \"VMLinux\": \"vmlinux-v5.8\" }, \"Network\": { \"CniNetworkName\": \"machines\", \"Interfaces\": { \"b6:16:f2:3d:29:cf\": { \"Gateway\": \"192.168.127.1\", \"HostDeviceName\": \"tap0\", \"IfName\": \"\", \"IP\": \"192.168.127.89\", \"IPAddr\": \"192.168.127.89/24\", \"IPMask\": \"ffffff00\", \"IPNet\": \"ip+net\", \"NameServers\": \"\" } } }, \"Users\": {}, \"VMMID\": \"wcabty1922gloailwrce\" } } } The metadata contains information about attached drives, network interfaces, simple machine data, entrypoint info and user’s SSH keys, if --identity-file and --ssh-user arguments were provided. The component responsible for bootstrapping the VM from this data is called vminit and can be found in this GitHub repository4. The compiled binary is baked into the baseos (suboptimal but it’s a first iteration) and invoked as a system service on VM start.\nCurrently, vminit does the following:\nupdate /etc/hosts file if the VM has a network interface and make sure the VM resolves itself via configured hostname on the interface IP address update /etc/hostname to the configured hostname create an environment variables /etc/profile.d/run-env.sh file for any variables passed via --env and --env-file flags of the run command when users contains a user entry with SSH keys, write those SSH keys to the respective authorized_keys file to enable SSH access; an example of a user entry: 1 2 3 4 5 \"Users\": { \"alpine\": { \"SSHKeys\": \"ssh-rsa ... \\nssh-rsa ...\\n\" } } write the /usr/bin/firebuild-entrypoint.sh program responsible for invoking the entrypoint from MMDS data When the machine starts, vminit looks for the /usr/bin/firebuild-entrypoint.sh and if one is found, executes it. Fingers crossed, things went well and the application starts automatically.\nThat was a high level overview of the process.\nother useful VM related commands List running VMs:\n1 sudo firebuild ls --profile=standard Inspect the metadata of a running VM:\n1 sudo firebuild inspect --profile=standard --vmm-id=... Terminate a running VM:\n1 sudo firebuild kill --profile=standard --vmm-id=... unclean shutdowns Firecracker VMs will stop when a reboot command is issued in the guest. I call these unclean meaning that they will leave a bunch of VM related directories on disk:\nthe jail directory the run cache directory the CNI cache for the VM interface and a veth pair To mass-clean all these for all exited VMs, run:\n1 sudo firebuild purge --profile=standard profile commands List profiles:\n1 sudo firebuild profile-ls Inspect a profile:\n1 sudo firebuild profile-inspect --profile=... Profiles may be updated by issuing subsequent profile-create commands with a name of an existing profile.\nwhat’s coming next These are still early stages for firebuild. There are many things to improve.\nshort term tests, tests, tests, …, end to end tests remove the requirement to have SSH access during rootfs build and move to the MMDS / vminit build add support for building directly from Docker images for special cases where the Dockerfile might not be available or is difficult to handle, and example is Jaeger Docker image where the Dockerfile does not incorporate the binary artifact build add a command to build a Linux kernel image directly from the tool manage resolv.conf and nsswitch.conf on the guest mid term add service catalog support for service service discovery add support for additional disks a VM management API an event bus / hook to be able to react to events originating in firebuild long term enable rootfs build and run related operation split via remote build and run operators provide a remote registry type of system to host rootfs and kernel files externally add networking tools to create CNI bridge and overlay networks and expose VMs on outside of the host And probably many, many more as the time goes by. I’ll be writing more as firebuild develops.\nThanks for reading. Stay safe.\nThe source code is on GitHub ↩︎\nfirebuild prerequisites ↩︎\nCaveats when building from the URL ↩︎\nfirebuild-mmds GitHub repository ↩︎\n","description":"Manage firecracker root file systems and VMMs","tags":["firecracker","microvm","firebuild","docker"],"title":"Introducing firebuild","uri":"/posts/2021-03-23-introducing-firebuild/"},{"content":"This article describes the prerequisites to the Introducing firebuild.\ninstall Firecracker and Jailer on the host Firecracker works only on Linux. You can use this program to install and link the binaries on your system.\ninstall and configure golang 1.16+ The tc-redirect-tap CNI plugin (mentioned below) requires golang to build, as does firebuild. firebuild requires golang 1.16+ so install it:\n1 2 3 4 rm -rf /usr/local/go \u0026\u0026 tar -C /usr/local -xzf go1.16.2.linux-amd64.tar.gz mkdir -p $HOME/dev/golang/{bin,src} export PATH=$PATH:/usr/local/go/bin:$HOME/dev/golang/bin export GOPATH=$HOME/dev/golang The most recent version can be downloaded from golang website.\ninstall CNI plugins firebuild assumes CNI availability. Installing the plugins is very straightforward. Create /opt/cni/bin/ directory and download the plugins:\n1 2 3 mkdir -p /opt/cni/bin curl -O -L https://github.com/containernetworking/plugins/releases/download/v0.9.1/cni-plugins-linux-amd64-v0.9.1.tgz tar -C /opt/cni/bin -xzf cni-plugins-linux-amd64-v0.9.1.tgz Firecracker requires also the tc-redirect-tap plugin. Unfortunately, this one does not offer downloadable binaries and has to be built from sources.\n1 2 3 4 mkdir -p $GOPATH/src/github.com/awslabs/tc-redirect-tap cd $GOPATH/src/github.com/awslabs/tc-redirect-tap git clone https://github.com/awslabs/tc-redirect-tap.git . make install create CNI network configurations The article assumes two different CNI networks:\none for machine builds one for running machines CNI network config lists are stored in /etc/cni/conf.d. Create both like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 mkdir -p /etc/cni/conf.d cat \u003c\u003cEOF \u003e /etc/cni/conf.d/machines.conflist { \"name\": \"machines\", \"cniVersion\": \"0.4.0\", \"plugins\": [ { \"type\": \"bridge\", \"name\": \"machines-bridge\", \"bridge\": \"machines0\", \"isDefaultGateway\": true, \"ipMasq\": true, \"hairpinMode\": true, \"ipam\": { \"type\": \"host-local\", \"subnet\": \"192.168.127.0/24\", \"resolvConf\": \"/etc/resolv.conf\" } }, { \"type\": \"firewall\" }, { \"type\": \"tc-redirect-tap\" } ] } EOF cat \u003c\u003cEOF \u003e /etc/cni/conf.d/machine-builds.conflist { \"name\": \"machine-builds\", \"cniVersion\": \"0.4.0\", \"plugins\": [ { \"type\": \"bridge\", \"name\": \"machine-builds-bridge\", \"bridge\": \"builds0\", \"isDefaultGateway\": true, \"ipMasq\": true, \"hairpinMode\": true, \"ipam\": { \"type\": \"host-local\", \"subnet\": \"192.168.128.0/24\", \"resolvConf\": \"/etc/resolv.conf\" } }, { \"type\": \"firewall\" }, { \"type\": \"tc-redirect-tap\" } ] } EOF build and install firebuild from sources At this moment, there are no binaries for firebuild. It must be built from sources:\n1 2 3 4 mkdir -p $GOPATH/src/github.com/combust-labs/firebuild cd $GOPATH/src/github.com/combust-labs/firebuild git clone https://github.com/combust-labs/firebuild . go install make the $GOPATH system wide 1 echo \\$GOPATH=$GOPATH \u003e\u003e /etc/profile That’s it.\n","description":"Read this first before reading about firebuild","tags":["firebuild"],"title":"firebuild prerequisites","uri":"/posts/2021-03-22-firebuild-prerequisites/"},{"content":"Dockerfiles are awesome There is so much software out there packaged as Docker images. Operating systems, SQL and NoSQL databases, reverse proxies, compilers, everything. Safe to say, most of that software available as Docker containers is built from the common file format - the Dockerfile. Dockerfiles are awesome. They are recipes for getting a bit of software functional.\nhow have I been building Firecracker VMMs so far So far, all of my VMMs were built from Docker images using the following steps:\npull / build a Docker image start a container with an additional volume where the host directory is a mounted ext4 file system copy the operating system directories I needed to this other volume stop the container use the resulting ext4 file system as the root VMM volume Here’s an example.\nThere isn’t much wrong with this process and it does work surprisingly well for the majority of the containers out there. This is also how Weaveworks Ignite works, this is what the official Firecracker documentation suggests and what many write ups on Firecracker describe.\nunder the magnifier The above approach gets us the first 98% of the work done. It’s okay. But, there are certain important details missing.\nThe most obvious one: after the conversion by copying, we lose the ENTRYPOINT information. The Docker image provides us with two commands: ENTRYPOINT and CMD. Both instruct the container which program to run and what arguments to pass when the container starts. Without the ENTRYPOINT and optionally the CMD, the resulting VMM will start but it will not execute anything. We have to somehow modify the file system, post-copy, and add the command to start what we want to start.\nIn my previous write ups, I was adding a local service definition to the VMM during the copy stage. That is really cool but the problem with this approach is that virtually every image out there has its own dedicated configuration. Even if we could assume that 99% of all Docker images use the docker-entrypoint.sh as a conventional ENTRYPOINT, the CMD is going to differ.\nThen, there are additional parameters affecting the ENTRYPOINT. There is the WORKDIR and the USER command, there is the SHELL command, there are build arguments and environment variables.\nBy just copying the container file system, yes, we do get the final product. However, we are losing a lot of context and insight into what the result really is when it is to be started.\nFinally, plenty of containers come with additional labels and exposed ports information. All that information is lost if we are not correlating the file system copy against the original Dockerfile.\nFor sure, the manual build is doable but it won’t scale.\nanatomy of a Dockerfile Back to a Dockerfile. Let’s have a look at the official HashiCorp Consul Dockerfile as an example. Well, it ain’t 10 lines of code but it ain’t rocker science either. If we focus on the structure, it turns out to be fairly easy to understand:\nuse base Alpine 3.12 run some commands Linux commands … and, … that’s it, really, sprinkled with some environment variables and labels The contract is: given a clean installation of the Alpine Linux 3.12, after executing the RUN commands, one can execute the ENTRYPOINT and have Consul up and running.\nNot all Dockerfiles are that easy. There is a lot of software out there built with multi-stage builds. To keep it simple and easy to track, let’s look at Kafka Proxy Dockerfile. Or Minio for that matter.\nWe can find two FROM commands there. The first FROM defines the named stage, people often call it builder. Docker will basically build an image using every command until the next FROM and save it on disk.\nThe next stage, the one without as ..., let’s call it the main stage, is created again from the base operating system. In case of kafka Proxy, it’s Alpine 3.12. In case of Minio, it is Red Hat UBI 8. The main stage can copy the resources from the previous stages using the COPY --from=$stage-name command. When such command is processed, Docker will reach into the first image it built and copy the selected resources into the main stage image. Clever and very effective.\nThe builder stage is essentially a cache. In both cases, it is a golang program that is compiled only once and the main stage can be built quicker, assuming that the compiled output in the builder stage hasn’t changed.\nwe can build a VMM from a Dockerfile It’s possible to take a Dockerfile, parse it and apply all the relevant operations on a clean base operating system installation. The single stage build files are easy. Multi-stage builds can be a little more complex. Let’s consider what the process might look like.\nThere are two types of artifacts:\nnamed stages serve as resource cache the rootfs, the final build when all previous stages are built There can be only one unnamed build stage in a Dockerfile and it will always be built last.\nNamed stages:\ngiven a Dockerfile, parse it using the BuildKit dockerfile parser find explicit stages delimited with the respective FROM commands every build stage with FROM ... as ... can be built as a Docker image using the Moby client for such build, remove the as ... part from the FROM command and save using a random image name build named stages as Docker images, no need to have a container for each stage export the image to a tar file search the layers for all resources required by COPY --from commands; the layers are just tar files embedded in the main tar file extract matched resources to a temporary build directory remove temporary image Main build stage:\nrequires an existing implementation of the underlying OS, think: alpine:3.12 or alpine:3.13 this is the only part which has to be built by hand execute relevant RUN commands in order, pay attention to ARG and ENV commands such that the RUN commands are expanded correctly execute ADD / COPY commands, pay attention to the --from flag in both cases, keep track of WORKDIR and USER changes such that added / copied resources are placed under correct paths and commands are executed as correct users in correct locations why By building the rootfs in this way, it is possible to infer additional information which is otherwise lost when copying the file system from a running container. For example:\ncorrectly set up a local service to automatically start the application on VMM boot start the application user the uid / gid defined in the Dockerfile infer a shell from the SHELL command extract otherwise missing metadata hiding in LABEL and EXPOSE commands Sounds doable. Does it make sense? Good question. Is the Docker image the right medium to source the Firecracker VMM root file system from? It gets us the first 98% of the work done but the devil is in details. Dockerfile can get us all the way there.\n","description":"TL’DR: won’t scale","tags":["docker","firecracker","microvm"],"title":"Thoughts on creating VMMs from Docker images","uri":"/posts/2021-03-03-thoughts-on-creating-vmms-from-docker-images/"},{"content":"","description":"","tags":null,"title":"jailer","uri":"/tags/jailer/"},{"content":"A Firecracker release comes with two binaries - the firecracker and the jailer programs. The jailer brings even more isolation options to Firecracker by creating and securing a unique execution environment for each VMM.\nwhat can it do check the uniqueness and validity of the VMM id, maximum length of 64 characters, alphanumeric only assign NUMA node check the existence of the exec_file run the VMM as a specific user / group assign cgroups assign the VMM into a dedicated network namespace a VMM can be damonized what does it do This part comes from the jailer documentation1. When the jailer starts, it goes through the following process:\nall paths and the VMM id will validated all open file descriptors based on /proc/\u003cjailer-pid\u003e/fd except input, output and error will be closed the \u003cchroot_base\u003e/\u003cexec_file_name\u003e/\u003cid\u003e/root directory will be created - this is the chroot_dir exec_file_name is the last path component of exec_file (for example, that would be firecracker for /usr/bin/firecracker) if the path already exists, the jailer will fail to start the VMM because the assumption is that the VMM IDs are unique if exec_file is a link, jailer will readlink the value and use the name of the link source the exec_file will copied to \u003cchroot_base\u003e/\u003cexec_file_name\u003e/\u003cid\u003e/root/\u003cexec_file_name\u003e cgroups folder structure will be created; right now the jailer uses cgroup v1 On most systems, this is mounted by default in /sys/fs/cgroup (should be mounted by the user otherwise). The jailer will parse /proc/mounts to detect where each of the controllers required in --cgroup can be found (multiple controllers may share the same path). For each identified location (referred to as \u003ccgroup_base\u003e), the jailer creates the \u003ccgroup_base\u003e/\u003cexec_file_name\u003e/\u003cid\u003e subfolder, and writes the current pid to \u003ccgroup_base\u003e/\u003cexec_file_name\u003e/\u003cid\u003e/tasks. Also, the value passed for each \u003ccgroup_file\u003e is written to the file. If --node is used the corresponding values are written to the appropriate cpuset.mems and cpuset.cpus files.\nunshare() into a new mount namespace will be called, use pivot_root() to switch the old system root mount point with a new one base in chroot_dir, switch the current working directory to the new root, unmount the old root mount point, and call chroot into the current directory /dev/net/tun will be created inside of the jail using mknod /dev/kvm will be created inside of the jail using mknod the ownership of the chroot_dir, /dev/net/tun and /dev/kvm will be changed using chown based on the provided uid:gid if --netns \u003cnetns\u003e is present, attempt to join the specified network namespace if --daemonize is specified, call setsid() and redirect STDIN, STDOUT, and STDERR to /dev/null. privileges will be dropped by setting the provided uid:gid exec into \u003cexec_file_name\u003e --id=\u003cid\u003e --start-time-us=\u003copaque\u003e --start-time-cpu-us=\u003copaque\u003e and forward any extra arguments provided to the jailer after --, where: id: (string) - the id argument provided to jailer opaque: (number) time calculated by the jailer that it spent doing its work The jailer seems to be the proper way of running Firecracker VMMs. firectl, which I have discussed previously, has the jailer support. It was pretty easy to convert existing VMMs. There’s a couple of quirks to the firectl configuration, mostly - arguments must be explicitly assigned. The Golang SDK supports the defaults, like /srv/jailer for the chroot_base but firectl does not properly use them internally so just make sure you always pass them.\nhow to do it Here’s how I run my VMM via the jailer:\n1 2 3 4 5 6 7 8 9 10 11 sudo $GOPATH/bin/firectl \\ --jailer=/usr/bin/jailer \\ --exec-file=$(readlink /usr/bin/firecracker) \\ --id=alpine1 \\ --chroot-base-dir=/srv/jailer \\ --kernel=/firecracker/kernels/vmlinux-v5.8 \\ --root-drive=/firecracker/filesystems/alpine-base-root.ext4 \\ --cni-network=alpine \\ --veth-iface-name=alpine1 \\ --ncpus=1 \\ --memory=128 The above will start the Firecracker VMM via the /usr/bin/jailer binary.\nI use readlink because my /usr/bin/firecracker is a link to /usr/bin/firecracker-v0.22.4-x86_64. If I don’t use readlink, the jailer for whatever reason creates \u003cchroot_dir\u003e/firecracker but attempts to launch the VMM from \u003cchroot_dir\u003e/firecracker-v0.22.4-x86_64 directory. readlink avoids that problem in my setup.\nI have assigned a unique id to my VMM and explicitly passed the --chroot-base-dir. If I would not, this would have happened. The rest is the standard Firecracker firectl stuff discussed in the previous write ups.\nAll omitted arguments are set to their defaults so things like uid:gid and NUMA node will be all 0. Good for now.\nHere’s what the chroot_dir structure looks like for a VMM with only a root file system:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 $ sudo tree /srv/jailer/ /srv/jailer/ └── firecracker-v0.22.4-x86_64 └── alpine1 └── root ├── alpine-base-root.ext4 ├── dev │ ├── kvm │ └── net │ └── tun ├── firecracker-v0.22.4-x86_64 ├── run │ └── firecracker.socket └── vmlinux-v5.8 the root/alpine-base-root.ext4 is a link to the actual file system the root/vmlinux-v5.8 is the a link to the actual kernel chroot strategy The file system and the kernel linking is not done by the jailer. It’s the firectl doing it via the chroot strategy mechanism. The Golang SDK provides a default naive strategy,. It’s actually called like that, I’m not being cocky. The default strategy can be replace with a custom logic implementing the firecracker.HandlerAdapter interface.\nSo in AWS, one selects a base AMI and launches a VM from it. That creates a volume and subsequent VM starts use that volume. This could be a way forward to build something similar for Firecracker.\nclosing words I have subconsciously avoided touching the jailer before as I have seen it as a pretty complex feature. Considering what it gives, I must admit, it was very easy to get it in. I haven’t yet tried launching anything under a specific uid:gid but I do not expect any issues there.\nThe jailer documentation ↩︎\n","description":"Making the Firecracker VMMs even more secure","tags":["firecracker","jailer","microvm"],"title":"The jailer","uri":"/posts/2021-02-19-the-jailer/"},{"content":"Last night’s problem with the second VMM conflicting on the network layer with the first one was indeed the veth0 name hard coded in firectl. I’ve added the --veth-iface-name argument to firectl and I am now able to start multiple VMMs on a single bridge.\n1 2 3 4 5 6 7 8 9 sudo firectl \\ --firecracker-binary=/usr/bin/firecracker \\ --kernel=/firecracker/kernels/vmlinux-v5.8 \\ --root-drive=/firecracker/filesystems/alpine-base-root.ext4 \\ --cni-network=alpine \\ --socket-path=/tmp/alpine.sock \\ --ncpus=1 \\ --memory=128 \\ --veth-iface-name=vethalpine1 1 2 3 4 5 6 7 8 9 sudo firectl \\ --firecracker-binary=/usr/bin/firecracker \\ --kernel=/firecracker/kernels/vmlinux-v5.8 \\ --root-drive=/firecracker/filesystems/alpine-base-root.ext4-2 \\ --cni-network=alpine \\ --socket-path=/tmp/alpine2.sock \\ --ncpus=1 \\ --memory=128 \\ --veth-iface-name=vethalpine2 The bit to looks at is the way to handle multiple copies of the root file system.\nOne step closer to the ETCD cluster on Firecracker.\n","description":"Multiple VMMs on a single bridge are working","tags":["firecracker","microvm"],"title":"It’s all about the the Iface name","uri":"/posts/2021-02-18-its-all-about-the-iface-name/"},{"content":"Today I have looked at creating my own bridge networks for Firecracker VMMs. I already used CNI setups when evaluating the HashiCorp Nomad firecracker task driver1. Back then I incorrectly stated that Firecracker depends on certain CNI plugins. It doesn’t, it can take advantage of any CNI setup as long as the tc-redirect-tap is in the chained plugins.\nThe Nomad task driver had some issues, briefly:\nevery now and then, oddly, the task would never shut the VMM down and the only way to make the VMM gow down was to sudo kill nomad I tried updating the task driver to latest SDK version but I was not able to upgrade the Firecracker dependency past a specific commit, any version after that specific commit makes the VMM come up, the network setup to be there but the VMM is not reachable, really, really weird issue - reported it here So today, I took a different route.\nfirectl firectl2 is a command line utility for launching VMMs and a reference implementation of an application built on top of the Firecracker Golang SDK3. I have some exposure to the firectl from when I was trying to upgrade the Nomad task driver. The task driver uses parts of firectl code internally.\nOne thing missing from thefirectl is the option to define the CNI network name to use. Something similar to this snippet from the SDK readme:\n1 2 3 4 5 6 7 8 { NetworkInterfaces: []firecracker.NetworkInterface{{ CNIConfiguration: \u0026firecracker.CNIConfiguration{ NetworkName: \"fcnet\", IfName: \"veth0\", }, }} } The first thing to do was to add support for that. I’ve created a firectl fork and pushed my changes to GitHub, here4.\nMy changes:\na new argument to declare the --cni-network to use add the network interface based on the new argument added --netns argument required when using the CNI networks Having built my version of firectl, I’ve declared this CNI conflist (in /firecracker/cni/conf.d/alpine.conflist):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 { \"name\": \"alpine\", \"cniVersion\": \"0.4.0\", \"plugins\": [ { \"type\": \"bridge\", \"name\": \"alpine-bridge\", \"bridge\": \"alpinebridge0\", \"isDefaultGateway\": true, \"ipMasq\": true, \"hairpinMode\": true, \"ipam\": { \"type\": \"host-local\", \"subnet\": \"192.168.127.0/24\", \"resolvConf\": \"/etc/resolv.conf\" } }, { \"type\": \"firewall\" }, { \"type\": \"tc-redirect-tap\" } ] } and started the VMM via firectl, like this:\n1 2 3 4 5 6 7 8 sudo ./firectl \\ --firecracker-binary=/usr/bin/firecracker \\ --kernel=/firecracker/kernels/vmlinux-v5.8 \\ --root-drive=/firecracker/filesystems/alpine-base-root.ext4 \\ --cni-network=alpine \\ --socket-path=/tmp/alpine.sock \\ --ncpus=1 \\ --memory=128 This works line a charm. I can SSH to the VMM via the IP address which can be found with:\n1 cat /var/lib/cni/networks/alpine/last_reserved_ip.0 The VMM can reach the outside world, no degradation here.\nhost-local IPAM Reading more about the host-local IPAM, I figured that the issue with IP addresses changing on every start of the VMM is the normal behaviour. Basically, it stores the allocations on disk, under the /var/lib/cni/networks/\u003cnetwork-name\u003e directory. This is, I think, referred to as IP address leakage.\nOne way to take care of that, is to have a custom operator listening for when the VMM is stopped and removing that IP allocation manually. How, it’s undefined but it should be rather straightforward by using a combination of MMDS and some custom agent.\nThe bridge The other thing to note is, I’m no longer using the ptp plugin. I’m using the bridge instead and longer the docker0 one. This was my little attempt at launching two VMMs on the same bridge. Well, this didn’t work…\nFirecracker Golang SDK tries to remove the existing network configuration when launching another VMM with the same network name. Here’s the error I have seen:\n1 2 3 4 5 6 7 8 sudo ./firectl \\ --firecracker-binary=/usr/bin/firecracker \\ --kernel=/firecracker/kernels/vmlinux-v5.8 \\ --root-drive=/firecracker/filesystems/alpine-base-root.ext4-2 \\ --cni-network=alpine \\ --socket-path=/tmp/alpine2.sock \\ --ncpus=1 \\ --memory=128 WARN[0000] Failed handler \"fcinit.SetupNetwork\": failure when invoking CNI: failed to delete pre-existing CNI network {NetworkName:alpine NetworkConfig:\u003cnil\u003e IfName:veth0 VMIfName: Args:[] BinPath:[/opt/cni/bin] ConfDir:/etc/cni/conf.d CacheDir:/var/lib/cni/32171e6d-2b1b-4060-9555-e17314972ace containerID:32171e6d-2b1b-4060-9555-e17314972ace netNSPath:/var/run/netns Force:false}: failed to delete CNI network list \"alpine\": running [/sbin/iptables -t nat -D POSTROUTING -s 192.168.127.2 -j CNI-166dba6e0b91a8f3d41c9a89 -m comment --comment name: \"alpine\" id: \"32171e6d-2b1b-4060-9555-e17314972ace\" --wait]: exit status 2: iptables v1.6.1: Couldn't load target `CNI-166dba6e0b91a8f3d41c9a89':No such file or directory Try `iptables -h' or 'iptables --help' for more information. FATA[0000] Failed to start machine: failure when invoking CNI: failed to delete pre-existing CNI network {NetworkName:alpine NetworkConfig:\u003cnil\u003e IfName:veth0 VMIfName: Args:[] BinPath:[/opt/cni/bin] ConfDir:/etc/cni/conf.d CacheDir:/var/lib/cni/32171e6d-2b1b-4060-9555-e17314972ace containerID:32171e6d-2b1b-4060-9555-e17314972ace netNSPath:/var/run/netns Force:false}: failed to delete CNI network list \"alpine\": running [/sbin/iptables -t nat -D POSTROUTING -s 192.168.127.2 -j CNI-166dba6e0b91a8f3d41c9a89 -m comment --comment name: \"alpine\" id: \"32171e6d-2b1b-4060-9555-e17314972ace\" --wait]: exit status 2: iptables v1.6.1: Couldn't load target `CNI-166dba6e0b91a8f3d41c9a89':No such file or directory In the process, some of the underlying network configuration for the running VMM was wiped so my SSH connection was handing. Similar to what happens when one disconnects the network cable or disables WiFi.\nMaybe it’s related to the fact that my veth0 interface name is hard coded in my firectl. Something to look at.\nWhat would be nice to have is to decouple the network setup into two steps:\ncreate the bridge before launching VMMs, like what docker network create does setup the tap device at the VMM launch time Maybe the CNI implementation from Weaveworks Ignite5 can serve as an inspiration (thanks, Michał…)\nFood for thought.\nVault on Firecracker with CNI plugins and Nomad ↩︎\nfirectl GitHub repository ↩︎\nFirecracker Golang SDK GitHub repository ↩︎\nCNI network support for firectl ↩︎\nthe CNI implementation from Weaveworks Ignite ↩︎\n","description":"Because docker0 is not the right choice","tags":["firecracker","microvm"],"title":"Bridging the Firecracker network gap","uri":"/posts/2021-02-17-bridging-the-firecracker-network-gap/"},{"content":"Towards the end of the Firecracker VMM with additional disks article1 I concluded that I didn’t know how to live resize an attached drive. It turns out it is possible and it’s very easy to do using the Firecracker VMM API.\nTo launch the VMM with the API, I have to drop the --no-api argument (obviously) and use --api-sock with the path to the socket file. In a production system, I’d use a directory other than /tmp.\n1 2 3 sudo firecracker \\ --config-file /firecracker/configs/alpine-config.json \\ --api-sock /tmp/alpine-base.sock The VMM API Firecracker server exposes a Swagger documented API on a unix socket available under the --api-sock path. There is an instance of the API per the socket file. In essence - one API for every VMM instance. A VMM will not start if the socket file exists.\nBecause firecracker command is executed with elevated privileges and the socket file is owned by the elevated user, the curl command has to be executed with elevated privileges.\nFirecracker server API consumes and returns JSON. At the time of writing, the Swagger file can be looked at on GitHub2. Some introductory examples are available here3.\nThe API offers two classes of operations: pre-boot and post-boot. The pre-boot ones are interesting. They are clearly documented as such. Quick glance shows that the boot device, network interfaces, drives and the balloon device can be called at the pre-boot stage. A VMM can also be launched from a snapshot.\nThere clearly exists the potential for an EBS, snapshots or ENI AWS-like management plane.\nLive resize the drive Back to the topic. So, right now, there is no API call to list the drives attached to the VMM. Neither the / nor the /machine-config returns that info.\nOne has to either have access to the VMM config file or could use the MMDS (Microvm Metadata Service) to store that info on boot. I’ll have a look at the MMDS at some other time.\nYes, the drive. I do know the drive ID I was trying to previously resize. It’s the vol2 stored at /firecracker/filesystems/alpine-vol2.ext4.\nThe API endpoint I’m investigating is the /drives/{drive_id}. The POST operation is the pre-boot one and it does complain when executing it on a running instance.\nHTTP/1.1 400 Server: Firecracker API Connection: keep-alive Content-Type: application/json Content-Length: 88 {\"fault_message\":\"The requested operation is not supported after starting the microVM.\"} Just PATCH it? The PATCH allows me to tell the VMM that something about the underlying volume has changed. First, a glance at fdisk -l output of the running VMM:\n1 172:~$ sudo fdisk -l Disk /dev/vda: 500 MiB, 524288000 bytes, 1024000 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk /dev/vdb: 500 MiB, 524288000 bytes, 1024000 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes The device I’m interested in is the /dev/vdb. Currently 500MiB.\nSo is live resize as simple as…, surely not…, is it … ?\n1 dd if=/dev/zero bs=1M count=50 \u003e\u003e /firecracker/filesystems/alpine-vol2.ext4 50+0 records in 50+0 records out 52428800 bytes (52 MB, 50 MiB) copied, 0.0442937 s, 1.2 GB/s 1 2 3 4 5 6 7 8 sudo curl \\ --unix-socket /tmp/alpine-base.sock -i \\ -X PATCH \"http://localhost/drives/vol2\" \\ -H \"Accept: application/json\" \\ -d \"{ \\\"drive_id\\\": \\\"vol2\\\", \\\"path_on_host\\\": \\\"/firecracker/filesystems/alpine-vol2.ext4\\\" }\" HTTP/1.1 204 Server: Firecracker API Connection: keep-alive Firecracker logs the following:\n[ 148.225812] virtio_blk virtio1: [vdb] new size: 1126400 512-byte logical blocks (577 MB/550 MiB) [ 148.227542] vdb: detected capacity change from 524288000 to 576716800 A quick look at fdisk -l again:\n1 172:~$ sudo fdisk -l /dev/vdb Disk /dev/vdb: 550 MiB, 576716800 bytes, 1126400 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Ah, yes! I could now use resize2fs to extends the existing ext4 partition to fit the new size. Awesome!\nOh, and I can now shut the VMM down without SSH:\n1 2 3 4 5 6 7 sudo curl --unix-socket /tmp/alpine-base.sock -i \\ -X PUT \"http://localhost/actions\" \\ -H \"Accept: application/json\" \\ -H \"Content-Type: application/json\" \\ -d \"{ \\\"action_type\\\": \\\"SendCtrlAltDel\\\" }\" HTTP/1.1 204 Server: Firecracker API Connection: keep-alive 1 sudo rm /tmp/alpine-base.sock Firecracker VMM with additional disks ↩︎\nFirecracker server Swagger file at the time of writing the article, version v0.22.4 ↩︎\nFirecracker API usage examples ↩︎\n","description":"It’s possible to live resize Firecracker VMM drive with the API","tags":["firecracker","microvm"],"title":"Live resize Firecracker VMM drive","uri":"/posts/2021-02-16-live-resize-firecracker-vmm-drive/"},{"content":"Before looking at the networking options, I have looked at adding extra drives to my Firecracker VMMs. Storing data on the root file system will not scale well long term. Additional disks will be a good solution to persist application specific data across reboots and upgrades.\nCreate the disk on the host First, create an additional file system on the host:\n1 2 dd if=/dev/zero of=\"/firecracker/filesystems/alpine-vol2.ext4\" bs=1M count=500 mkfs.ext4 \"/firecracker/filesystems/alpine-vol2.ext4\" Reconfigure the VMM Change the VMM drives configuration:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 \"drives\": [ { \"drive_id\": \"rootfs\", \"path_on_host\": \"/firecracker/filesystems/alpine-base-root.ext4\", \"is_root_device\": true, \"is_read_only\": false }, { \"drive_id\": \"vol2\", \"path_on_host\": \"/firecracker/filesystems/alpine-vol2.ext4\", \"is_root_device\": false, \"is_read_only\": false } ], and relaunch the VMM.\nVerify the configuration In another terminal:\n1 172:~$ sudo fdisk -l Disk /dev/vda: 500 MiB, 524288000 bytes, 1024000 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk /dev/vdb: 500 MiB, 524288000 bytes, 1024000 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes I can mount it:\n1 2 172:/home/alpine# mkdir /mnt 172:/home/alpine# mount /dev/vdb /mnt Firecracker shows me:\n[ 1800.756913] EXT4-fs (vdb): mounted filesystem with ordered data mode. Opts: (null) [ 1800.758116] ext4 filesystem being mounted at /mnt supports timestamps until 2038 (0x7fffffff) I can also write to it:\n1 172:/home/alpine# ls -la /mnt/ total 14 drwxr-xr-x 3 root root 1024 Feb 14 15:07 . drwxr-xr-x 16 root root 1024 Feb 14 15:49 .. drwx------ 2 root root 12288 Feb 14 15:07 lost+found 1 2 172:/home/alpine# echo 1 \u003e /mnt/test 172:/home/alpine# ls -la /mnt/ total 15 drwxr-xr-x 3 root root 1024 Feb 14 15:50 . drwxr-xr-x 16 root root 1024 Feb 14 15:49 .. drwx------ 2 root root 12288 Feb 14 15:07 lost+found -rw-r--r-- 1 root root 2 Feb 14 15:50 test Live resize does not work Not sure at this stage if this is because of how the system is built or if it’s a limitation of the VMM but I am not able to live resize the disk. Executing on the host:\n1 dd if=/dev/zero bs=1M count=100 \u003e\u003e /firecracker/filesystems/alpine-vol2.ext4 100+0 records in 100+0 records out 104857600 bytes (105 MB, 100 MiB) copied, 0.0798364 s, 1.3 GB/s and in the VMM:\n1 2 172:/home/alpine# sudo blockdev --flushbufs /dev/vdb 172:/home/alpine# sudo blockdev --rereadpt /dev/vdb does not reflect the change. The change only becomes visible after VMM restart.\n","description":"Adding more disks to the Firecracker VMM","tags":["firecracker","microvm"],"title":"Firecracker VMM with additional disks","uri":"/posts/2021-02-14-firecracker-vmm-with-additional-disks/"},{"content":"","description":"","tags":null,"title":"alpine","uri":"/tags/alpine/"},{"content":"The quest to launch an ETCD cluster on Firecracker starts here.\nIn this post, I’m describing how I’ve built my initial Alpine 3.13 VMM with OpenSSH and a dedicated sudoer user. In AWS, when one launches a Ubuntu instance, one can access it via ssh ubuntu@\u003caddress\u003e, a CentOS VM is ssh centos@\u003caddress\u003e. At the end of this write up, I’ll have ssh alpine@\u003caddress\u003e. This VMM will have access to the outside world so I can install additional software and even ping the BBC! For the networking, I’ll use the Docker docker0 bridge; inspired again by Julia Evans, the Day 41: Trying to understand what a bridge is1 was very helpful. I will look at my own networking setup in future write ups.\nThe result is a refinement of the process from my previous Firecracker articles.\nDockerfile The root file system is built from an Alpine 3.13 Docker image.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 FROM alpine:3.13 RUN apk update \\ \u0026\u0026 apk add openrc openssh sudo util-linux \\ \u0026\u0026 ssh-keygen -A \\ \u0026\u0026 mkdir -p /home/alpine/.ssh \\ \u0026\u0026 addgroup -S alpine \u0026\u0026 adduser -S alpine -G alpine -h /home/alpine -s /bin/sh \\ \u0026\u0026 echo \"alpine:$(cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 32 | head -n1)\" | chpasswd \\ \u0026\u0026 echo '%alpine ALL=(ALL) NOPASSWD: ALL' \u003e /etc/sudoers.d/alpine \\ \u0026\u0026 ln -s agetty /etc/init.d/agetty.ttyS0 \\ \u0026\u0026 echo ttyS0 \u003e /etc/securetty \\ \u0026\u0026 rc-update add agetty.ttyS0 default \\ \u0026\u0026 rc-update add devfs boot \\ \u0026\u0026 rc-update add procfs boot \\ \u0026\u0026 rc-update add sysfs boot \\ \u0026\u0026 rc-update add local default COPY ./key.pub /home/alpine/.ssh/authorized_keys RUN chown -R alpine:alpine /home/alpine \\ \u0026\u0026 chmod 0740 /home/alpine \\ \u0026\u0026 chmod 0700 /home/alpine/.ssh \\ \u0026\u0026 chmod 0400 /home/alpine/.ssh/authorized_keys \\ \u0026\u0026 mkdir -p /run/openrc \\ \u0026\u0026 touch /run/openrc/softlevel \\ \u0026\u0026 rc-update add sshd Plenty but rather straightforward, let’s break it down:\nupdate the source packages and install required packages: openrc because an init system in required openssh and other other packages so there is a minimalistic system that can be accessed and used after launch 1 2 apk update \\ \u0026\u0026 apk add openrc openssh sudo util-linux \\ generate host keys: 1 \u0026\u0026 ssh-keygen -A \\ create the home directory structure for the alpine user: 1 \u0026\u0026 mkdir -p /home/alpine/.ssh \\ create the alpine group and the user, assign home directory, init shell and a random password; without the password the user account stays disabled and it’s not possible to SSH as that user: 1 2 \u0026\u0026 addgroup -S alpine \u0026\u0026 adduser -S alpine -G alpine -h /home/alpine -s /bin/sh \\ \u0026\u0026 echo \"alpine:$(cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 32 | head -n1)\" | chpasswd \\ make the user a password-less sudoer: 1 \u0026\u0026 echo '%alpine ALL=(ALL) NOPASSWD: ALL' \u003e /etc/sudoers.d/alpine \\ mount special file systems on boot and enable local services: 1 2 3 4 5 6 7 \u0026\u0026 ln -s agetty /etc/init.d/agetty.ttyS0 \\ \u0026\u0026 echo ttyS0 \u003e /etc/securetty \\ \u0026\u0026 rc-update add agetty.ttyS0 default \\ \u0026\u0026 rc-update add devfs boot \\ \u0026\u0026 rc-update add procfs boot \\ \u0026\u0026 rc-update add sysfs boot \\ \u0026\u0026 rc-update add local default copy the generated public key to authorized keys, there’s a single key so add directly to authorized_keys: 1 COPY ./key.pub /home/alpine/.ssh/authorized_keys finally, apply settings required to access the system via SSH: OpenSSH is picky about home and .ssh directory permissions so I make sure these are correct: 0740 for home, 0700 for $HOME/.ssh and 0400 for the keys file enable OpenSSH and make sure it starts when the system starts 1 2 3 4 5 6 7 RUN chown -R alpine:alpine /home/alpine \\ \u0026\u0026 chmod 0740 /home/alpine \\ \u0026\u0026 chmod 0700 /home/alpine/.ssh \\ \u0026\u0026 chmod 0400 /home/alpine/.ssh/authorized_keys \\ \u0026\u0026 mkdir -p /run/openrc \\ \u0026\u0026 touch /run/openrc/softlevel \\ \u0026\u0026 rc-update add sshd I have a /firecracker directory structure which I described in Taking Firecracker for a spin2. The Dockerfile is saved in /firecracker/docker/alpine-3.13/Dockerfile.\nFile system Now I put together the program to start the container and extract the file system. The program is saved as /firecracker/docker/create-alpine-3.13.sh and goes like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 #!/bin/bash set -eu build_dir=\"/tmp/alpine-build\" dockerfile=\"/firecracker/docker/alpine-3.13/Dockerfile\" filesystem_target=\"/firecracker/filesystems/alpine-base-root.ext4\" key_file=\"alpine\" image_tag=\"local/alpine-base:latest\" pre_build_dir=$(pwd) echo \"Generating a keypair...\" set +e ssh-keygen -t rsa -b 4096 -C \"alpine@firecracker\" -f \"${HOME}/.ssh/${key_file}\" set -e First, I’m setting up the build context and generating a key pair. ssh-keygen is smart to check if the key pair already exists and answering no will prevent it from overwriting on every run.\nIn the Dockerfile, I was using a build local key.pub for the image (step 8). Here’s how I make sure it exists:\n1 2 3 4 5 echo \"Creating build directory...\" mkdir -p \"${build_dir}\" \u0026\u0026 cd \"${build_dir}\" echo \"Copying public key to the build directory...\" cp \"${HOME}/.ssh/${key_file}.pub\" \"${build_dir}/key.pub\" Next, bring the Dockerfile to the build directory and build the Docker image. Tag the image with a known name. If the docker build fails, the program will report that fact and exit.\n1 2 3 4 5 6 7 8 9 10 11 echo \"Building Docker image...\" cp \"${dockerfile}\" \"${build_dir}/Dockerfile\" docker build -t \"${image_tag}\" . retVal=$? cd \"${pre_build_dir}\" rm -r \"${build_dir}\" if [ $retVal -ne 0 ]; then echo \" ==\u003e build failed with status $?\" exit $retVal fi The next step is to prepare the root file system:\n1 2 3 4 5 6 echo \"Creating file system...\" mkdir -p \"${build_dir}/fsmnt\" dd if=/dev/zero of=\"${build_dir}/rootfs.ext4\" bs=1M count=500 mkfs.ext4 \"${build_dir}/rootfs.ext4\" echo \"Mounting file system...\" sudo mount \"${build_dir}/rootfs.ext4\" \"${build_dir}/fsmnt\" and start the container:\n1 2 echo \"Starting container from new image ${image_tag}...\" CONTAINER_ID=$(docker run --rm -v ${build_dir}/fsmnt:/export-rootfs -td ${image_tag} /bin/sh) followed by copying everything out of the container to the file system file. I do it the same way as with the Vault VMM root file system in my previous articles.\nI could combine the first two commands together but I decided to keep them separate to distinguish what belongs to the file system and what’s mine, in this case that’s the /home directory alone:\n1 2 3 4 echo \"Copying Docker file system...\" docker exec ${CONTAINER_ID} /bin/sh -c 'for d in home; do tar c \"/$d\" | tar x -C /export-rootfs; done; exit 0' docker exec ${CONTAINER_ID} /bin/sh -c 'for d in bin dev etc lib root sbin usr; do tar c \"/$d\" | tar x -C /export-rootfs; done; exit 0' docker exec ${CONTAINER_ID} /bin/sh -c 'for dir in proc run sys var; do mkdir /export-rootfs/${dir}; done; exit 0' When everything is copied, unmount the file system, stop the container and clean up:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 echo \"Unmounting file system...\" sudo umount \"${build_dir}/fsmnt\" echo \"Removing docker container...\" docker stop $CONTAINER_ID echo \"Moving file system...\" mv \"${build_dir}/rootfs.ext4\" \"${filesystem_target}\" echo \"Cleaning up build directory...\" rm -r \"${build_dir}\" echo \"Removing Docker image...\" docker rmi ${image_tag} echo \" \\\\o/ File system written to ${filesystem_target}.\" To run it simply execute /firecracker/docker/create-alpine-3.13.sh. On my machine, assuming that I already have alpine:3.13 Docker image, the process takes about 20 seconds.\nNetworking The resulting VMM would be useless without access to the outside world. My previous write ups didn’t discuss any of that, none of those VMMs were able reach the internet.\nHere, I’m using the method from Julia’s article - use the docker0 bridge. This is really straightforward. I have the following /firecracker/docker/tap-alpine-3.13.sh program:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 #!/bin/bash set -eu sudo apt-get install bridge-utils -y # create and configure a tap device # to launch firecracker VMM on the docker0 bridge TAP_DEV=alpine-test CONTAINER_IP=172.17.0.42 GATEWAY_IP=172.17.0.1 DOCKER_MASK_LONG=255.255.255.0 sudo ip tuntap add dev \"$TAP_DEV\" mode tap sudo brctl addif docker0 $TAP_DEV sudo ip link set dev \"$TAP_DEV\" up # as Julia Evans, I also need to figure out the meaning of this: sudo sysctl -w net.ipv4.conf.${TAP_DEV}.proxy_arp=1 \u003e /dev/null sudo sysctl -w net.ipv6.conf.${TAP_DEV}.disable_ipv6=1 \u003e /dev/null The gateway IP and mask come from the docker0 bridge:\n1 2 3 4 5 $ ip addr show docker0 6: docker0: \u003cNO-CARRIER,BROADCAST,MULTICAST,UP\u003e mtu 1500 qdisc noqueue state DOWN group default link/ether 02:42:3c:de:fe:d5 brd ff:ff:ff:ff:ff:ff inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 valid_lft forever preferred_lft forever The IP address of the VMM is an arbitrary selection.\nRun it with /firecracker/docker/tap-alpine-3.13.sh, the outcome will be similar to:\n1 2 3 $ ip link show alpine-test 13: alpine-test: \u003cNO-CARRIER,BROADCAST,MULTICAST,UP\u003e mtu 1500 qdisc fq_codel master docker0 state DOWN mode DEFAULT group default qlen 1000 link/ether b6:53:d1:78:ee:2d brd ff:ff:ff:ff:ff:ff Time to configure the VMM.\nVMM configuration file A couple of things to take a note of:\nip=172.17.0.42::172.17.0.1:255.255.255.0::eth0:off is of the format ip=${VMM_IP}::${GATEWAY_IP}:{DOCKER_MASK_LONG}::${VMM_INTERFACE_ID}:off network-interfaces[0].host_dev_name matches the value of $TAP_DEV 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 cat \u003c\u003cEOF \u003e /firecracker/configs/alpine-config.json { \"boot-source\": { \"kernel_image_path\": \"/firecracker/kernels/vmlinux-v5.8\", \"boot_args\": \"ro console=ttyS0 noapic reboot=k panic=1 pci=off nomodules random.trust_cpu=on ip=172.17.0.42::172.17.0.1:255.255.255.0::eth0:off\" }, \"drives\": [ { \"drive_id\": \"rootfs\", \"path_on_host\": \"/firecracker/filesystems/alpine-base-root.ext4\", \"is_root_device\": true, \"is_read_only\": false } ], \"network-interfaces\": [ { \"iface_id\": \"eth0\", \"guest_mac\": \"02:FC:00:00:00:05\", \"host_dev_name\": \"alpine-test\" } ], \"machine-config\": { \"vcpu_count\": 1, \"mem_size_mib\": 128, \"ht_enabled\": false } } EOF Run the VMM To start the VMM, simply execute:\n1 sudo firecracker --no-api --config-file /firecracker/configs/alpine-config.json About two seconds later:\n* Mounting misc binary format filesystem ... [ ok ] * Mounting /sys ... [ ok ] * Mounting security filesystem ... [ ok ] * Mounting debug filesystem ... [ ok ] * Mounting SELinux filesystem ... [ ok ] * Mounting persistent storage (pstore) filesystem ... [ ok ] * Starting local ... [ ok ] Welcome to Alpine Linux 3.13 Kernel 5.8.0 on an x86_64 (ttyS0) 172 login: SSH into the VMM In another terminal:\n1 ssh -i ~/.ssh/alpine alpine@172.17.0.42 The authenticity of host '172.17.0.42 (172.17.0.42)' can't be established. ECDSA key fingerprint is SHA256:gYxEJdQIXM3242/yV/RV9qVQBaGSdLoUtpFSmBKEyHE. Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added '172.17.0.42' (ECDSA) to the list of known hosts. Enter passphrase for key '/home/radek/.ssh/alpine': Welcome to Alpine! The Alpine Wiki contains a large amount of how-to guides and general information about administrating Alpine systems. See \u003chttp://wiki.alpinelinux.org/\u003e. You can setup the system with the command: setup-alpine You may change this message by editing /etc/motd. 1 2 172:~$ sudo sh 172:/home/alpine# apk update fetch https://dl-cdn.alpinelinux.org/alpine/v3.13/main/x86_64/APKINDEX.tar.gz fetch https://dl-cdn.alpinelinux.org/alpine/v3.13/community/x86_64/APKINDEX.tar.gz v3.13.1-115-gf65775dfbc [https://dl-cdn.alpinelinux.org/alpine/v3.13/main] v3.13.1-117-g6a5e33f63c [https://dl-cdn.alpinelinux.org/alpine/v3.13/community] OK: 13880 distinct packages available 1 172:/home/alpine# ping 1.1.1.1 PING 1.1.1.1 (1.1.1.1): 56 data bytes 64 bytes from 1.1.1.1: seq=0 ttl=58 time=18.995 ms 64 bytes from 1.1.1.1: seq=1 ttl=58 time=15.660 ms 64 bytes from 1.1.1.1: seq=2 ttl=58 time=16.246 ms 64 bytes from 1.1.1.1: seq=3 ttl=58 time=17.889 ms ^C --- 1.1.1.1 ping statistics --- 4 packets transmitted, 4 packets received, 0% packet loss round-trip min/avg/max = 15.660/17.197/18.995 ms 1 172:/home/alpine# ip link 1: lo: \u003cLOOPBACK,UP,LOWER_UP\u003e mtu 65536 qdisc noqueue state UNKNOWN qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 2: eth0: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 02:fc:00:00:00:05 brd ff:ff:ff:ff:ff:ff 1 2 3 4 5 6 7 8 9 10 11 12 13 172:/home/alpine# ip addr 1: lo: \u003cLOOPBACK,UP,LOWER_UP\u003e mtu 65536 qdisc noqueue state UNKNOWN qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: eth0: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 02:fc:00:00:00:05 brd ff:ff:ff:ff:ff:ff inet 172.17.0.42/24 brd 172.17.0.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::fc:ff:fe00:5/64 scope link valid_lft forever preferred_lft forever Nice. Everything is working as expected. The UX is not fully complete, to ping stuff I do have to sudo. Whatever, if I can ping the BBC, I’m good:\n1 172:/home/alpine# ping bbc.co.uk PING bbc.co.uk (151.101.64.81): 56 data bytes 64 bytes from 151.101.64.81: seq=0 ttl=58 time=23.371 ms 64 bytes from 151.101.64.81: seq=1 ttl=58 time=20.238 ms 64 bytes from 151.101.64.81: seq=2 ttl=58 time=24.788 ms 64 bytes from 151.101.64.81: seq=3 ttl=58 time=24.047 ms ^C --- bbc.co.uk ping statistics --- 4 packets transmitted, 4 packets received, 0% packet loss round-trip min/avg/max = 20.238/23.111/24.788 ms Next steps Next time I am going to look at setting up the network with IPAM so the IP addresses are assigned from a given range.\nThat’s it for today:\n1 172:/home/alpine# reboot 172:/home/alpine# Connection to 172.17.0.42 closed by remote host. Connection to 172.17.0.42 closed. Day 41: Trying to understand what a bridge is ↩︎\nTaking Firecracker for a spin ↩︎\n","description":"One step back, two steps forward; create a base image to investigate Firecracker further","tags":["firecracker","microvm","alpine","docker"],"title":"Launching Alpine Linux on Firecracker like a boss","uri":"/posts/2021-02-13-launching-alpine-linux-on-firecracker-like-a-boss/"},{"content":"So I’ve been on the fence with the vendor directory.\nOn one hand, it’s great to have the modules in the project because it speeds up the build and serves as a safe storage.\nOn the other hand, it does increase the churn in the repository and creates a lot of duplication on disk because many projects often contain the same dependencies.\nSince I do like holding on to my dependencies and go mod works great for me, I’ve decided to try out The Athens. I’s awesome.\nWhat is it It is a self-hosted golang module mirror service. Unlike the public proxy.golang.org, it can cache private modules so no more GOPRIVATE, yay!\nAthens offers a selection of storage back ends: local disk, S3, Minio, GCS and the like. One instance can be pointed at another with upstream services. The documentation is very approachable.\nHow I run it KISS, Docker service backed with the disk type storage. Basically like this:\n1 2 3 4 5 6 7 mkdir -p /var/lib/gomod docker run --rm \\ -p 15001:3000 \\ -v /var/lib/gomod:/gomod \\ -e ATHENS_STORAGE_TYPE=disk \\ -e ATHENS_DISK_STORAGE_ROOT=/gomod \\ -d gomods/athens:latest I’ve exported the GOPROXY=http://localhost:15001 in my Bash profile so tools use it automatically. I have a dedicated git repository where I sync my local modules to a couple of time a day.\nLong term, maybe something like the S3 storage would be a better option but I’m not sure how expensive it can get.\n","description":"Historical place with great weather and fantastic cuisine, no wonder…","tags":["golang"],"title":"My golang modules live in Athens","uri":"/posts/2021-02-10-my-golang-modules-live-in-athens/"},{"content":"","description":"","tags":null,"title":"cni","uri":"/tags/cni/"},{"content":"","description":"","tags":null,"title":"vault","uri":"/tags/vault/"},{"content":"It’s good to know how to set up Firecracker VM by hand but that’s definitely suboptimal long term. So today I am looking at setting up Firecracker with CNI plugins. Firecracker needs four CNI plugins to operate: ptp, firewall, host-local and tc-redirect-tap. First three come from the CNI plugins1 repository, the last one comes from AWS Labs tc-redirect-tap2 repository.\nGolang CNI plugins and tc-redirect-tap require golang to build. I’m using 1.15.8.\nCNI plugins 1 2 3 4 mkdir ~/cni-plugins cd ~/cni-plugins git clone https://github.com/containernetworking/plugins.git . ./build_linux.sh After about 30 seconds, the are under the bin/ directory:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 $ tree bin bin ├── bandwidth ├── bridge ├── dhcp ├── firewall ├── flannel ├── host-device ├── host-local ├── ipvlan ├── loopback ├── macvlan ├── portmap ├── ptp ├── sbr ├── static ├── tuning ├── vlan └── vrf tc-redirect-tap 1 2 3 4 mkdir ~/tc-redirect-tap cd ~/tc-redirect-tap git clone https://github.com/awslabs/tc-redirect-tap.git . make all The binary can be found in the root of the sources directory.\nInstalling the plugins CNI plugins are sought from the /opt/cni/bin directory. Some tools allow overriding that path but there is no consistency so the default directory is the safest choice. However, to keep everything tidy, I will place by plugins in the /firecracker/cni/bin directory, per the structure from Taking Firecracker for a spin3:\n1 2 3 mkdir -p /firecracker/cni/bin cp ~/cni-plugins/bin/* /firecracker/cni/bin/ cp ~/tc-redirect-tap/tc-redirect-tap /firecracker/cni/bin/tc-redirect-tap then link them to where they are expected to be:\n1 2 sudo mkdir -p /opt/cni sudo ln -sfn /firecracker/cni/bin /opt/cni/bin Prepare Nomad and task driver I’ll use HashiCorp Nomad with the firecracker-task.driver. First, get Nomad:\n1 2 3 4 5 cd /tmp wget https://releases.hashicorp.com/nomad/1.0.3/nomad_1.0.3_linux_amd64.zip unzip nomad_1.0.3_linux_amd64.zip sudo mv nomad /usr/bin/nomad rm nomad_1.0.3_linux_amd64.zip Second, get the task driver sources and build them:\n1 2 3 4 mkdir ~/firecracker-task-driver cd ~/firecracker-task-driver git clone https://github.com/cneira/firecracker-task-driver.git . go build -mod=mod -o ./firecracker-task-driver ./main.go Default Nomad plugins directory is /opt/nomad/plugins:\n1 2 sudo mkdir -p /opt/nomad/plugins sudo mv firecracker-task-driver /opt/nomad/plugins/firecracker-task-driver Create the network definition These are to be placed under /etc/cni/conf.d. Again, to keep it tidy and in one place:\n1 2 3 mkdir /firecracker/cni/conf.d sudo mkdir -p /etc/cni sudo ln -sfn /firecracker/cni/conf.d /etc/cni/conf.d Create the network definition file:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 cat \u003c\u003cEOF \u003e /firecracker/cni/conf.d/vault.conflist { \"name\": \"vault\", \"cniVersion\": \"0.4.0\", \"plugins\": [ { \"type\": \"ptp\", \"ipMasq\": true, \"ipam\": { \"type\": \"host-local\", \"subnet\": \"192.168.127.0/24\", \"resolvConf\": \"/etc/resolv.conf\" } }, { \"type\": \"firewall\" }, { \"type\": \"tc-redirect-tap\" } ] } EOF Start Nomad in dev mode Create the Nomad configuration directory:\n1 sudo mkdir /etc/nomad We have to create this minimalistic server configuration to tell Nomad where our plugins are (plugins directory under data_dir):\n1 2 3 4 cat \u003c\u003cEOF | sudo tee -a /etc/nomad/server.conf data_dir = \"/opt/nomad\" bind_addr = \"0.0.0.0\" # the default EOF And we can start Nomad development agent:\n1 sudo nomad agent -dev -config=/etc/nomad/server.conf Create the Nomad task Create the directory for Nomad jobs:\n1 sudo mkdir -p /etc/nomad/jobs And write the job definition. The process of getting the kernel and the root image is described in the previous post3.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 cat \u003c\u003cEOF | sudo tee -a /etc/nomad/jobs/vault.nomad job \"vault-with-cni\" { datacenters = [\"dc1\"] type = \"service\" group \"vault-test\" { restart { attempts = 0 mode = \"fail\" } task \"vault1\" { driver = \"firecracker-task-driver\" config { BootDisk = \"/firecracker/filesystems/vault-root.ext4\" Firecracker = \"/usr/bin/firecracker\" KernelImage = \"/firecracker/kernels/vmlinux-v5.8\" Mem = 128 Network = \"vault\" Vcpus = 1 } } } } EOF Test the job configuration:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 $ sudo nomad job plan /etc/nomad/jobs/vault.nomad + Job: \"vault-with-cni\" + Task Group: \"vault-test\" (1 create) + Task: \"vault1\" (forces create) Scheduler dry-run: - All tasks successfully allocated. Job Modify Index: 0 To submit the job with version verification run: nomad job run -check-index 0 /etc/nomad/jobs/vault.nomad When running the job with the check-index flag, the job will only be run if the job modify index given matches the server-side version. If the index has changed, another user has modified the job and the plan's results are potentially invalid. Okay, looks good, let’s run it:\n1 2 3 4 5 6 7 8 $ sudo nomad job run /etc/nomad/jobs/vault.nomad ==\u003e Monitoring evaluation \"2e42b090\" Evaluation triggered by job \"vault-with-cni\" ==\u003e Monitoring evaluation \"2e42b090\" Evaluation within deployment: \"d12624cb\" Allocation \"a57d68ec\" created: node \"10f89343\", group \"vault-test\" Evaluation status changed: \"pending\" -\u003e \"complete\" ==\u003e Evaluation \"2e42b090\" finished with status \"complete\" Awesome, what does the status say?\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 $ sudo nomad status vault ID = vault-with-cni Name = vault-with-cni Submit Date = 2021-02-07T13:49:20Z Type = service Priority = 50 Datacenters = dc1 Namespace = default Status = running Periodic = false Parameterized = false Summary Task Group Queued Starting Running Failed Complete Lost vault-test 0 0 1 0 0 0 Latest Deployment ID = d12624cb Status = running Description = Deployment is running Deployed Task Group Desired Placed Healthy Unhealthy Progress Deadline vault-test 1 1 0 0 2021-02-07T13:59:20Z Allocations ID Node ID Task Group Version Desired Status Created Modified a57d68ec 10f89343 vault-test 0 run running 7s ago 6s ago Sweet. Let’s have a look at the veth device:\n1 2 3 $ ip -c link show type veth 7: veth200fa5e4@if3: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc noqueue state UP mode DEFAULT group default link/ether 42:ee:02:f4:98:3a brd ff:ff:ff:ff:ff:ff link-netnsid 0 Can we talk to it? 1 curl http://192.168.127.2:8200/sys/health 1 {\"errors\":[]} Yep, it running!\nCaveats Stopping the job does not remove the veth interface so a manual cleanup of the unused interfaces is needed. Subsequent runs give the task the next IP address. If 192.168.127.2 does not work for you, try .1 .3, .4 and so on… Something to look into in detail a little bit later. CNI plugins GitHub repository ↩︎\nAWS Labs tcp-redirect-tap GitHub repository ↩︎\nTaking Firecracker for a spin ↩︎ ↩︎\n","description":"Setting up Vault on Firecracker with CNI network on HashiCorp Nomad","tags":["firecracker","microvm","cni","nomad","vault"],"title":"Vault on Firecracker with CNI plugins and Nomad","uri":"/posts/2021-02-07-vault-on-firecracker-with-cni-plugins-and-nomad/"},{"content":"Firecracker1 is recently making rounds on the internet as this relatively new, awesome technology for running lightweight VMs.\nAs something coming from AWS and powering AWS Lambda, my original perception was that it’s not easy to set up and use. However, this write from Julia Evans2 proved me wrong. So, as I have recently picked up a used Dell R720 with decent amount of RAM and CPUs, it was time to take these two for a spin together.\nSipping the first coffee this gloomy Nürburgring weather morning, the thought of putting Vault on Firecracker seemed somewhat amusing, … and the day was gone.\nI’m going to show you how I’ve done it. Do I like Firecracker? It’s been only one day but yes, it’s pretty neat and easy to use.\nEnvironment Clean HWE Ubuntu 18.04.5 Server installation with the password-less sudoer user.\nIntro A Firecracker VM requires a Linux Linux Kernel and a root file system. Most of the people who talk about Firecracker use example hello-vmlinux and a root file system from AWS. I wanted to learn how to build these myself so I went with building my own 5.8 kernel and a root file system extracted from the default official HashiCorp Vault Docker image. The steps:\ninstall all the software required to build Linux kernel and install Docker setup directory structure to work with install Firecracker get, configure and build Linux kernel extract Vault file system from a running container run Vault Firecracker microVM Dependencies To avoid multiple apt-get updates, add Docker repository first:\n1 2 3 4 5 curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo add-apt-repository \\ \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) \\ stable\" Next, install all the software required to compile the kernel:\n1 2 3 4 5 6 7 8 9 sudo apt-get update sudo apt-get install \\ bison \\ build-essential \\ flex \\ git \\ libelf-dev \\ libncurses5-dev \\ libssl-dev -y followed by Docker dependencies:\n1 2 3 4 5 6 sudo apt-get install \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg-agent \\ software-properties-common -y and finally, Docker itself:\n1 2 3 4 5 6 sudo apt-get install \\ docker-ce \\ docker-ce-cli \\ containerd.io -y sudo groupadd docker # this may report that the group already exists sudo usermod -aG docker $USER Setup the directory structure To have everything in one place, create the following directories:\n1 2 3 sudo mkdir -p /firecracker sudo chown -R ${USER}:${USER} /firecracker mkdir -p /firecracker/{configs,filesystems,kernels,linux.git,releases} configs: this is where we will put the VM config JSON files filesystems: this is where the extracted file systems will reside kernels: this directory contains pre-built Linux kernels linux.git: Linux sources go here releases: Firecracker releases will be installed here Install Firecracker This is as simple as downloading a pre-built binary release from GitHub and putting it on the PATH. I wanted to have versioning available for ease of upgrading in the future so I’ve built a shell program to manage this for me. You can find the program in this repository on GitHub3. Long story, short:\ndownload install-firecracker.sh program and put it in /firecracker directory; technically does not matter where but the program assumes the /firecracker/... directory structure from the previous step chmod +x /firecracker/install-firecracker.sh run: sudo /firecracker/install-firecracker.sh This will download the latest Firecracker release, install the release in /firecracker/releases directory and create /usr/bin/firecracker-\u003cversion\u003e and /usr/bin/jailer-\u003cversion\u003e links. You’ll also get /usr/bin/firecracker and /usr/bin/jailer links pointing to the version links. You can now run Firecracker:\n1 firecracker --help Get Linux Kernel I’m going to use 5.8 kernel. Do I need one? Not sure but why not.\n1 export KERNEL_VERSION=v5.8 Clone sources from GitHub:\n1 2 3 cd /firecracker/linux.git git clone https://github.com/torvalds/linux.git . git checkout ${KERNEL_VERSION} Configure the kernel So, I’m not really fluent at this and…\nThis is the first time I’m building the kernel but fortunately one of the the Firecracker getting started documents points to a recommended kernel config. What’s less fortunate, the document talks about kernel v4.20 and the config is for v4.14.174. I’ve downloaded the file and placed it in /firecracker/linux.git/.config anyway but when I tried building the v5.8 kernel, make was insisting on recreating the config and asked me a lot of questions about what I want.\nI don’t know what I want and I don’t know if make took the values from the old .config so I basically held Enter down for a bit. make moved my .config to .config.old and gave me a new .config file for v5.8 kernel. I took the new generated file and compared it with the original v4.14.174 config.\nThese files have few thousand lines so I wrote a program in Golang which loads both versions and compares the values.\nWith a flag, it allows bringing non-existing values from the good config to the new one. You can find this program in the kernel-configs directory of this repository[]. To get the v5.8 config, I basically executed:\n1 2 3 4 go run ./compare-configs.go \\ --good-config=./4.14.174.config \\ --new-config=./5.8.config \\ --bring-old-non-existing Next, I’ve replaced the /firecracker/linux.git/.config file contents with the result of my compare configs program and re-run kernel build. This time, it did not insist on recreating anything and used my config, definitely. Happy times!\nBuild the kernel You probably want to change the value of -j to something like 4 or 8. The build will take a bit more time.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 time make vmlinux -j32 ... LD vmlinux.o MODPOST vmlinux.symvers MODINFO modules.builtin.modinfo GEN modules.builtin LD .tmp_vmlinux.kallsyms1 KSYM .tmp_vmlinux.kallsyms1.o LD .tmp_vmlinux.kallsyms2 KSYM .tmp_vmlinux.kallsyms2.o LD vmlinux SORTTAB vmlinux SYSMAP System.map real\t0m54.052s user\t23m51.313s sys\t2m35.287s After the build is complete, copy the vmlinux binary to the /firecracker/kernels directory:\n1 mv ./vmlinux /firecracker/kernels/vmlinux-${KERNEL_VERSION} Build the file system Fun part starts here. The article from Julia Evans talks about getting an init system installed in the container. I have naively tried simply extracting the file system and running it bluntly without doing any additional configuration but Firecracker was complaining about not having /sbin/openrc available. Makes sense, Docker images generally don’t need an init system. So I had to find a method to get one in.\nFortunately, Vault Docker image is built from Alpine Linux and the Creating a rootfs Image instructions show how to add an init system to a file system of an Alpine based container.\nWe’ve building a file system for Vault:\n1 export FS=vault Create a file system file and format it as ext4:\n1 2 3 rm /firecracker/filesystems/vault-root.ext4 dd if=/dev/zero of=/firecracker/filesystems/vault-root.ext4 bs=1M count=500 mkfs.ext4 /firecracker/filesystems/vault-root.ext4 I have no idea how much space I needed. 50 megs wasn’t enough so I gave it 500 instead.\nCreate a mount directory and mount the file system file:\n1 2 3 mkdir -p /firecracker/filesystems/mnt-${FS} sudo mount /firecracker/filesystems/${FS}-root.ext4 \\ /firecracker/filesystems/mnt-${FS} Now, run the Vault container and fetch the container ID, attach file system mount directory to the container:\n1 export CONTAINER_ID=$(docker run -t --rm -v /firecracker/filesystems/mnt-${FS}:/export-rootfs -d vault:latest) Get the shell in the container:\n1 docker exec -ti ${CONTAINER_ID} /bin/sh Now, in the container shell, execute these commands:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 # install the init system and some extra tools: apk add openrc apk add util-linux # set up a login terminal on the serial console (ttyS0): ln -s agetty /etc/init.d/agetty.ttyS0 echo ttyS0 \u003e /etc/securetty rc-update add agetty.ttyS0 default # Make sure special file systems are mounted on boot: rc-update add devfs boot rc-update add procfs boot rc-update add sysfs boot # EXTRA: I had to add these lines: # -------------------------------- # 1. enable local services: # https://wiki.gentoo.org/wiki//etc/local.d rc-update add local default # 2. create a local service to Start Vault dev server on system boot: echo \"#!/bin/sh\" \u003e\u003e /etc/local.d/HashiCorpVault.start echo \"/usr/local/bin/docker-entrypoint.sh server -dev \u0026\u0026 reboot || reboot\" \u003e\u003e /etc/local.d/HashiCorpVault.start # 3. make it executable: chmod +x /etc/local.d/HashiCorpVault.start # 4. For convenience, enable output from local service so I can see errors: echo rc_verbose=yes \u003e /etc/conf.d/local # 5. make sure I also have /home and /vault directories in my exported file system for d in home vault; do tar c \"/$d\" | tar x -C /export-rootfs; done # EXTRA / end # Then, copy the newly configured system to the rootfs image: for d in bin etc lib root sbin usr; do tar c \"/$d\" | tar x -C /export-rootfs; done for dir in dev proc run sys var; do mkdir /export-rootfs/${dir}; done # All done, exit docker shell exit A few words about the HashiCorpVault.start service file. One method to shut the Firecracker VM gracefully down is to call reboot from inside of the VM. This is because Firecracker exits on CPU reset, more info here. Hence the command:\n1 /usr/local/bin/docker-entrypoint.sh server -dev \u0026\u0026 reboot || reboot will start the Vault server using the regular docker-entrypoint.sh from the original image.\nThe \u0026\u0026 reboot part will ensure the VM stops automatically after Vault exits gracefully. The || reboot part will stop the VM if Vault does not start for whatever reason.\nThis saves a hassle of doing ps -a and sudo kill \u003cpid\u003e dance when things go south.\nYou can now stop the container and unmount the file system:\n1 2 docker stop ${CONTAINER_ID} sudo umount /firecracker/filesystems/mnt-${FS} Launch Vault on Firecracker Firecracker VMs can, by design, only use Linux tap devices for networking. There are tools that create ad-hoc devices using CNI plugins but I went with the method from Julia Evans. So, this part is directly lifted from Julia Evans.\nPrepare kernel boot args:\n1 2 3 4 5 export MASK_LONG=\"255.255.255.252\" export FC_IP=\"169.254.0.21\" export TAP_IP=\"169.254.0.22\" export KERNEL_BOOT_ARGS=\"ro console=ttyS0 noapic reboot=k panic=1 pci=off nomodules random.trust_cpu=on\" export KERNEL_BOOT_ARGS=\"${KERNEL_BOOT_ARGS} ip=${FC_IP}::${TAP_IP}:${MASK_LONG}::eth0:off\" Setup tap network interface:\n1 2 3 4 5 6 7 8 9 10 export TAP_DEV=\"fc-88-tap0\" export MASK_SHORT=\"/30\" export FC_MAC=\"02:FC:00:00:00:05\" sudo ip link del \"$TAP_DEV\" 2\u003e /dev/null || true sudo ip tuntap add dev \"$TAP_DEV\" mode tap sudo sysctl -w net.ipv4.conf.${TAP_DEV}.proxy_arp=1 \u003e /dev/null sudo sysctl -w net.ipv6.conf.${TAP_DEV}.disable_ipv6=1 \u003e /dev/null sudo ip addr add \"${TAP_IP}${MASK_SHORT}\" dev \"$TAP_DEV\" sudo ip link set dev \"$TAP_DEV\" up Write the VM config:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 cat \u003c\u003cEOF \u003e /firecracker/configs/vault-config.json { \"boot-source\": { \"kernel_image_path\": \"/firecracker/kernels/vmlinux-${KERNEL_VERSION}\", \"boot_args\": \"$KERNEL_BOOT_ARGS\" }, \"drives\": [ { \"drive_id\": \"rootfs\", \"path_on_host\": \"/firecracker/filesystems/${FS}-root.ext4\", \"is_root_device\": true, \"is_read_only\": false } ], \"network-interfaces\": [ { \"iface_id\": \"eth0\", \"guest_mac\": \"${FC_MAC}\", \"host_dev_name\": \"${TAP_DEV}\" } ], \"machine-config\": { \"vcpu_count\": 1, \"mem_size_mib\": 128, \"ht_enabled\": false } } EOF And launch:\n1 firecracker --no-api --config-file /firecracker/configs/vault-config.json The result will be similar to:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 [ 0.000000] Linux version 5.8.0 (radek@r720sas) (gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0, GNU ld (GNU Binutils for Ubuntu) 2.30) #4 SMP Sat Feb 6 18:50:30 UTC 2021 [ 0.000000] Command line: ro console=ttyS0 noapic reboot=k panic=1 pci=off nomodules random.trust_cpu=on ip=169.254.0.21::169.254.0.22:255.255.255.252::eth0:off root=/dev/vda rw virtio_mmio.device=4K@0xd0000000:5 virtio_mmio.device=4K@0xd0001000:6 [ 0.000000] x86/fpu: Supporting XSAVE feature 0x001: 'x87 floating point registers' [ 0.000000] x86/fpu: Supporting XSAVE feature 0x002: 'SSE registers' [ 0.000000] x86/fpu: Supporting XSAVE feature 0x004: 'AVX registers' [ 0.000000] x86/fpu: xstate_offset[2]: 576, xstate_sizes[2]: 256 [ 0.000000] x86/fpu: Enabled xstate features 0x7, context size is 832 bytes, using 'standard' format. [ 0.000000] BIOS-provided physical RAM map: [ 0.000000] BIOS-e820: [mem 0x0000000000000000-0x000000000009fbff] usable [ 0.000000] BIOS-e820: [mem 0x0000000000100000-0x0000000007ffffff] usable ... * Mounting persistent storage (pstore) filesystem ... [ ok ] * Starting local ... * Executing \"/etc/local.d/HashiCorpVault.start\" ...==\u003e Vault server configuration: Api Address: http://0.0.0.0:8200 Cgo: disabled Cluster Address: https://0.0.0.0:8201 Go Version: go1.15.7 Listener 1: tcp (addr: \"0.0.0.0:8200\", cluster address: \"0.0.0.0:8201\", max_request_duration: \"1m30s\", max_request_size: \"33554432\", tls: \"disabled\") Log Level: info Mlock: supported: true, enabled: false Recovery Mode: false Storage: inmem Version: Vault v1.6.2 Version Sha: be65a227ef2e80f8588b3b13584b5c0d9238c1d7 ==\u003e Vault server started! Log data will stream in below: 2021-02-06T21:13:22.006Z [INFO] proxy environment: http_proxy= https_proxy= no_proxy= 2021-02-06T21:13:22.007Z [WARN] no `api_addr` value specified in config or in VAULT_API_ADDR; falling back to detection if possible, but this value should be manually set 2021-02-06T21:13:22.020Z [INFO] core: security barrier not initialized 2021-02-06T21:13:22.020Z [INFO] core: security barrier initialized: stored=1 shares=1 threshold=1 ... and starts unsealed with a single unseal key. The root token is already authenticated to the CLI, so you can immediately begin using Vault. You may need to set the following environment variable: $ export VAULT_ADDR='http://0.0.0.0:8200' The unseal key and root token are displayed below in case you want to seal/unseal the Vault or re-authenticate. Unseal Key: YqKuQlzPiMUQXphehp0M7DAvsqImqNJrvJqAn/R0nyc= Root Token: s.F4erraVTvHnx3oU4Ac8zwaCP Development mode should NOT be used in production installations! Try it out by opening another terminal on the same machine and running:\n1 2 curl http://169.254.0.21:8200/sys/health {\"errors\":[]} It’s alive.\nYou can stop the VM by simply pressing CTRL+C in the terminal window where Vault is running. The VM will shut gracefully down:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 ^C==\u003e Vault shutdown triggered 2021-02-06T21:13:33.863Z [INFO] core: marked as sealed 2021-02-06T21:13:33.866Z [INFO] core: pre-seal teardown starting 2021-02-06T21:13:33.869Z [INFO] rollback: stopping rollback manager 2021-02-06T21:13:33.872Z [INFO] core: pre-seal teardown complete 2021-02-06T21:13:33.873Z [INFO] core: stopping cluster listeners 2021-02-06T21:13:33.874Z [INFO] core.cluster-listener: forwarding rpc listeners stopped 2021-02-06T21:13:34.076Z [INFO] core.cluster-listener: rpc listeners successfully shut down 2021-02-06T21:13:34.082Z [INFO] core: cluster listeners successfully shut down 2021-02-06T21:13:34.085Z [INFO] core: vault is sealed [ ok ] [ ok ] * Stopping local ... [ ok ] The system is going down NOW! Sent SIGTERM to all processes Sent SIGKILL to all processes Requesting system reboot [ 15.877742] Unregister pv shared memory for cpu 0 [ 15.880260] reboot: Restarting system [ 15.881627] reboot: machine restart radek@r720:/firecracker$ That’s all for today.\nFirecracker on GitHub ↩︎\nFirecracker: start a VM in less than a second ↩︎\nComplementary GitHub repository ↩︎\n","description":"Doing semi-meaningful things with Firecracker","tags":["firecracker","microvm","vault"],"title":"Taking Firecracker for a spin","uri":"/posts/2021-02-06-taking-firecracker-for-a-spin/"},{"content":"This is a clarification to the previous write up about Keycloak Authorization Services1. The documentation of the response_mode documents the two values which can be used: decision and permissions. In the first Keycloak article2, I have wrongly assumed that no response_mode in the grant_type=urn:ietf:params:oauth:grant-type:uma-ticket call implies the value of permissions.\nMmm, that was a wrong assumption.\nNow, looking at the documentation and trying it out, the distinction seems pretty obvious. It turns out there are three types of responses for this grant_type.\nAsking for RPT (requesting party token - an access token with permissions): without response_mode parameter 1 2 3 4 5 6 curl --silent -X POST \\ ${KEYCLOAK_TOKEN_URL} \\ -H \"Authorization: Bearer ${access_token}\" \\ --data \"grant_type=urn:ietf:params:oauth:grant-type:uma-ticket\" \\ --data \"audience=customers\" \\ --data \"permission=CustomerB#customer-b\" | jq '.' where the ${access_token} is the outcome of:\n1 2 3 4 export access_token=`curl --silent -u customers:${KEYCLOAK_CLIENT_SECRET} \\ -k -d \"grant_type=password\u0026username=member@service-team\u0026password=${USER_PASSWORD}\u0026scope=email profile\" \\ -H \"Content-Type:application/x-www-form-urlencoded\" \\ ${KEYCLOAK_TOKEN_URL} | jq '.access_token' -r` the response looks like this, the access_token is the RPT:\n1 2 3 4 5 6 7 8 9 { \"upgraded\": false, \"access_token\": \"eyJhbGciOiJSUzI1NiIsI...n8AC51T1AMwDtoqfCEXrdwcrQ\", \"expires_in\": 300, \"refresh_expires_in\": 1800, \"refresh_token\": \"eyJhbGciOiJIUz...RG3zFus\", \"token_type\": \"Bearer\", \"not-before-policy\": 0 } Asking for permissions - decision only: response_mode=decision By adding:\n--data \"response_mode=decision\" the full call is:\n1 2 3 4 5 6 7 curl --silent -X POST \\ ${KEYCLOAK_TOKEN_URL} \\ -H \"Authorization: Bearer ${access_token}\" \\ --data \"grant_type=urn:ietf:params:oauth:grant-type:uma-ticket\" \\ --data \"audience=customers\" \\ --data \"response_mode=decision\" \\ --data \"permission=CustomerB#customer-b\" | jq '.' and returns only a decision. If user has access to the resources, the response is:\n1 2 3 { \"result\": true } otherwise, the response is:\n1 2 3 4 { \"error\": \"access_denied\", \"error_description\": \"not_authorized\" } Asking for permissions: response_mode=permissions By specifying\n--data \"response_mode=permissions\" full call being:\n1 2 3 4 5 6 7 curl --silent -X POST \\ ${KEYCLOAK_TOKEN_URL} \\ -H \"Authorization: Bearer ${access_token}\" \\ --data \"grant_type=urn:ietf:params:oauth:grant-type:uma-ticket\" \\ --data \"audience=customers\" \\ --data \"response_mode=permissions\" \\ --data \"permission=CustomerB#customer-b\" | jq '.' Keycloak answers:\n1 2 3 4 5 6 7 8 9 [ { \"scopes\": [ \"customer-b\" ], \"rsid\": \"00f34b81-c45b-4e28-b267-45fad4e48b4d\", \"rsname\": \"CustomerB\" } ] However, what is more interesting, is the call without specific permissions listed:\n1 2 3 4 5 6 curl --silent -X POST \\ ${KEYCLOAK_TOKEN_URL} \\ -H \"Authorization: Bearer ${access_token}\" \\ --data \"grant_type=urn:ietf:params:oauth:grant-type:uma-ticket\" \\ --data \"audience=customers\" \\ --data \"response_mode=permissions\" | jq '.' which returns all available permissions for the original access token:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 [ { \"scopes\": [ \"customer-a\" ], \"rsid\": \"715f6cc5-8ca7-44e4-a8ce-924493db76b1\", \"rsname\": \"CustomerA\" }, { \"scopes\": [ \"customer-b\" ], \"rsid\": \"00f34b81-c45b-4e28-b267-45fad4e48b4d\", \"rsname\": \"CustomerB\" } ] Listing permissions without the name Optionally, we can ask Keycloak to not return resource names, only IDs. This is achieved by using response_include_resource_name=false, an example:\n1 2 3 4 5 6 7 curl --silent -X POST \\ ${KEYCLOAK_TOKEN_URL} \\ -H \"Authorization: Bearer ${access_token}\" \\ --data \"grant_type=urn:ietf:params:oauth:grant-type:uma-ticket\" \\ --data \"audience=customers\" \\ --data \"response_mode=permissions\" \\ --data \"response_include_resource_name=false\" | jq '.' gives:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 [ { \"scopes\": [ \"customer-a\" ], \"rsid\": \"715f6cc5-8ca7-44e4-a8ce-924493db76b1\" }, { \"scopes\": [ \"customer-b\" ], \"rsid\": \"00f34b81-c45b-4e28-b267-45fad4e48b4d\" } ] Keycloak Authorization Services - retrieving the decision only ↩︎\nIntroduction to Keycloak Authorization Services ↩︎\n","description":"Keycloak Authorization Services - RPT, permissions or a decision only","tags":["keycloak","iam","uma"],"title":"Keycloak Authorization Services - RPT, permissions or a decision only","uri":"/posts/2020-09-16-keycloak-authorization-services-rpt-permissions-or-a-decision-only/"},{"content":"","description":"","tags":null,"title":"uma","uri":"/tags/uma/"},{"content":"This always gets me, npm publish fails to authenticate:\nnpm ERR! code E401 npm ERR! Unable to authenticate, need: Basic realm=\"Artifactory Realm\" npm ERR! A complete log of this run can be found in: npm ERR! /Users/rad/.npm/_logs/...Z-debug.log The solution is:\nSign-in to JFrog. Find Edit profile under the Welcome, ... menu. Put JFrog password in and unlock. Copy the encrypted password. Issue a curl request like this: 1 curl -u ${JFROG_USER}:${JFROG_ENCRYPTED_PASSWORD} https://${JFROG_ORG}.jfrog.io/${JFROG_ORG}/api/npm/auth Copy the output and put it in ~/.npmrc. The file should be like: 1 2 3 4 _auth=\"cm...M=\" always-auth=true email=email@address registry=https://${JFROG_ORG}.jfrog.io/${JFROG_ORG}/api/npm/${JFROG_REPO}/ ","description":"Authenticate to private JFrog npm registry","tags":["npm","jfrog"],"title":"Authenticate to private JFrog npm registry","uri":"/posts/2020-09-09-authenticate-to-private-jfrog-npm-registry/"},{"content":"","description":"","tags":null,"title":"jfrog","uri":"/tags/jfrog/"},{"content":"","description":"","tags":null,"title":"multi-tenant","uri":"/tags/multi-tenant/"},{"content":"In the previous article1, I have investigated modern PKI software alternatives. One of the options on the list was HashiCorp Vault. The natural next step is to set up a Vault PKI.\nThis article documents setting up an imaginary multi-tenant Vault PKI with custom PEM bundles generated with OpenSSL. The steps the following:\ncreate a root CA with OpenSSL create intermediate CAs for imaginary clients with OpenSSL using HashiCorp Vault in development mode: import custom bundle with root and intermediate certificates configure Vault roles issue a certificate The method for generating the root and intermediate CAs comes from OpenSSL Certificate Authority guide written by Jamie Nguyen2. I’m including the scripts and the configuration in this article for reference.\nThe result As an outcome of this article, the reader will be able to create a new root CA and add new intermediate CAs to existing root CAs with a single command. This command will also prepare the Vault PEM bundle. With the Vault bundle and four more shell commands, the user will have a ready to use certificate with a CA chain. The process will be:\nrun init-intermediate.sh \u003cca-name\u003e \u003cclient-id\u003e to have an intermediate CA enable PKI for the new client with vault shell command import a PEM bundle with vault shell command add a permission with vault shell command issue a certificate with vault shell command Create a root CA I’m going to start by creating an environment file which will contain a location for the CA and distinguished name settings. As the root of the CA, I’m going to use ~/.ca directory.\n1 mkdir -p ~/.ca Now, I will create the environment file, ~/.ca/.article-ca:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 tee ~/.ca/.article-ca \u003e/dev/null \u003c\u003cEOF # DN defaults: export CA_DN_DEFAULT_COUNTRY_CODE=DE export CA_DN_DEFAULT_STATE_OR_PROVINCE=\"NRW\" export CA_DN_DEFAULT_LOCALITY=\"Herzogenrath\" export CA_DN_DEFAULT_ORG=\"klarrio.com\" export CA_DN_DEFAULT_ORG_UNIT=gmbh export CA_DN_DEFAULT_EMAIL_ADDRESS=\"radek.gruchalski@klarrio.com\" export CA_DN_DEFAULT_COMMON_NAME=\"${CA_DN_DEFAULT_ORG_UNIT}.${CA_DN_DEFAULT_ORG}\" # Certificate settings: export DEFAULT_BITS=2048 export DEFAULT_CA_DAYS=365 export DEFAULT_DAYS=90 export NS_CLIENT_CERT_COMMENT=\"OpenSSL Generated Server Certificate\" export NS_SERVER_CERT_COMMENT=\"OpenSSL Generated Server Certificate\" EOF The next step is to create a shell program responsible for writing the CA root configuration. This, and the following intermediate, are by far the two longest bits of code in this article.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 #!/usr/bin/env bash set -eu CA_NAME=${1} if [ -f ${HOME}/.ca/.${CA_NAME} ]; then source ${HOME}/.ca/.${CA_NAME} else echo ${HOME}/.ca/.${CA_NAME} not found exit 1 fi CADIR=${HOME}/.ca/${CA_NAME} mkdir -p ${CADIR} \u0026\u0026 cd ${CADIR} # init subdirectories, index.txt and serial files: mkdir certs crl newcerts private chmod 700 private touch index.txt echo 1000 \u003e serial # write the root CA config: tee openssl.cnf \u003e/dev/null \u003c\u003cEOF # OpenSSL root CA configuration file. [ ca ] # man ca default_ca = CA_default [ CA_default ] # Directory and file locations. dir = ${CADIR} certs = \\$dir/certs crl_dir = \\$dir/crl new_certs_dir = \\$dir/newcerts database = \\$dir/index.txt serial = \\$dir/serial RANDFILE = \\$dir/private/.rand # The root key and root certificate. private_key = \\$dir/private/ca.key.pem certificate = \\$dir/certs/ca.cert.pem # For certificate revocation lists. crlnumber = \\$dir/crlnumber crl = \\$dir/crl/ca.crl.pem crl_extensions = crl_ext default_crl_days = 30 # SHA-1 is deprecated, so use SHA-2 instead. default_md = sha256 name_opt = ca_default cert_opt = ca_default default_days = ${DEFAULT_DAYS} preserve = no policy = policy_strict [ policy_strict ] # The root CA should only sign intermediate certificates that match. # See the POLICY FORMAT section of 'man ca'. countryName = match stateOrProvinceName = match organizationName = match organizationalUnitName = optional commonName = supplied emailAddress = optional [ policy_loose ] # Allow the intermediate CA to sign a more diverse range of certificates. # See the POLICY FORMAT section of the 'ca' man page. countryName = optional stateOrProvinceName = optional localityName = optional organizationName = optional organizationalUnitName = optional commonName = supplied emailAddress = optional [ req ] # Options for the 'req' tool ('man req'). default_bits = ${DEFAULT_BITS} distinguished_name = req_distinguished_name string_mask = utf8only # SHA-1 is deprecated, so use SHA-2 instead. default_md = sha256 # Extension to add when the -x509 option is used. x509_extensions = v3_ca [ req_distinguished_name ] # See \u003chttps://en.wikipedia.org/wiki/Certificate_signing_request\u003e. countryName = Country Name (2 letter code) stateOrProvinceName = State or Province Name localityName = Locality Name 0.organizationName = Organization Name organizationalUnitName = Organizational Unit Name commonName = Common Name emailAddress = Email Address # Optionally, specify some defaults. countryName_default = ${CA_DN_DEFAULT_COUNTRY_CODE} stateOrProvinceName_default = \"${CA_DN_DEFAULT_STATE_OR_PROVINCE}\" localityName_default = \"${CA_DN_DEFAULT_LOCALITY}\" 0.organizationName_default = \"${CA_DN_DEFAULT_ORG}\" organizationalUnitName_default = \"${CA_DN_DEFAULT_ORG_UNIT}\" emailAddress_default = \"${CA_DN_DEFAULT_EMAIL_ADDRESS}\" [ v3_ca ] # Extensions for a typical CA ('man x509v3_config'). subjectKeyIdentifier = hash authorityKeyIdentifier = keyid:always,issuer basicConstraints = critical, CA:true keyUsage = critical, digitalSignature, cRLSign, keyCertSign [ v3_intermediate_ca ] # Extensions for a typical intermediate CA ('man x509v3_config'). subjectKeyIdentifier = hash authorityKeyIdentifier = keyid:always,issuer basicConstraints = critical, CA:true, pathlen:0 keyUsage = critical, digitalSignature, cRLSign, keyCertSign [ usr_cert ] # Extensions for client certificates ('man x509v3_config'). basicConstraints = CA:FALSE nsCertType = client, email nsComment = \"${NS_CLIENT_CERT_COMMENT}\" subjectKeyIdentifier = hash authorityKeyIdentifier = keyid,issuer keyUsage = critical, nonRepudiation, digitalSignature, keyEncipherment extendedKeyUsage = clientAuth, emailProtection [ server_cert ] # Extensions for server certificates ('man x509v3_config'). basicConstraints = CA:FALSE nsCertType = server nsComment = \"${NS_SERVER_CERT_COMMENT}\" subjectKeyIdentifier = hash authorityKeyIdentifier = keyid,issuer:always keyUsage = critical, digitalSignature, keyEncipherment extendedKeyUsage = serverAuth [ crl_ext ] # Extension for CRLs ('man x509v3_config'). authorityKeyIdentifier=keyid:always [ ocsp ] # Extension for OCSP signing certificates ('man ocsp'). basicConstraints = CA:FALSE subjectKeyIdentifier = hash authorityKeyIdentifier = keyid,issuer keyUsage = critical, digitalSignature extendedKeyUsage = critical, OCSPSignings EOF # generate root CA: openssl genrsa -aes256 -out ${CADIR}/private/ca.key.pem 4096 chmod 400 ${CADIR}/private/ca.key.pem openssl req -config ${CADIR}/openssl.cnf \\ -key ${CADIR}/private/ca.key.pem \\ -new -x509 -days ${DEFAULT_CA_DAYS} -sha256 -extensions v3_ca \\ -out ${CADIR}/certs/ca.cert.pem chmod 444 ${CADIR}/certs/ca.cert.pem openssl x509 -noout -text -in ${CADIR}/certs/ca.cert.pem After saving the program as ~/.ca/init-ca.sh, making it executable with chmod +x ~/.ca/init-ca.sh and running as ~/.ca/init-ca.sh article-ca, the new CA has been created in ~/.ca/article-ca. Using this method and changing the CA name given to the program as the first argument, more root CAs can be created.\nThe program will ask three times for the CA private key passphrase: to create the key, to verify and to use the key. Next, it will ask to confirm default values for the distinguished name. When all values are confirmed and correct, the CA root certificate will be generated.\nThe output will be similar to:\nGenerating RSA private key, 4096 bit long modulus ..........................++ ...........................................................................................................................................................................................++ e is 65537 (0x10001) Enter pass phrase for /Users/rad/.ca/article-ca/private/ca.key.pem: Verifying - Enter pass phrase for /Users/rad/.ca/article-ca/private/ca.key.pem: Enter pass phrase for /Users/rad/.ca/article-ca/private/ca.key.pem: You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter '.', the field will be left blank. ----- Country Name (2 letter code) [DE]: State or Province Name [NRW]: Locality Name [Herzogenrath]: Organization Name [klarrio.com]: Organizational Unit Name [gmbh]: Common Name []: Email Address [radek.gruchalski@klarrio.com]: Certificate: Data: Version: 3 (0x2) Serial Number: 14292483172117400839 (0xc65919b8604e4d07) Signature Algorithm: sha256WithRSAEncryption Issuer: C=DE, ST=NRW, L=Herzogenrath, O=klarrio.com, OU=gmbh/emailAddress=radek.gruchalski@klarrio.com Validity Not Before: Sep 8 20:53:48 2020 GMT Not After : Sep 8 20:53:48 2021 GMT Subject: C=DE, ST=NRW, L=Herzogenrath, O=klarrio.com, OU=gmbh/emailAddress=radek.gruchalski@klarrio.com Subject Public Key Info: Public Key Algorithm: rsaEncryption Public-Key: (4096 bit) Modulus: 00:ab:bf:02:ef:72:b3:ce:ac:4b:37:01:1b:57:fc: ... 0d:84:73:28:32:e8:f1:99:64:ee:f5:b2:6f:21:7f: 9e:08:21 Exponent: 65537 (0x10001) X509v3 extensions: X509v3 Subject Key Identifier: 06:DE:59:CD:9A:21:9F:A0:B2:32:EE:B3:E6:89:11:10:9C:6E:83:0F X509v3 Authority Key Identifier: keyid:06:DE:59:CD:9A:21:9F:A0:B2:32:EE:B3:E6:89:11:10:9C:6E:83:0F X509v3 Basic Constraints: critical CA:TRUE X509v3 Key Usage: critical Digital Signature, Certificate Sign, CRL Sign Signature Algorithm: sha256WithRSAEncryption 35:30:29:2c:39:1b:57:81:d8:95:9a:26:1f:5c:44:62:65:ca: ... fe:28:53:b1:76:63:22:cd on disk, there should be:\n1 2 3 4 5 6 7 8 9 10 11 12 13 [rad] ~ $ tree ~/.ca/article-ca/ /Users/rad/.ca/article-ca/ ├── certs │ └── ca.cert.pem ├── crl ├── index.txt ├── newcerts ├── openssl.cnf ├── private │ └── ca.key.pem └── serial 4 directories, 5 files Create an intermediate CA The intermediate CA reuses the environment file of the root CA. The program to create it is very similar to the init-ca.sh.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 #!/usr/bin/env bash set -eu CA_NAME=${1} INTERMEDIATE_NAME=${2} if [ -f ${HOME}/.ca/.${CA_NAME} ]; then source ${HOME}/.ca/.${CA_NAME} else echo ${HOME}/.ca/.${CA_NAME} not found exit 1 fi CADIR=${HOME}/.ca/${CA_NAME} INTERMEDIATEDIR=${CADIR}/intermediate/${INTERMEDIATE_NAME} mkdir -p ${INTERMEDIATEDIR} \u0026\u0026 cd ${INTERMEDIATEDIR} # init subdirectories, index.txt and serial files: mkdir certs chain crl csr newcerts private vault-bundle chmod 700 private touch index.txt echo 1000 \u003e serial echo 1000 \u003e crlnumber # write the intermediate config: tee openssl.cnf \u003e/dev/null \u003c\u003cEOF # OpenSSL intermediate CA configuration file. [ ca ] # 'man ca' default_ca = CA_default [ CA_default ] # Directory and file locations. dir = ${INTERMEDIATEDIR} certs = \\$dir/certs crl_dir = \\$dir/crl new_certs_dir = \\$dir/newcerts database = \\$dir/index.txt serial = \\$dir/serial RANDFILE = \\$dir/private/.rand # The root key and root certificate. private_key = \\$dir/private/intermediate.key.pem certificate = \\$dir/certs/intermediate.cert.pem # For certificate revocation lists. crlnumber = \\$dir/crlnumber crl = \\$dir/crl/intermediate.crl.pem crl_extensions = crl_ext default_crl_days = 30 # SHA-1 is deprecated, so use SHA-2 instead. default_md = sha256 name_opt = ca_default cert_opt = ca_default default_days = ${DEFAULT_DAYS} preserve = no policy = policy_loose [ policy_strict ] # The root CA should only sign intermediate certificates that match. # See the POLICY FORMAT section of 'man ca'. countryName = match stateOrProvinceName = match organizationName = match organizationalUnitName = optional commonName = supplied emailAddress = optional [ policy_loose ] # Allow the intermediate CA to sign a more diverse range of certificates. # See the POLICY FORMAT section of the 'ca' man page. countryName = optional stateOrProvinceName = optional localityName = optional organizationName = optional organizationalUnitName = optional commonName = supplied emailAddress = optional [ req ] # Options for the 'req' tool ('man req'). default_bits = ${DEFAULT_BITS} distinguished_name = req_distinguished_name string_mask = utf8only # SHA-1 is deprecated, so use SHA-2 instead. default_md = sha256 # Extension to add when the -x509 option is used. x509_extensions = v3_ca [ req_distinguished_name ] # See \u003chttps://en.wikipedia.org/wiki/Certificate_signing_request\u003e. countryName = Country Name (2 letter code) stateOrProvinceName = State or Province Name localityName = Locality Name 0.organizationName = Organization Name organizationalUnitName = Organizational Unit Name commonName = Common Name emailAddress = Email Address # Optionally, specify some defaults. countryName_default = ${CA_DN_DEFAULT_COUNTRY_CODE} stateOrProvinceName_default = \"${CA_DN_DEFAULT_STATE_OR_PROVINCE}\" localityName_default = \"${CA_DN_DEFAULT_LOCALITY}\" 0.organizationName_default = \"${CA_DN_DEFAULT_ORG}\" organizationalUnitName_default = \"${CA_DN_DEFAULT_ORG_UNIT}\" commonName_default = \"${INTERMEDIATE_NAME}.${CA_DN_DEFAULT_COMMON_NAME}\" emailAddress_default = \"${CA_DN_DEFAULT_EMAIL_ADDRESS}\" [ v3_ca ] # Extensions for a typical CA ('man x509v3_config'). subjectKeyIdentifier = hash authorityKeyIdentifier = keyid:always,issuer basicConstraints = critical, CA:true keyUsage = critical, digitalSignature, cRLSign, keyCertSign [ v3_intermediate_ca ] # Extensions for a typical intermediate CA ('man x509v3_config'). subjectKeyIdentifier = hash authorityKeyIdentifier = keyid:always,issuer basicConstraints = critical, CA:true, pathlen:0 keyUsage = critical, digitalSignature, cRLSign, keyCertSign [ usr_cert ] # Extensions for client certificates ('man x509v3_config'). basicConstraints = CA:FALSE nsCertType = client, email nsComment = \"${NS_CLIENT_CERT_COMMENT}\" subjectKeyIdentifier = hash authorityKeyIdentifier = keyid,issuer keyUsage = critical, nonRepudiation, digitalSignature, keyEncipherment extendedKeyUsage = clientAuth, emailProtection [ server_cert ] # Extensions for server certificates ('man x509v3_config'). basicConstraints = CA:FALSE nsCertType = server nsComment = \"${NS_SERVER_CERT_COMMENT}\" subjectKeyIdentifier = hash authorityKeyIdentifier = keyid,issuer:always keyUsage = critical, digitalSignature, keyEncipherment extendedKeyUsage = serverAuth [ crl_ext ] # Extension for CRLs ('man x509v3_config'). authorityKeyIdentifier=keyid:always [ ocsp ] # Extension for OCSP signing certificates ('man ocsp'). basicConstraints = CA:FALSE subjectKeyIdentifier = hash authorityKeyIdentifier = keyid,issuer keyUsage = critical, digitalSignature extendedKeyUsage = critical, OCSPSigning EOF # generate intermediate CA certificate for ${CA_NAME}/${INTERMEDIATE_NAME} in ${INTERMEDIATEDIR}...\" openssl genrsa -aes256 \\ -out ${INTERMEDIATEDIR}/private/intermediate.key.pem 4096 chmod 400 ${INTERMEDIATEDIR}/private/intermediate.key.pem openssl req -config ${INTERMEDIATEDIR}/openssl.cnf -new -sha256 \\ -key ${INTERMEDIATEDIR}/private/intermediate.key.pem \\ -out ${INTERMEDIATEDIR}/csr/intermediate.csr.pem openssl ca -config ${CADIR}/openssl.cnf -extensions v3_intermediate_ca \\ -days ${DEFAULT_CA_DAYS} -notext -md sha256 \\ -in ${INTERMEDIATEDIR}/csr/intermediate.csr.pem \\ -out ${INTERMEDIATEDIR}/certs/intermediate.cert.pem chmod 444 ${INTERMEDIATEDIR}/certs/intermediate.cert.pem openssl x509 -noout -text \\ -in ${INTERMEDIATEDIR}/certs/intermediate.cert.pem openssl verify -CAfile ${CADIR}/certs/ca.cert.pem \\ ${INTERMEDIATEDIR}/certs/intermediate.cert.pem # generate the chain: cat ${INTERMEDIATEDIR}/certs/intermediate.cert.pem \\ ${CADIR}/certs/ca.cert.pem \u003e ${INTERMEDIATEDIR}/chain/ca-chain.cert.pem chmod 444 ${INTERMEDIATEDIR}/certs/ca-chain.cert.pem # convert intermediate CA key to PKCS1 format required by Vault bundle openssl rsa -in ${INTERMEDIATEDIR}/private/intermediate.key.pem \\ -out ${INTERMEDIATEDIR}/vault-bundle/intermediate-pkcs1.key.pem \\ -outform pem # generate the actual Vault bundle: cat ${INTERMEDIATEDIR}/chain/ca-chain.cert.pem \\ ${INTERMEDIATEDIR}/vault-bundle/intermediate-pkcs1.key.pem \u003e \\ ${INTERMEDIATEDIR}/vault-bundle/bundle.pem # remove the pkcs1 file: rm ${INTERMEDIATEDIR}/vault-bundle/intermediate-pkcs1.key.pem After saving as ~/.ca/init-intermediate.sh, making executable with chmod +x ~/.ca/init-intermediate.sh, it can be executed. I’m going to create two intermediate CAs immediately. In my case, mirroring the imaginary use case for setting up Keycloak with multiple clients3, I have:\nClientA ~/.ca/init-intermediate.sh article-ca client-a ClientB ~/.ca/init-intermediate.sh article-ca client-b In each case, the program will ask the following:\nthe intermediate CA key password, three times: to create the key, verify the password and use the key verify to confirm distinguished name defaults, additionally a common name root CA key password during intermediate signing a couple of confirmations once again a passphrase of the intermediate CA key for pkcs1 conversion After these two commands are finished, the files on disk look similar to:\n[rad] ~ $ tree ~/.ca/article-ca/ /Users/rad/.ca/article-ca/ ├── certs │ └── ca.cert.pem ├── crl ├── index.txt ├── index.txt.attr ├── index.txt.attr.old ├── index.txt.old ├── intermediate │ ├── client-a │ │ ├── certs │ │ │ └── intermediate.cert.pem │ │ ├── chain │ │ │ └── ca-chain.cert.pem │ │ ├── crl │ │ ├── crlnumber │ │ ├── csr │ │ │ └── intermediate.csr.pem │ │ ├── index.txt │ │ ├── newcerts │ │ ├── openssl.cnf │ │ ├── private │ │ │ └── intermediate.key.pem │ │ ├── serial │ │ └── vault-bundle │ │ └── bundle.pem │ └── client-b │ ├── certs │ │ └── intermediate.cert.pem │ ├── chain │ │ └── ca-chain.cert.pem │ ├── crl │ ├── crlnumber │ ├── csr │ │ └── intermediate.csr.pem │ ├── index.txt │ ├── newcerts │ ├── openssl.cnf │ ├── private │ │ └── intermediate.key.pem │ ├── serial │ └── vault-bundle │ └── bundle.pem ├── newcerts │ ├── 1000.pem │ └── 1001.pem ├── openssl.cnf ├── private │ └── ca.key.pem ├── serial └── serial.old 21 directories, 29 files The output of the intermediate CA init, for one of the executions, is similar to:\nGenerating RSA private key, 4096 bit long modulus ...........................................++ .....................................................................................................................................................................................................................................................................................++ e is 65537 (0x10001) Enter pass phrase for /Users/rad/.ca/article-ca/intermediate/client-a/private/intermediate.key.pem: Verifying - Enter pass phrase for /Users/rad/.ca/article-ca/intermediate/client-a/private/intermediate.key.pem: Enter pass phrase for /Users/rad/.ca/article-ca/intermediate/client-a/private/intermediate.key.pem: You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter '.', the field will be left blank. ----- Country Name (2 letter code) [DE]: State or Province Name [NRW]: Locality Name [Herzogenrath]: Organization Name [klarrio.com]: Organizational Unit Name [gmbh]: Common Name [client-a.gmbh.klarrio.com]: Email Address [radek.gruchalski@klarrio.com]: Using configuration from /Users/rad/.ca/article-ca/openssl.cnf Enter pass phrase for /Users/rad/.ca/article-ca/private/ca.key.pem: Check that the request matches the signature Signature ok Certificate Details: Serial Number: 4096 (0x1000) Validity Not Before: Sep 8 21:05:08 2020 GMT Not After : Sep 8 21:05:08 2021 GMT Subject: countryName = DE stateOrProvinceName = NRW organizationName = klarrio.com organizationalUnitName = gmbh commonName = client-a.gmbh.klarrio.com emailAddress = radek.gruchalski@klarrio.com X509v3 extensions: X509v3 Subject Key Identifier: 47:7D:8A:9E:DC:23:03:7A:AA:E4:79:A8:98:EE:40:54:01:84:1C:E8 X509v3 Authority Key Identifier: keyid:06:DE:59:CD:9A:21:9F:A0:B2:32:EE:B3:E6:89:11:10:9C:6E:83:0F X509v3 Basic Constraints: critical CA:TRUE, pathlen:0 X509v3 Key Usage: critical Digital Signature, Certificate Sign, CRL Sign Certificate is to be certified until Sep 8 21:05:08 2021 GMT (365 days) Sign the certificate? [y/n]:y 1 out of 1 certificate requests certified, commit? [y/n]y Write out database with 1 new entries Data Base Updated Certificate: Data: Version: 3 (0x2) Serial Number: 4096 (0x1000) Signature Algorithm: sha256WithRSAEncryption Issuer: C=DE, ST=NRW, L=Herzogenrath, O=klarrio.com, OU=gmbh/emailAddress=radek.gruchalski@klarrio.com Validity Not Before: Sep 9 21:05:08 2020 GMT Not After : Sep 9 21:05:08 2021 GMT Subject: C=DE, ST=NRW, O=klarrio.com, OU=gmbh, CN=client-a.gmbh.klarrio.com/emailAddress=radek.gruchalski@klarrio.com Subject Public Key Info: Public Key Algorithm: rsaEncryption Public-Key: (4096 bit) Modulus: 00:aa:c3:eb:7e:c4:2a:79:2e:bc:7b:6f:c2:61:f9: ... 80:27:da:c6:b8:ff:20:3b:7f:9b:fd:ff:15:d0:2c: e7:2b:03 Exponent: 65537 (0x10001) X509v3 extensions: X509v3 Subject Key Identifier: 47:7D:8A:9E:DC:23:03:7A:AA:E4:79:A8:98:EE:40:54:01:84:1C:E8 X509v3 Authority Key Identifier: keyid:06:DE:59:CD:9A:21:9F:A0:B2:32:EE:B3:E6:89:11:10:9C:6E:83:0F X509v3 Basic Constraints: critical CA:TRUE, pathlen:0 X509v3 Key Usage: critical Digital Signature, Certificate Sign, CRL Sign Signature Algorithm: sha256WithRSAEncryption 22:b5:e3:6e:a5:d6:0d:6b:30:65:a3:d9:68:27:38:2b:ea:64: ... 39:ac:c3:66:88:8c:f0:eb /Users/rad/.ca/article-ca/intermediate/client-a/certs/intermediate.cert.pem: OK Enter pass phrase for /Users/rad/.ca/article-ca/intermediate/client-a/private/intermediate.key.pem: writing RSA key At this stage, I have a root CA and two intermediate CA for respective common names:\nClientA: client-a.gmbh.klarrio.com ClientB: client-b.gmbh.klarrio.com For every intermediate, I have a PEM bundle ready to be imported to HashiCorp Vault.\nStart Vault in development mode I’m going to use Vault development mode, with VAULT_DEV_ROOT_TOKEN_ID hard coded and default port 8200 exposed so it can be reached from localhost.\n1 2 3 4 5 6 7 8 9 10 11 12 13 docker run --rm \\ -p 8200:8200 \\ -ti \\ -e 'VAULT_DEV_ROOT_TOKEN_ID=dev-token' \\ -e 'VAULT_DEV_LISTEN_ADDRESS=0.0.0.0:8200' \\ --cap-add=IPC_LOCK \\ vault ... 2020-09-08T21:08:07.855Z [INFO] expiration: revoked lease: lease_id=auth/token/root/h128b471d9329710fbbd34dc5a4a98b4d0c7fe104799e818447a013762aed1e66 2020-09-08T21:08:07.859Z [INFO] core: successful mount: namespace= path=secret/ type=kv 2020-09-08T21:08:07.869Z [INFO] secrets.kv.kv_9795fd99: collecting keys to upgrade 2020-09-08T21:08:07.869Z [INFO] secrets.kv.kv_9795fd99: done collecting keys: num_keys=1 2020-09-08T21:08:07.869Z [INFO] secrets.kv.kv_9795fd99: upgrading keys finished Now, in another terminal window, I am going to export the token and Vault address environment variables:\n1 2 export VAULT_ADDR=http://127.0.0.1:8200 export VAULT_TOKEN=dev-token Finally, I can enable PKIs:\n1 2 vault secrets enable -path=client-a pki vault secrets enable -path=client-b pki I can see Vault confirming:\n2020-09-08T21:11:32.133Z [INFO] core: successful mount: namespace= path=client-a/ type=pki 2020-09-08T21:11:32.914Z [INFO] core: successful mount: namespace= path=client-b/ type=pki Importing the bundles The next step is to import the PEM bundles, they were prepared during the intermediate CA creation:\n1 2 3 4 [rad] ~ $ vault write client-a/config/ca pem_bundle=@${HOME}/.ca/article-ca/intermediate/client-a/vault-bundle/bundle.pem Success! Data written to: client-a/config/ca [rad] ~ $ vault write client-b/config/ca pem_bundle=@${HOME}/.ca/article-ca/intermediate/client-b/vault-bundle/bundle.pem Success! Data written to: client-b/config/ca 1 2 3 4 5 6 7 8 9 10 11 12 13 14 [rad] ~ $ vault write client-a/roles/system-of-client-a-com \\ allow_localhost=true \\ allow_bare_domains=true \\ allowed_domains=\"localhost,client-a.gmbh.klarrio.com\" \\ allow_subdomains=true \\ max_ttl=\"720h\" Success! Data written to: client-a/roles/system-of-client-a-com [rad] ~ $ vault write client-b/roles/system-of-client-b-com \\ allow_localhost=true \\ allow_bare_domains=true \\ allowed_domains=\"localhost,client-b.gmbh.klarrio.com\" \\ allow_subdomains=true \\ max_ttl=\"720h\" Success! Data written to: client-b/roles/system-of-client-b-com Notes to the examples above By setting allow_bare_domains to true, I allow the user to generate a certificate for any literal domain specified in allowed_domains, so the user can issue a certificate for client-[X].gmbh.klarrio.com. By setting allow_localhost to true, I allow issuing a certificate for localhost, useful for testing. By setting allow_subdomains to true, I give the user the ability to issue certificates with common names that are subdomains of allowed_domains. All the interesting options are documented in the Vault documentation4.\nRequesting certificates To request a certificate, simply issue the following command:\n1 2 3 4 vault write client-a/issue/system-of-client-a-com \\ common_name=\"become.client-a.gmbh.klarrio.com\" \\ ttl=\"240h\" \\ format=pem To which the output is … pretty verbose …:\nKey Value --- ----- ca_chain [-----BEGIN CERTIFICATE----- MIIF+zCCA+OgAwIBAgICEAAwDQYJKoZIhvcNAQELBQAwgYQxCzAJBgNVBAYTAkRF MQwwCgYDVQQIDANOUlcxFTATBgNVBAcMDEhlcnpvZ2VucmF0aDEUMBIGA1UECgwL ... KhlaTNBfyYaIYXeTQgCa+ar7OcZQhKMvPv1dTOFtZQ8Rjk5+8Y0yOazDZoiM8Os= -----END CERTIFICATE----- -----BEGIN CERTIFICATE----- MIIF8DCCA9igAwIBAgIJAMZZGbhgTk0HMA0GCSqGSIb3DQEBCwUAMIGEMQswCQYD ... kY3o7PdJs57rbyjVo3UWLZwQbqBkMpH3zxjKLco8lE3UscEU7Up/ZxqQF9i3Wxj1 bahjbSfvI/V5gKaVkfcp/Pe1wavMFSI0GueEzP4oU7F2YyLN -----END CERTIFICATE-----] certificate -----BEGIN CERTIFICATE----- MIIE8TCCAtmgAwIBAgIUZWaMXolFNt/ufzgUOnvu79ZCVjEwDQYJKoZIhvcNAQEL BQAwgZMxCzAJBgNVBAYTAkRFMQwwCgYDVQQIDANOUlcxFDASBgNVBAoMC2tsYXJy ... nJOv5KyaSz8OCKdX+JlVmU9Qoapj4EyXOZQ+LS8RBsFXrbjpjqxdd3kpiEhpcQwN 7UXijzXX25gZKwFPTof+VdAKa5M1/uU9G15KwL4S/vJwYGwL/zo799qrGdd/+plV cOd4GA883CW0DdC8QuU2+w3HIRS6 -----END CERTIFICATE----- expiration 1600616181 issuing_ca -----BEGIN CERTIFICATE----- MIIF+zCCA+OgAwIBAgICEAAwDQYJKoZIhvcNAQELBQAwgYQxCzAJBgNVBAYTAkRF MQwwCgYDVQQIDANOUlcxFTATBgNVBAcMDEhlcnpvZ2VucmF0aDEUMBIGA1UECgwL a2xhcnJpby5jb20xDTALBgNVBAsMBGdtYmgxKzApBgkqhkiG9w0BCQEWHHJhZGVr ... pIjI/wXZwURNn920ODhsA0v467Llw4gMTTjxOfDIFrdZ46936IMzHNOXI5b87dfU KhlaTNBfyYaIYXeTQgCa+ar7OcZQhKMvPv1dTOFtZQ8Rjk5+8Y0yOazDZoiM8Os= -----END CERTIFICATE----- private_key -----BEGIN RSA PRIVATE KEY----- MIIEpAIBAAKCAQEA57Ww9OC6UzHaLAALdbotEoTf5c2qw4BufVlOB7zZh+GbX2hR ... zcDV96GoXPEnuXHdVsfePFIdPS7IRmAEn72UW6u39mVqGJuX1/5tk76ay6cPHP/B xtX2UAplk9bD046wNh1/PxudBnqavOFf4F5uDizYhyihbmmhS/mcxA== -----END RSA PRIVATE KEY----- private_key_type rsa serial_number 65:66:8c:5e:89:45:36:df:ee:7f:38:14:3a:7b:ee:ef:d6:42:56:31 Cool, we have the certificate, a private key and a certificate chain. The CA chain can now be copied to chain.pem file.\nPay attention to the -----END CERTIFICATE----- -----BEGIN CERTIFICATE----- bit. It needs to be saved as:\n... -----END CERTIFICATE----- -----BEGIN CERTIFICATE----- ... Copy the certificate to certificate.pem:\nAnd the private to key.pem:\nA server with TLS transport can now be created using the chain.pem, certificate.pem and key.pem files.\nWhy is this cool It’s not possible to request certificates for domain names not in allowed_domains: 1 2 3 4 5 6 7 8 9 10 [rad] ~ $ vault write client-a/issue/system-of-client-a-com \\ common_name=\"become.client-b.gmbh.klarrio.com\" \\ ttl=\"240h\" \\ format=pem Error writing data to client-a/issue/system-of-client-a-com: Error making API request. URL: PUT http://127.0.0.1:8200/v1/client-a/issue/system-of-client-a-com Code: 400. Errors: * common name become.client-b.gmbh.klarrio.com not allowed by this role It’s possible to provide alt names: 1 2 3 4 5 vault write client-b/issue/system-of-client-b-com \\ common_name=\"get-in-touch.client-b.gmbh.klarrio.com\" \\ alt_names=\"you.client-b.gmbh.klarrio.com,or-you.client-b.gmbh.klarrio.com\" \\ ttl=\"720h\" \\ format=pem Certificates generated for ClientA do not validate for ClientB and vice versa - different intermediate It is very easy to define a policy with different criteria. For example: to allow a user of ClientB to generate a certificate for specific domain only: 1 2 3 4 [rad] ~ $ vault write client-a/roles/get-in-touch \\ allow_bare_domains=true \\ allowed_domains=\"get-in-touch.client-a.gmbh.klarrio.com\" \\ max_ttl=\"1h\" so that it’s possible to:\n1 2 3 4 vault write client-a/issue/get-in-touch \\ common_name=\"get-in-touch.client-a.gmbh.klarrio.com\" \\ ttl=\"1h\" \\ format=pem but not:\n1 2 3 4 vault write client-a/issue/get-in-touch \\ common_name=\"maybe.get-in-touch.client-a.gmbh.klarrio.com\" \\ ttl=\"1h\" \\ format=pem Conclusion Once the initial setting up of the program to create the CAs is finished, the rest of the process is straightforward. Adding new clients (intermediates) is fairly easy. Configuring Vault is not a big job. Vault PKI is a solid choice for a multi-tenant PKI solution.\nOf course, there are still some challenges left on the table. Mainly storing the root and intermediate certificates safely and properly ensuring who can access them.\nCertificate Authority is not Voodoo ↩︎\nOpenSSL Certificate Authority ↩︎\nIntroduction to Keycloak Authorization Services ↩︎\nVault PKI Create/Update Role Parameters ↩︎\n","description":"Configuring multi-tenant Vault PKI with OpenSSL root and intermediate CAs","tags":["pki","ca","tls","openssl","vault","multi-tenant"],"title":"Multi-tenant Vault PKI with custom root PEM bundles","uri":"/posts/2020-09-09-multi-tenant-vault-pki-with-custom-root-pem-bundle/"},{"content":"","description":"","tags":null,"title":"npm","uri":"/tags/npm/"},{"content":"","description":"","tags":null,"title":"openssl","uri":"/tags/openssl/"},{"content":"Modern applications tend to get fairly complex pretty quick. A usual stack will consist of many moving parts. Starting from a cloud environment, maybe abstracted behind Kubernetes or Mesos, through multitude of web servers, GRPC services, to monitoring systems like Grafana, Jaeger, Prometheus, all fronted with load balancers or proxies like Traefik. Many of these components have fairly complex dependencies, ETCD or Zookeeper come to mind. All these power a highly dynamic environment where containers and virtual machines iterate and get replaced often. Some businesses operate multiple copies of stacks for development, staging and production environments.\nWith so many gizmos and the security breaches happening on a nearly daily basis, it’s important to ensure that the traffic between the individual systems can be traced, what is probably even more important: authenticated and encrypted.\nIn cloud environments, vendor provided tools help to a certain extent. For example, a common pattern in AWS is to use security groups or / and ACLs. As efficient as they are, one only gets so far. Security groups will not help with containerized workflows. An individual host can run containers for different workloads, maybe different customers. Maybe Kubernetes has some answers for some with its networking capabilities. For more advanced requirements, Project Calico 1 may come handy. But even then, certain workflows can’t be covered with any of that.\nThis is where many people will turn to TLS. Today, it is very difficult to find software that would not support TLS out of the box. With HTTP / public facing applications, things are pretty easy. Thanks to Let’s Encrypt2, anybody can get TLS certificates for public facing systems for free. That is awesome. But what about internal infrastructure? Let’s Encrypt, even though the limits are generous, does not really work very well. Fair percentage of software integrates well with Let’s Encrypt. For example nginx, Traefik. But not everything. And what about the systems identified by IP addresses? What about internal DNS names?\nOrganizations end up shoehorning a custom TLS solution, very often disregarding the most reasonable answer: running a custom Certificate Authority.\nTo be honest, even a few years ago, running a custom PKI, Certificate Authority, beeing a part of it, was a hassle. Searching for ready recipes for how to set things up in order to get a root certificate, many tutorials glossing over the importance of intermediate certificates, certificate signing requests, … Ins and outs of configuring and running a CA seemed to be some secret, insider’s knowledge.\nMy personal perception is that the raise of Golang, with its unprecedented ease of crypto library use, broke this trend and brought TLS to the masses. There are many complete code examples in Golang tests. Even today, not taking away from the authors of BouncyCastle (which is really, really, really awesome when one knows the library), trying to do more complex things with TLS in Java can be mind boggling. Maybe the impulse came in 2013, with Ivan Ristić publishing the Bulletproof SSL and TLS 3 book, which is a immense resource for anybody trying to master SSL and TLS, I don’t know. Sorry, slight deviation, personal perception…\nA software developer who is looking to implement TLS at some point is going to search for\nhow to generate an ssl certificate\nAnswers will turn up. Majority of these will show how to get a self-signed certificate. A self-signed certificate is generally okay, for example, in integration testing there is no real need for a CA. In tests, it does not matter who issued a certificate, what matters is that TLS works.\nHowever, the problem with a self-signed certificate is that it is not signed by a Certificate Authority. So, yes, it does provide security in the sense that the traffic is TLS encrypted, but it does not tell anything about who generated and deployed the certificate. Anybody can issue a self-signed certificate for anything and it’s going to take some effort to figure out where it came from. A Certificate Authority with the correct set of intermediates is going to to provide a verification chain required to properly ensure the authenticity of a certificate.\nIt’s fair to say that a production system should not see a self-signed certificate in operation.\nWhat are the options Below is a very brief overview of a few modern Certificate Authority options.\nOpenSSL Certificate Authority Starting from the most widely available tool for creating a Certificate Authority: OpenSSL. The one and only resource I can recommend for anybody who would like to setup and operate a CA with OpenSSL is the OpenSSL Certificate Authority guide created by Jamie Nguyen. 4 The reader of the guide will find a very well documented step-by-step guide, where every step is easy to follow and well explained. An appendix to the the guide contains ready to use recipes.\nAll documents available on that website are licensed under Creative Commons Attribution-ShareAlike 4.0 International License.\nFor actual operating, a knowledge of OpenSSL will be required.\nCFSSL A good alternative is CloudFlare’s PKI/TLS toolkit: CFSSL. 5 Written in Golang, available on GitHub. CFSSL offers a command line and a HTTP API for signing, verifying and bundling TLS certificates. With CFSSL, one can stand multiple Certificate Authorities within minutes. The readme offers a pretty good overview of the capabilities.\nAvailable under BSD-2-Clause License.\nSquare certstrap Another tool written in Golang, this time from Square, the company providing financial and merchant services. Square certstrap is also available on GitHub. 6 This tool offers a very easy to use command line API for creating new Certificate Authorities, identities, certificate signing requests and generating and signing certificates. As with the previous solution, the readme gives a great overview or what’s possible.\nAvailable under Apache 2.0 License.\nHashiCorp Vault HashiCorp is a very prominent player in the modern infrastructure software. Known for tools like Vagrant, Packer, Terraform, Consul, and Vault. One of the features of Vault is the PKI. From all the tools mentioned here, HashiCorp Vault PKI 7 is by far most complete and the easiest to integrate into an existing infrastructure.\nIt comes packaged as a binary, there are Docker images available. Vault itself can be configured for high availability. It offers a command line API and a HTTP API. Vault can host multiple PKIs in a single installation. Access to each PKI can be controlled using Vault policies and the access is protected by tokens with configurable leases. It’s really good. By default, Vault PKI is configured with a root certificate generated by Vault. Alternatively, a root bundle can be imported. There is a very good guide on setting up a fresh PKI with Vault: Build Your Own Certificate Authority (CA).\nConclusion: lower bar of entry No matter which tool is the tool of choice, there are still challenges in operating a Certificate Authority. Root and intermediate certificates have to be stored secure, they need to be rotated. Preparing an underlying, bulletproof infrastructure for reliably relaunching a Certificate Authority is not a walk in the park. Certificate Revocation Lists are still difficult. In certain environments, certificate distribution is a challenge.\nOn a positive note: the bar of running a CA is today much lower.\nProject Calico ↩︎\nLet’s Encrypt ↩︎\nUnderstanding and deploying SSL/TLS and PKI to secure servers and web applications, by Ivan Ristić ↩︎\nOpenSSL Certificate Authority ↩︎\nCloudFlare’s PKI/TLS toolkit ↩︎\nSquare certstrap ↩︎\nHashiCorp Vault PKI ↩︎\n","description":"Investigation of available options for running and operating a Certificate Authority for modern applications","tags":["pki","ca","tls","ssl","infrastructure"],"title":"Certificate Authority is not Voodoo","uri":"/posts/2020-09-07-certificate-authority-is-not-voodoo/"},{"content":"","description":"","tags":null,"title":"infrastructure","uri":"/tags/infrastructure/"},{"content":"","description":"","tags":null,"title":"ssl","uri":"/tags/ssl/"},{"content":"In Introduction to Keycloak Authorization Services 1, I have described how to use the Authorization Services to find out if the user has access to certain resources.\nI have done so by asking Keycloak to issue an access token with a special grant_type with the value of urn:ietf:params:oauth:grant-type:uma-ticket which returned a list of permissions the has access to.\nThe request looked like this:\n1 2 3 4 5 6 curl --silent -X POST \\ ${KEYCLOAK_TOKEN_URL} \\ -H \"Authorization: Bearer ${access_token}\" \\ --data \"grant_type=urn:ietf:params:oauth:grant-type:uma-ticket\" \\ --data \"audience=customers\" \\ --data \"permission=CustomerB#customer-b\" | jq '.' where the ${access_token} was the result of\n1 2 3 4 export access_token=`curl --silent -u customers:${KEYCLOAK_CLIENT_SECRET} \\ -k -d \"grant_type=password\u0026username=member@service-team\u0026password=${USER_PASSWORD}\u0026scope=email profile\" \\ -H \"Content-Type:application/x-www-form-urlencoded\" \\ ${KEYCLOAK_TOKEN_URL} | jq '.access_token' -r` The response I was looking for looked like:\n1 2 3 4 5 6 7 8 9 { \"upgraded\": false, \"access_token\": \"eyJhbGciOiJSUzI1NiIsI...n8AC51T1AMwDtoqfCEXrdwcrQ\", \"expires_in\": 300, \"refresh_expires_in\": 1800, \"refresh_token\": \"eyJhbGciOiJIUz...RG3zFus\", \"token_type\": \"Bearer\", \"not-before-policy\": 0 } However, since I was only interested in the confirmation if the user has access to certain resources, I could have used an additional parameter to the grant_type=urn:ietf:params:oauth:grant-type:uma-ticket call:\n--data \"response_mode=decision\" With this parameter, the call to retrieve the decision would look like:\n1 2 3 4 5 6 7 curl --silent -X POST \\ ${KEYCLOAK_TOKEN_URL} \\ -H \"Authorization: Bearer ${access_token}\" \\ --data \"grant_type=urn:ietf:params:oauth:grant-type:uma-ticket\" \\ --data \"audience=customers\" \\ --data \"response_mode=decision\" \\ --data \"permission=CustomerB#customer-b\" | jq '.' In case of the user having access, Keycloak would return:\n1 2 3 { \"result\": true } Otherwise, the response would be:\n1 2 3 4 { \"error\": \"access_denied\", \"error_description\": \"not_authorized\" } Introduction to Keycloak Authorization Services ↩︎\n","description":"Retrieving just the access decision from Key Authorization Services","tags":["keycloak","iam","uma"],"title":"Keycloak Authorization Services - retrieving the decision only","uri":"/posts/2020-09-06-keycloak-authorization-services-decision-only/"},{"content":"As the number of applications and websites in the organization grows, the developer will inevitably receive a request to implement Single Sign-On. Single Sign-On (SSO for short) is an authentication scheme allowing the user to log in with a single set of credentials and share the session across multiple, independent, potentially unrelated systems.\nThe savvy developer will roll out Keycloak, enable Standard Flow client, maybe enable some of the social login options, like GitHub, Google or Facebook and call it a day. The users will be happy. When they go to any of the internet properties requiring signing in, if they are not signed in, they will be redirected to Keycloak login page. Once they log in, they receive a token, with which they can use to access any other property requiring login.\nAs a bonus, the SSO usually introduces a Single Log-Out (SLO). By invalidating the access token, the user is logged out of all the properties relying on that token.\ntable of contents setting up the scene the goal add a realm configure roles the OpenID Client settings tab scope tab authorization tab authorization / settings authorization / policies authorization / authorization scopes authorization / resources authorization / permissions create a user user credentials fetch the OpenID Client credentials play time! UMA tickets top secret customer listing available customers conclusion further reading setting up the scene But what if the complexity of the system goes one step further? For example, the user of the properties is a Member of the Support Team and the property in question is a support system where, for example, the Support Team member can view and manage some data on behalf of a Customer. The company has many Customers and many Support Team Members. Maybe some of the Support Team Members are dedicated to certain Customers? When they sign in to the Support System, they should only see and be able to act, on behalf of only those selected, dedicated Customers.\nThe first thought of any seasoned developer would most likely be to create a new database and store a mapping between the Support Team Member user and the Customer. The Support Application would then query the new database and only display the Customers for which the mappings exist. And that’s fine, there is nothing wrong with approach. However, that’s another database to maintain. Someone has to create the rules of which Member supports which Customer. This knowledge has to be stored somewhere and someone has to build an application to ensure the data in the new database is always up to date and relevant.\nAn alternative approach would be to use Keycloak for storing, managing and retrieval of all of this knowledge. If we consider Keycloak to be a single source of truth across the organization, we remove quite a lot of complexity.\nSo, further in this article, I am showing a proof of concept of Keycloak as a mechanism to allow Support Team Members to access selected Customers only, without any other database. This will also present how to use Keycloak Authorization Services in real-world scenario and give the reader a glimpse into User Managed Access (UMA).\nI have described how to start a local development version of Keycloak. Examples here will build on top of the previous write up. 1\nthe goal The outcome of this article is to have a Keycloak realm with an OpenID client configured so that a program can be created to query Keycloak for users’ entitlements and discover all available entitlements of a given type by leveraging Keycloak token and resource set endpoints.\nWe will configure a realm with required roles and set up Authorization Services resources, policies, scopes and permissions for two different access levels: a regular user, Service Team Member, and a supervisor, the user who is entitled to see all available resources, regardless of the role membership.\nWithout any further due, let’s start!\nadd a realm Open the browser, go to http://localhost:28080/auth/admin/master/console/, sign in as admin:admin.\nBy default, we are signed in to the Master realm. So the first thing to do, is to create a new realm. In the top left corner, under the Keycloak logo, hover over Master or Select realm text. A menu will appear, there is the Add realm button. Click the button and on the form that shows up, type multi-customer in the Name field.\nThe realm is our disposable proving ground. All the users, roles and everything we will do further, resides inside. Deleting a realm, deletes all users and settings.\nconfigure roles We will represent our imaginary Customers as roles. We have to add a role for every Customer. In the left menu, find and click Roles. Create a role for every customer, for the sake of this article, I’ll go with:\nCustomerA CustomerB the OpenID Client Now, we have to create an OpenID Client. Client is what allows the users of our application securely exchanging client id and secret for an access token. Click Clients in the left menu. On the page that opens, find the Create button near the top of the right corner of the page, click it.\nType customers as a Client ID. Leave Client Protocol as openid-connect and put http://localhost:28080 as Root URL. Click Save. The page will reload and a bunch of other settings will become available.\nsettings tab Set them as follows:\nAccess Type: confidential Standard Flow Enabled: off Implicit Flow enabled: off Direct Grants Enabled: on Authorization Enabled: on this will enable Service Accounts URLs are pre-populated and good for what we need to do Click Save the Authorization tab will appear scope tab Great, now go to Scope tab.\nFull Scope Allowed: off Realm Roles will appear. Select all Available Roles and click Add selected button.\nauthorization tab This is where things get a little bit involved. A bunch of new tabs have appeared.\nauthorization / settings Policy Enforcement Mode: Enforcing Decision Strategy: Unanimous Remote Resource Management: off Click Save.\nauthorization / policies delete Default Policy For each Customer, create a Role based policy (dropdown on the right side of the page): CustomerA: Name: Policy-CustomerA Realm Roles: type and select: CustomerA (click Required Logic: Positive CustomerB: Name: Policy-CustomerB Realm Roles: type and select: CustomerB (click Required) Logic: Positive authorization / authorization scopes Create a scope for each customer:\nfor CustomerA: customer-a for CustomerB: customer-b authorization / resources delete Default Resource Create a resource for each customer:\nCustomerA: Name: CustomerA Display Name: Customer A Resource Type: urn:customers:resources:customer URI: /customers/CustomerA (irrelevant for our use case) Scope: customer-a User Managed Access: off CustomerB: Name: CustomerB Display Name: Customer B Resource Type: urn:customers:resources:customer URI: /customers/CustomerB (irrelevant for our use case) Scope: customer-b User Managed Access: off authorization / permissions Finally, create permissions. One Scope-Based permission per customer. As with policies, the option to add is on the right side of the screen, a dropdown.\nCustomerA: Name: CustomerA Permission Resource: CustomerA Scope: customer-a Apply policy: Select Existing Policy: Policy-CustomerA Decision strategy: Unanimous CustomerB: Name: CustomerB Permission Resource: CustomerB Scope: customer-b Apply policy: Select Existing Policy: Policy-CustomerB Decision strategy: Unanimous We have finished setting up Authorization services.\nBefore we can play around, we will add our Support Team Member user to Keycloak.\ncreate a user In the left Keycloak menu, click Users. On the right hand side of the Lookup header, there is an Add user button. Click it. Populate the fields as follows:\nUsername: member@service-team Email: member@service-team First name: Member Last name: ServiceTeam User enabled: on Email verified: on Click Save.\nuser credentials We are going to be interacting with Keycloak via command line only and the purpose of this exercise is to validate specific user’s resource access. Hence, we need to set the password for the user because we will use Resource owner credentials grant Section 4.3.\nSet password: password123 Temporary: off Click Set password.\nfetch the OpenID Client credentials Once again, in the left Keycloak menu, click Clients. Find customers client and click on it. Go to Credentials tab. Your client id is the client name—customers. The secret is displayed. Copy it and in the terminal, export as:\n1 export KEYCLOAK_CLIENT_SECRET=... Also, export the username and password.\nThis is obviously done only for the sake of this tutorial. Don’t export passwords or secrets like this in a production system. Really, never. Use something like Ansible Vault or HashiCorp Vault to store secrets.\nEven storing the password in the file and using:\n1 $(cat /path/to/the/password/file) would be better than what we do below. But now…\n1 2 export USER_NAME=member@service-team export USER_PASSWORD=password123 play time! If you receive an Invalid bearer token error at any step further, you need to obtain a new access token. It simply means the token has expired.\nWhoa, that was a lot of stuff to set up! The good news, all that can be easily automated. But it was important to execute this once manually to see what goes where. As we have done it, we are ready for some real action!\nIn the same terminal window where we exported the secret and user password, let’s export the token URL so the examples below are a little bit more concise.\n1 export KEYCLOAK_TOKEN_URL=http://127.0.0.1:28080/auth/realms/multi-customer/protocol/openid-connect/token You can introspect your realm by going to\nhttp://localhost:28080/auth/realms/multi-customer/.well-known/openid-configuration/.\nKeep in mind, we haven’t assigned any roles to member@service-team user yet, other than what’s the default for Keycloak: offline_access and uma_authorization.\nLet’s see if we can obtain an access token:\n1 2 3 4 curl --silent -u customers:${KEYCLOAK_CLIENT_SECRET} \\ -k -d \"grant_type=password\u0026username=${USER_NAME}\u0026password=${USER_PASSWORD}\u0026scope=email profile\" \\ -H \"Content-Type:application/x-www-form-urlencoded\" \\ ${KEYCLOAK_TOKEN_URL} | jq '.' -r The outcome should be similar to this:\n1 2 3 4 5 6 7 8 9 10 { \"access_token\": \"eyJhbGciOiJSUzI1NiIsIn...iWNlvIOVA\", \"expires_in\": 300, \"refresh_expires_in\": 1800, \"refresh_token\": \"eyJhbGciOiJIUzI1N...K6O5QoSDl_t2JA\", \"token_type\": \"bearer\", \"not-before-policy\": 0, \"session_state\": \"c0536fd1-35f3-4563-b1a4-006f59da7465\", \"scope\": \"profile email\" } We will always need the value of the access token so further, we will export the access token as an environment variables directly from curl output using jq.\nOkay, let’s get another token then…\n1 2 3 4 export access_token=`curl --silent -u customers:${KEYCLOAK_CLIENT_SECRET} \\ -k -d \"grant_type=password\u0026username=${USER_NAME}\u0026password=${USER_PASSWORD}\u0026scope=email profile\" \\ -H \"Content-Type:application/x-www-form-urlencoded\" \\ ${KEYCLOAK_TOKEN_URL} | jq '.access_token' -r` UMA tickets From Wikipedia:\nUMA stands for User Managed Access and is an OAuth based access management protocol standard.\nIt enables a resource owner to control the authorization of data sharing and other protected-resource access made between online services on the owner’s behalf or with the owner’s authorization by an autonomous requesting party. 2\nWe can now ask Keycloak to tell us which resources the user has access to.\nIn order to do so, we have to ask for an UMA token by sending a post request to the realm token endpoint with our existing access token and a special grant type: urn:ietf:params:oauth:grant-type:uma-ticket.\nThe audience parameter is required.\n1 2 3 4 5 curl --silent -X POST \\ ${KEYCLOAK_TOKEN_URL} \\ -H \"Authorization: Bearer ${access_token}\" \\ --data \"grant_type=urn:ietf:params:oauth:grant-type:uma-ticket\" \\ --data \"audience=customers\" We receive the response:\n1 {\"error\":\"access_denied\",\"error_description\":\"not_authorized\"} I know it does not look like it but this is a Great News!\nThe reason why we have received this answer is because we have removed the Default Resource and not assigned any customer roles to our user. Let’s change that.\nGo to the user roles (Manage / Users (left menu in Keycloak) / member@service-team / Role Mappings) and assign CustomerA role.\nObtain another access token:\n1 2 3 4 export access_token=`curl --silent -u customers:${KEYCLOAK_CLIENT_SECRET} \\ -k -d \"grant_type=password\u0026username=${USER_NAME}\u0026password=${USER_PASSWORD}\u0026scope=email profile\" \\ -H \"Content-Type:application/x-www-form-urlencoded\" \\ ${KEYCLOAK_TOKEN_URL} | jq '.access_token' -r` and rerun the previous command:\n1 2 3 4 5 curl --silent -X POST \\ ${KEYCLOAK_TOKEN_URL} \\ -H \"Authorization: Bearer ${access_token}\" \\ --data \"grant_type=urn:ietf:params:oauth:grant-type:uma-ticket\" \\ --data \"audience=customers\" | jq '.' The response is different! Better! We can look at what we are interested in:\n1 2 3 4 5 6 7 8 9 { \"upgraded\": false, \"access_token\": \"eyJhbGci...7I_pA\", \"expires_in\": 300, \"refresh_expires_in\": 1800, \"refresh_token\": \"eyJhbGc...QYko\", \"token_type\": \"Bearer\", \"not-before-policy\": 0 } Copy the access_token from this response and decode it in jwt.io (or any other tool, it’s just three different base64 encoded strings concatenated with a dot). Look at realm_access and authorization claims. They are like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 \"realm_access\": { \"roles\": [ \"CustomerA\", \"offline_access\", \"uma_authorization\" ] }, \"authorization\": { \"permissions\": [ { \"scopes\": [ \"customer-a\" ], \"rsid\": \"715f6cc5-8ca7-44e4-a8ce-924493db76b1\", \"rsname\": \"CustomerA\" } ] }, This response tells us that the user behind the Bearer token is allowed access to CustomerA using customer-a scope. The realm_access.roles claim contains the CustomerA role but we get that info in a regular access token already.\nHowever, there is no CustomerB on this list.\nFair enough, let’s ask th server Keycloak directly if this user has access to CustomerB:\n1 2 3 4 5 6 curl --silent -X POST \\ ${KEYCLOAK_TOKEN_URL} \\ -H \"Authorization: Bearer ${access_token}\" \\ --data \"grant_type=urn:ietf:params:oauth:grant-type:uma-ticket\" \\ --data \"audience=customers\" \\ --data \"permission=CustomerB#customer-b\" | jq '.' The response is:\n1 2 3 4 { \"error\": \"access_denied\", \"error_description\": \"not_authorized\" } Correct! The user does not have access to the CustomerB. What if we don’t specify a scope?\n1 2 3 4 5 6 curl --silent -X POST \\ ${KEYCLOAK_TOKEN_URL} \\ -H \"Authorization: Bearer ${access_token}\" \\ --data \"grant_type=urn:ietf:params:oauth:grant-type:uma-ticket\" \\ --data \"audience=customers\" \\ --data \"permission=CustomerB\" | jq '.' The response is:\n1 2 3 4 { \"error\": \"access_denied\", \"error_description\": \"not_authorized\" } Correct again! But let’s verify that this indeed works for CustomerA, the user should have access:\n1 2 3 4 5 6 curl --silent -X POST \\ ${KEYCLOAK_TOKEN_URL} \\ -H \"Authorization: Bearer ${access_token}\" \\ --data \"grant_type=urn:ietf:params:oauth:grant-type:uma-ticket\" \\ --data \"audience=customers\" \\ --data \"permission=CustomerA\" | jq '.' We receive:\n1 2 3 4 5 6 7 8 9 { \"upgraded\": false, \"access_token\": \"eyJhbGciOiJSUzI...7dlw\", \"expires_in\": 300, \"refresh_expires_in\": 1800, \"refresh_token\": \"eyJhbGciOi...QaM2ZA9nY1M\", \"token_type\": \"Bearer\", \"not-before-policy\": 0 } Which is correct. Can the user request CustomerA resource with incorrect scope?\n1 2 3 4 5 6 curl --silent -X POST \\ ${KEYCLOAK_TOKEN_URL} \\ -H \"Authorization: Bearer ${access_token}\" \\ --data \"grant_type=urn:ietf:params:oauth:grant-type:uma-ticket\" \\ --data \"audience=customers\" \\ --data \"permission=CustomerA#customer-b\" | jq '.' 1 2 3 4 { \"error\": \"invalid_resource\", \"error_description\": \"Resource with id [CustomerA] does not exist.\" } Correct, the resource does not exist with this scope! So let’s use the correct scope again, just to make sure everything is fine:\n1 2 3 4 5 6 curl --silent -X POST \\ ${KEYCLOAK_TOKEN_URL} \\ -H \"Authorization: Bearer ${access_token}\" \\ --data \"grant_type=urn:ietf:params:oauth:grant-type:uma-ticket\" \\ --data \"audience=customers\" \\ --data \"permission=CustomerA#customer-a\" | jq '.' Once again, we get:\n1 2 3 4 5 6 7 8 9 { \"upgraded\": false, \"access_token\": \"eyJhbGciOiJSUzI1N...p3tFd4cjH1UAGOlY0g_U3b5Lj19zH4I3wkzA\", \"expires_in\": 300, \"refresh_expires_in\": 1800, \"refresh_token\": \"eyJhbGciOiJIUzI1N...FeEV8qFCVLM\", \"token_type\": \"Bearer\", \"not-before-policy\": 0 } Phew. So far so good. Now, go to the user Role Mappings and assign CustomerB role. The user now has offline_access, uma_authorization, CustomerA and CustomerB roles assigned.\nWe require a new token:\n1 2 3 4 export access_token=`curl --silent -u customers:${KEYCLOAK_CLIENT_SECRET} \\ -k -d \"grant_type=password\u0026username=member@service-team\u0026password=${USER_PASSWORD}\u0026scope=email profile\" \\ -H \"Content-Type:application/x-www-form-urlencoded\" \\ ${KEYCLOAK_TOKEN_URL} | jq '.access_token' -r` Do we now have the permission to access CustomerB? Well, let’s find out:\n1 2 3 4 5 6 curl --silent -X POST \\ ${KEYCLOAK_TOKEN_URL} \\ -H \"Authorization: Bearer ${access_token}\" \\ --data \"grant_type=urn:ietf:params:oauth:grant-type:uma-ticket\" \\ --data \"audience=customers\" \\ --data \"permission=CustomerB\" | jq '.' Gives us:\n1 2 3 4 5 6 7 8 9 { \"upgraded\": false, \"access_token\": \"eyJhbGciOiJSUzI1NiIsI...n8AC51T1AMwDtoqfCEXrdwcrQ\", \"expires_in\": 300, \"refresh_expires_in\": 1800, \"refresh_token\": \"eyJhbGciOiJIUz...RG3zFus\", \"token_type\": \"Bearer\", \"not-before-policy\": 0 } And with the scope?\n1 2 3 4 5 6 curl --silent -X POST \\ ${KEYCLOAK_TOKEN_URL} \\ -H \"Authorization: Bearer ${access_token}\" \\ --data \"grant_type=urn:ietf:params:oauth:grant-type:uma-ticket\" \\ --data \"audience=customers\" \\ --data \"permission=CustomerB#customer-b\" | jq '.' 1 2 3 4 5 6 7 8 9 { \"upgraded\": false, \"access_token\": \"eyJhbGciOiJSUzI1NiIsInR5cCIgOiAiSldUIiwia2...KxKyysheoaDgwRaVkyl158flOD5CtyHbjKFkWhA\", \"expires_in\": 300, \"refresh_expires_in\": 1800, \"refresh_token\": \"eyJhbGciOi...1tBGk6vebI\", \"token_type\": \"Bearer\", \"not-before-policy\": 0 } Cool! Everything works. Our regular user can now see all customers he has been given access to.\ntop secret customer Eventually, we may decide that we should be able to discover all customer resources available in our Keycloak resource server. What would be unfortunate though, if our regular user could see customers who they should never be aware of.\nFrankly speaking, we could have a TopSecretCustomer in the system and nobody, ever, except of the TopSecretCustomer (and us) should be aware of their existence.\nKeycloak offers something called a resource set. Resource set allows us to introspect resources available on our resource server.\nIn order to access the resource set, the user must have the uma_protection role of the client assigned. Which is great. This implies we can just create a dedicated user with access to the resource set. Separation of concern at work! …\nLet’s do so.\nlisting available customers Go to Manage / Users and click Add user once again. Set the following:\nUsername: supervisor@company Email: supervisor@company First name: Supervisor Last name: Company User enabled and Email verified: on Click Save.\nSet the password. Go to Credentials tab and set the password to password123!, Temporary: off. Click Set password. Now, go to Role Mappings and in the Client Roles, select customers client. Select uma_protection from Available roles and click Add selected.\nOn the command line, we can now list our customers. First, more environment variables to export.\n1 2 3 export ADMIN_USER_NAME=supervisor@company export ADMIN_USER_PASSWORD='password123!' export KEYCLOAK_RESOURCE_SET_URL=http://127.0.0.1:28080/auth/realms/multi-customer/authz/protection/resource_set As with well known OpenID configuration, you can introspect well known UMA configuration by going to\nhttp://localhost:28080/auth/realms/multi-customer/.well-known/uma2-configuration/\nWe need an access token for the supervisor user:\n1 2 3 4 export supervisor_access_token=`curl --silent -u customers:${KEYCLOAK_CLIENT_SECRET} \\ -k -d \"grant_type=password\u0026username=${ADMIN_USER_NAME}\u0026password=${ADMIN_USER_PASSWORD}\u0026scope=email profile\" \\ -H \"Content-Type:application/x-www-form-urlencoded\" \\ ${KEYCLOAK_TOKEN_URL} | jq '.access_token' -r` The token received above is technically a protection API token (PAT). PAT is a special OAuth2 access token with a scope defined as uma_protection. 3\nWe can query for available customers like this:\n1 2 3 curl --silent \\ -H \"Authorization: Bearer ${supervisor_access_token}\" \\ ${KEYCLOAK_RESOURCE_SET_URL}?type=urn:customers:resources:customer | jq '.' 1 2 3 4 [ \"715f6cc5-8ca7-44e4-a8ce-924493db76b1\", \"00f34b81-c45b-4e28-b267-45fad4e48b4d\" ] The above request queried the resource set with the type filter set to urn:customers:resources:customer.\nMore about querying the resource set in Keycloak Authorization Services Guide, Managing Resources.\nLet’s check if our filter is working:\n1 2 3 curl --silent \\ -H \"Authorization: Bearer ${supervisor_access_token}\" \\ ${KEYCLOAK_RESOURCE_SET_URL}?type=urn:customers:resources:kitten | jq '.' 1 [] It is working! Let’s focus on the first response:\n1 2 3 4 [ \"715f6cc5-8ca7-44e4-a8ce-924493db76b1\", \"00f34b81-c45b-4e28-b267-45fad4e48b4d\" ] We can query each individual resource:\n1 2 3 curl --silent \\ -H \"Authorization: Bearer ${supervisor_access_token}\" \\ ${KEYCLOAK_RESOURCE_SET_URL}/715f6cc5-8ca7-44e4-a8ce-924493db76b1 | jq '.' Gives us:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 { \"name\": \"CustomerA\", \"type\": \"urn:customers:resources:customer\", \"owner\": { \"id\": \"95027cdf-4044-4622-bfdb-19ba8f7db65c\" }, \"ownerManagedAccess\": false, \"displayName\": \"Customer A Resource\", \"attributes\": {}, \"_id\": \"715f6cc5-8ca7-44e4-a8ce-924493db76b1\", \"uris\": [ \"/customers/CustomerA\" ], \"resource_scopes\": [ { \"name\": \"customer-a\" } ], \"scopes\": [ { \"name\": \"customer-a\" } ] } and:\n1 2 3 curl --silent \\ -H \"Authorization: Bearer ${supervisor_access_token}\" \\ ${KEYCLOAK_RESOURCE_SET_URL}/00f34b81-c45b-4e28-b267-45fad4e48b4d | jq '.' results in:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 { \"name\": \"CustomerB\", \"type\": \"urn:customers:resources:customer\", \"owner\": { \"id\": \"95027cdf-4044-4622-bfdb-19ba8f7db65c\" }, \"ownerManagedAccess\": false, \"displayName\": \"Customer B Resource\", \"attributes\": {}, \"_id\": \"00f34b81-c45b-4e28-b267-45fad4e48b4d\", \"uris\": [ \"/customers/CustomerB\" ], \"resource_scopes\": [ { \"name\": \"customer-b\" } ], \"scopes\": [ { \"name\": \"customer-b\" } ] } We could now easily create an application to find and return all available Customers. All with Keycloak and without querying any database directly.\nconclusion Keycloak is a very versatile tool and can be easily used as a single source of truth for authentication, single sign-on and authorization within an organization. This article only touches a tip of an iceberg but it presents to the reader a real-world, useful scenario of using Keycloak as a driver for multi-tenant single sign-on.\nfurther reading Authorization Services Guide User Managed Access Managing Resources User-Managed Access (UMA) 2.0 Grant for OAuth 2.0 Authorization Keycloak with Docker Compose ↩︎\nhttps://en.wikipedia.org/wiki/User-Managed_Access ↩︎\nWhat is a PAT and how to obtain it ↩︎\n","description":"Investigating Keycloak Authorization Services using a real-world back office application scenario","tags":["keycloak","iam","multi-tenant","sso","uma"],"title":"Introduction to Keycloak Authorization Services","uri":"/posts/2020-09-05-introduction-to-keycloak-authorization-services/"},{"content":"managing director / software engineer @ Klarrio GmbH Currently employed at at Klarrio GmbH.\nSoftware engineer by trade. With over twenty years of experience specializing in distributed and back end systems: R\u0026D, development and operations. No stranger to the DevOps and CI/CD world. Author of open source tools.\nComputer polyglot who delivered production systems in multiple technologies, including: Scala, Erlang, golang, Java, Ruby, Python and JavaScript deployed in Amazon Web Services, Google Cloud Platform, OpenStack and SoftLayer. Advocate of functional programming. Combines technology and business oriented skills. Striving to deliver high quality work and automate as much as possible. Working with fully remote and on-site teams across different time zones.\npersonal ventures Self employment / Freiberuf Steuer-Nr.: 202/5134/2825\nUSt-IdNr.: DE306381795\n","description":"","tags":null,"title":"About Radek Gruchalski","uri":"/about/"},{"content":"","description":"","tags":null,"title":"idp","uri":"/tags/idp/"},{"content":"Updated on 15th of May 2021 for Keycloak 13.0.0 with Postgres 13.2.\n6th of June 2021: Follow up: setting up Keycloak with TLS for local development.\nKeycloak is an open source Identity and Access Management System developed as a JBoss community project under the stewardship of Red Hat. Keycloak makes it is easy to secure apps and services written in many technologies using a large number client libraries.\nOut of the box, we get things like Single Sign-On, Identity Brokering and Social Login, User Federation and Authorization Services.\nWith little to no code, we can give users of our apps the ability to sign in with Identity Providers like GitHub, Twitter or Google. Well, anything that’s capable of talking OpenID or SAML. On the other hand, we can easily connect to existing LDAP or Active Directory servers to integrate with corporate services of this world.\nHere, I’m going to show how can we launch and configure a local Keycloak server to play with. The only dependency is docker with docker-compose.\nDocker Compose Docker Hub contains prebuilt Keycloak images. The version deployed here is 13.0.0.\nLet’s start with the compose.yml file.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 version: '3.9' services: postgres: image: postgres:13.2 restart: unless-stopped environment: POSTGRES_DB: ${POSTGRESQL_DB} POSTGRES_USER: ${POSTGRESQL_USER} POSTGRES_PASSWORD: ${POSTGRESQL_PASS} networks: - local-keycloak keycloak: depends_on: - postgres container_name: local_keycloak environment: DB_VENDOR: postgres DB_ADDR: postgres DB_DATABASE: ${POSTGRESQL_DB} DB_USER: ${POSTGRESQL_USER} DB_PASSWORD: ${POSTGRESQL_PASS} image: jboss/keycloak:${KEYCLOAK_VERSION} ports: - \"28080:8080\" restart: unless-stopped networks: - local-keycloak networks: local-keycloak: Next to the compose.yml file, we need the .env file.\nKEYCLOAK_VERSION=13.0.0 PORT_KEYCLOAK=8080 POSTGRESQL_USER=keycloak POSTGRESQL_PASS=keycloak POSTGRESQL_DB=keycloak That’s it, we can now start our Keycloak:\ndocker-compose -f compose.yml up After a short moment, we can go to Keycloak landing page using the local address http://localhost:28080. Keycloak will welcome us with this page:\nIntermission Before we move on, let’s quickly figure out what has happened so far.\nWith Docker Compose, we have started a Keycloak server with PostgreSQL 13.2 as a database. PostgreSQL protocol is at a stable version 3 since PostgeSQL 8 days so it does not really matter which newer version is used. Using the .env file, we have specified that we want Keycloak 13.0.0 and our Keycloak shall connect to Postgres using keycloak username and keycloak password as the credential. The database used by Keycloak is also called keycloak. The same variables are used for the postgres service. The Postgres container will automagically create a user identified by POSTGRES_USER variable, authenticated by the value of POSTGRES_PASSWORD. We have specified POSTGRES_DB so the container created the database and configured access for our new user. Both containers run in the same bridge network called local-keycloak. Actually, in Docker it’s called something else: [rad] dev-keycloak $ docker network ls | grep keycloak 920dd184892c dev-keycloak_local-keycloak bridge local The name of the network is essentially:\nbasename $(pwd) + \u003cname-from-compose.yml\u003e We have exposed the 28080 port to the host so we can reach Keycloak from the browser. Finally, we named the Keycloak container as local_keycloak. We will use this name shortly. Administrator account Okay, so Keycloak is running but we can’t do anything with it because we need to create an Administrator account. That we can also do with Docker.\nWhile the compose setup is running, run this in your terminal:\ndocker exec local_keycloak \\ /opt/jboss/keycloak/bin/add-user-keycloak.sh \\ -u admin \\ -p admin \\ \u0026\u0026 docker restart local_keycloak Once this command finishes, you will see that the compose local_keycloak is going to restart. Give it a short moment and reload the Keycloak landing page.\nClick Administration Console link and sign in with admin as the username and admin as the password. Welcome to Keycloak.\nFurther reading Keycloak Documentation Janua Technical Blog Keycloak source code can be found on GitHub.\n","description":"Start and configure Keycloak with Docker Compose","tags":["keycloak","iam","idp","sso","docker"],"title":"Keycloak With Docker Compose","uri":"/posts/2020-09-03-keycloak-with-docker-compose/"},{"content":"","description":"","tags":null,"title":"central","uri":"/tags/central/"},{"content":"","description":"","tags":null,"title":"maven","uri":"/tags/maven/"},{"content":"First, create an account on the Sonatype JIRA, unless you have one. For the new group ID, create a ticket using the form under this URI. Once requested, wait for the ticket to go into Resolved state. When this happens, you can publish your project to Sonatype.\nTo do so, configure your SBT project. Add the following lines to your project/plugins.sbt:\n1 2 addSbtPlugin(\"org.xerial.sbt\" % \"sbt-sonatype\" % \"1.1\") addSbtPlugin(\"com.jsuereth\" % \"sbt-pgp\" % \"1.0.0\") Configure Sonatype credentials for sbt-sonatype, create the ~/.sbt/0.13/sonatype.sbt file with the contents like:\n1 2 3 4 credentials += Credentials(\"Sonatype Nexus Repository Manager\", \"oss.sonatype.org\", \"your-username\", \"your-sonatype-password\") Add the following to your build.sbt:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 organization := \"uk.co.appministry\" // set to whatever you requested as an organization in the form publishMavenStyle := true pomIncludeRepository := { _ =\u003e false } pomExtra := ( \u003curl\u003ehttps://github.com/AppMinistry/scathon\u003c/url\u003e \u003clicenses\u003e \u003clicense\u003e \u003cname\u003eApache 2\u003c/name\u003e \u003curl\u003ehttp://www.apache.org/licenses/LICENSE-2.0.txt\u003c/url\u003e \u003c/license\u003e \u003c/licenses\u003e \u003cscm\u003e \u003cconnection\u003escm:git:git://github.com/AppMinistry/scathon.git\u003c/connection\u003e \u003cdeveloperConnection\u003escm:git:git://github.com/AppMinistry/scathon.git\u003c/developerConnection\u003e \u003curl\u003ehttps://github.com/AppMinistry/scathon/tree/master\u003c/url\u003e \u003c/scm\u003e \u003cdevelopers\u003e \u003cdeveloper\u003e \u003cid\u003eradekg\u003c/id\u003e \u003cname\u003eRadoslaw Gruchalski\u003c/name\u003e \u003curl\u003ehttp://gruchalski.com\u003c/url\u003e \u003c/developer\u003e \u003c/developers\u003e ) Obviously, make sure you provide the details correct to your project: URL, license details, SCM details, developers data and so on. For the comprehensive explanation, look here.\nIn order to publish to Sonatype, we need gnupg installed. This is for macOS:\n$ brew install gnupg gnupg2 Generate a key pair, use the defaults, set the expiry to some same value - the recommended value is less than 2 years. Also, consider setting a passphrase!\n$ gpg --gen-key Check the keyid of the newly generated key:\n$ gpg --list-keys /Users/rad/.gnupg/pubring.gpg ----------------------------- pub 2048R/6664767F 2017-01-26 [expires: 2019-01-26] uid [ultimate] Radoslaw Gruchalski \u003cradek@gruchalski.com\u003e sub 2048R/C0C1DF05 2017-01-26 [expires: 2019-01-26] The keyid in the output above is 6664767F. Send this key to the key server used by Sonatype:\n$ gpg --keyserver hkp://pgp.mit.edu --send-keys 6664767F Give it a few minutes to propagate your key across different machines. It’ll get a little bit confusing otherwise…\nFinally, deploy the artifacts to the staging repository:\n$ sbt publishSigned A valid output should be look like this:\n[rad] scathon (master) $ sbt publishSigned [info] Loading global plugins from /Users/rad/.sbt/0.13/plugins [info] Updating {file:/Users/rad/.sbt/0.13/plugins/}global-plugins... [info] Resolving org.fusesource.jansi#jansi;1.4 ... [info] Done updating. [info] Loading project definition from /Users/rad/dev/my/scathon/project [info] Set current project to scathon (in build file:/Users/rad/dev/my/scathon/) [info] Packaging /Users/rad/dev/my/scathon/scathon-client/target/scala-2.11/scathon-client_2.11-0.2.1-sources.jar ... [info] Packaging /Users/rad/dev/my/scathon/scathon-models/target/scala-2.11/scathon-models_2.11-0.2.1-sources.jar ... [info] Packaging /Users/rad/dev/my/scathon/scathon-testServer/target/scala-2.11/scathon-testserver_2.11-0.2.1-sources.jar ... [info] Done packaging. [info] Done packaging. [info] Done packaging. [info] Wrote /Users/rad/dev/my/scathon/scathon-models/target/scala-2.11/scathon-models_2.11-0.2.1.pom [info] :: delivering :: uk.co.appministry#scathon_2.11;0.2.1 :: 0.2.1 :: release :: Mon Jan 30 23:34:00 GMT 2017 [info] delivering ivy file to /Users/rad/dev/my/scathon/target/scala-2.11/ivy-0.2.1.xml [info] :: delivering :: uk.co.appministry#scathon-models_2.11;0.2.1 :: 0.2.1 :: release :: Mon Jan 30 23:34:00 GMT 2017 [info] delivering ivy file to /Users/rad/dev/my/scathon/scathon-models/target/scala-2.11/ivy-0.2.1.xml [info] Wrote /Users/rad/dev/my/scathon/scathon-testServer/target/scala-2.11/scathon-testserver_2.11-0.2.1.pom [info] Wrote /Users/rad/dev/my/scathon/scathon-client/target/scala-2.11/scathon-client_2.11-0.2.1.pom [info] Main Scala API documentation to /Users/rad/dev/my/scathon/scathon-models/target/scala-2.11/api... [info] :: delivering :: uk.co.appministry#scathon-testserver_2.11;0.2.1 :: 0.2.1 :: release :: Mon Jan 30 23:34:00 GMT 2017 [info] delivering ivy file to /Users/rad/dev/my/scathon/scathon-testServer/target/scala-2.11/ivy-0.2.1.xml [info] :: delivering :: uk.co.appministry#scathon-client_2.11;0.2.1 :: 0.2.1 :: release :: Mon Jan 30 23:34:00 GMT 2017 [info] delivering ivy file to /Users/rad/dev/my/scathon/scathon-client/target/scala-2.11/ivy-0.2.1.xml [info] Main Scala API documentation to /Users/rad/dev/my/scathon/scathon-testServer/target/scala-2.11/api... [info] Packaging /Users/rad/dev/my/scathon/scathon-models/target/scala-2.11/scathon-models_2.11-0.2.1.jar ... [info] Done packaging. [info] Packaging /Users/rad/dev/my/scathon/scathon-testServer/target/scala-2.11/scathon-testserver_2.11-0.2.1.jar ... [info] Main Scala API documentation to /Users/rad/dev/my/scathon/scathon-client/target/scala-2.11/api... [info] Done packaging. [info] Packaging /Users/rad/dev/my/scathon/scathon-client/target/scala-2.11/scathon-client_2.11-0.2.1.jar ... [info] Done packaging. model contains 16 documentable templates [warn] there was one feature warning; re-run with -feature for details model contains 17 documentable templates [info] Main Scala API documentation successful. [info] Packaging /Users/rad/dev/my/scathon/scathon-client/target/scala-2.11/scathon-client_2.11-0.2.1-javadoc.jar ... [info] Done packaging. Please enter PGP passphrase (or ENTER to abort): *************************** [warn] there was one feature warning; re-run with -feature for details [warn] one warning found [info] Main Scala API documentation successful. [info] Packaging /Users/rad/dev/my/scathon/scathon-testServer/target/scala-2.11/scathon-testserver_2.11-0.2.1-javadoc.jar ... [info] Done packaging. model contains 157 documentable templates [warn] one warning found [info] Main Scala API documentation successful. [info] Packaging /Users/rad/dev/my/scathon/scathon-models/target/scala-2.11/scathon-models_2.11-0.2.1-javadoc.jar ... [info] Done packaging. [info] published scathon-client_2.11 to https://oss.sonatype.org/service/local/staging/deploy/maven2/uk/co/appministry/scathon-client_2.11/0.2.1/scathon-client_2.11-0.2.1-javadoc.jar [info] published scathon-client_2.11 to https://oss.sonatype.org/service/local/staging/deploy/maven2/uk/co/appministry/scathon-client_2.11/0.2.1/scathon-client_2.11-0.2.1-sources.jar.asc [info] published scathon-client_2.11 to https://oss.sonatype.org/service/local/staging/deploy/maven2/uk/co/appministry/scathon-client_2.11/0.2.1/scathon-client_2.11-0.2.1.jar.asc [info] published scathon-client_2.11 to https://oss.sonatype.org/service/local/staging/deploy/maven2/uk/co/appministry/scathon-client_2.11/0.2.1/scathon-client_2.11-0.2.1-javadoc.jar.asc [info] published scathon-client_2.11 to https://oss.sonatype.org/service/local/staging/deploy/maven2/uk/co/appministry/scathon-client_2.11/0.2.1/scathon-client_2.11-0.2.1-sources.jar [info] published scathon-client_2.11 to https://oss.sonatype.org/service/local/staging/deploy/maven2/uk/co/appministry/scathon-client_2.11/0.2.1/scathon-client_2.11-0.2.1.pom [info] published scathon-client_2.11 to https://oss.sonatype.org/service/local/staging/deploy/maven2/uk/co/appministry/scathon-client_2.11/0.2.1/scathon-client_2.11-0.2.1.pom.asc [info] published scathon-client_2.11 to https://oss.sonatype.org/service/local/staging/deploy/maven2/uk/co/appministry/scathon-client_2.11/0.2.1/scathon-client_2.11-0.2.1.jar [info] published scathon-models_2.11 to https://oss.sonatype.org/service/local/staging/deploy/maven2/uk/co/appministry/scathon-models_2.11/0.2.1/scathon-models_2.11-0.2.1.pom.asc [info] published scathon-models_2.11 to https://oss.sonatype.org/service/local/staging/deploy/maven2/uk/co/appministry/scathon-models_2.11/0.2.1/scathon-models_2.11-0.2.1-sources.jar.asc [info] published scathon-models_2.11 to https://oss.sonatype.org/service/local/staging/deploy/maven2/uk/co/appministry/scathon-models_2.11/0.2.1/scathon-models_2.11-0.2.1-javadoc.jar.asc [info] published scathon-models_2.11 to https://oss.sonatype.org/service/local/staging/deploy/maven2/uk/co/appministry/scathon-models_2.11/0.2.1/scathon-models_2.11-0.2.1.jar.asc [info] published scathon-models_2.11 to https://oss.sonatype.org/service/local/staging/deploy/maven2/uk/co/appministry/scathon-models_2.11/0.2.1/scathon-models_2.11-0.2.1-javadoc.jar [info] published scathon-models_2.11 to https://oss.sonatype.org/service/local/staging/deploy/maven2/uk/co/appministry/scathon-models_2.11/0.2.1/scathon-models_2.11-0.2.1.jar [info] published scathon-models_2.11 to https://oss.sonatype.org/service/local/staging/deploy/maven2/uk/co/appministry/scathon-models_2.11/0.2.1/scathon-models_2.11-0.2.1.pom [info] published scathon-models_2.11 to https://oss.sonatype.org/service/local/staging/deploy/maven2/uk/co/appministry/scathon-models_2.11/0.2.1/scathon-models_2.11-0.2.1-sources.jar [info] published scathon-testserver_2.11 to https://oss.sonatype.org/service/local/staging/deploy/maven2/uk/co/appministry/scathon-testserver_2.11/0.2.1/scathon-testserver_2.11-0.2.1-javadoc.jar [info] published scathon-testserver_2.11 to https://oss.sonatype.org/service/local/staging/deploy/maven2/uk/co/appministry/scathon-testserver_2.11/0.2.1/scathon-testserver_2.11-0.2.1-sources.jar [info] published scathon-testserver_2.11 to https://oss.sonatype.org/service/local/staging/deploy/maven2/uk/co/appministry/scathon-testserver_2.11/0.2.1/scathon-testserver_2.11-0.2.1.pom [info] published scathon-testserver_2.11 to https://oss.sonatype.org/service/local/staging/deploy/maven2/uk/co/appministry/scathon-testserver_2.11/0.2.1/scathon-testserver_2.11-0.2.1.jar [info] published scathon-testserver_2.11 to https://oss.sonatype.org/service/local/staging/deploy/maven2/uk/co/appministry/scathon-testserver_2.11/0.2.1/scathon-testserver_2.11-0.2.1-sources.jar.asc [info] published scathon-testserver_2.11 to https://oss.sonatype.org/service/local/staging/deploy/maven2/uk/co/appministry/scathon-testserver_2.11/0.2.1/scathon-testserver_2.11-0.2.1.jar.asc [info] published scathon-testserver_2.11 to https://oss.sonatype.org/service/local/staging/deploy/maven2/uk/co/appministry/scathon-testserver_2.11/0.2.1/scathon-testserver_2.11-0.2.1.pom.asc [info] published scathon-testserver_2.11 to https://oss.sonatype.org/service/local/staging/deploy/maven2/uk/co/appministry/scathon-testserver_2.11/0.2.1/scathon-testserver_2.11-0.2.1-javadoc.jar.asc [success] Total time: 35 s, completed Jan 30, 2017 11:34:35 PM If I was to publish this project for Scala 2.12, I’d do the following:\nSCALA_VERSION=2.12.1 $ sbt ++$SCALA_VERSION -Dscala.version=$SCALA_VERSION publishSigned And release:\n$ sbt sonatypeRelease The output should look like:\n$ sbt sonatypeRelease [info] Loading global plugins from /Users/rad/.sbt/0.13/plugins [info] Loading project definition from /Users/rad/dev/my/scathon/project [info] Set current project to scathon (in build file:/Users/rad/dev/my/scathon/) [info] Nexus repository URL: https://oss.sonatype.org/service/local [info] sonatypeProfileName = uk.co.appministry [info] Reading staging repository profiles... [info] Reading staging profiles... [info] Closing staging repository [ukcoappministry-1008] status:open, profile:uk.co.appministry(439d1bb6b3d35e) description: Implicitly created (auto staging). [info] Activity open started:2017-01-30T23:34:14.860Z, stopped:2017-01-30T23:34:18.364Z [info] repositoryCreated: id:ukcoappministry-1008, user:radekg, ip:92.79.39.101 [info] Activity close started:2017-01-30T23:38:12.624Z, stopped: [info] Evaluate: id:5e9e8e6f8d20a3, rule:sources-staging [info] Evaluate: checksum-staging [info] Passed: checksum-staging [info] Evaluate: signature-staging [info] Passed: signature-staging [info] Evaluate: sources-staging [info] Passed: sources-staging [info] Evaluate: javadoc-staging [info] Passed: javadoc-staging [info] Evaluate: pom-staging [info] Passed: pom-staging [info] Passed: id:5e9e8e6f8d20a3 [info] email: to:radek@gruchalski.com [info] repositoryClosed: id:ukcoappministry-1008 [info] Closed successfully [info] Promoting staging repository [ukcoappministry-1008] status:closed, profile:uk.co.appministry(439d1bb6b3d35e) description: Implicitly created (auto staging). [info] Activity release started:2017-01-30T23:38:26.336Z, stopped: [info] Evaluate: id:5e9e8e6f8d20a3, rule:sources-staging [info] Evaluate: sources-staging [info] Passed: sources-staging [info] Evaluate: javadoc-staging [info] Passed: javadoc-staging [info] Evaluate: signature-staging [info] Passed: signature-staging [info] Evaluate: pom-staging [info] Passed: pom-staging [info] Evaluate: checksum-staging [info] Passed: checksum-staging [info] Passed: id:5e9e8e6f8d20a3 [info] Evaluate: id:nx-internal-ruleset, rule:RepositoryWritePolicy [info] Evaluate: RepositoryWritePolicy [info] Passed: RepositoryWritePolicy [info] Passed: id:nx-internal-ruleset [info] copyItems: source:ukcoappministry-1008, target:releases [info] email: to:radek@gruchalski.com [info] repositoryReleased: id:ukcoappministry-1008, target:releases [info] Promoted successfully [info] Dropping staging repository [ukcoappministry-1008] status:released, profile:uk.co.appministry(439d1bb6b3d35e) description: Implicitly created (auto staging). [info] Dropped successfully: ukcoappministry-1008 [info] Set current project to scathon (in build file:/Users/rad/dev/my/scathon/) If this was a first release with a given organization, go back to your Sonatype JIRA ticket and let them know you’ve released your first version. The Maven Central sync will be enabled for your organization. It might take a couple of hours before you see your changes in Central.\nThat’s about it.\nSome interesting links:\nOSSRH Guide Sonatype Requirements Cake Solutions Publishing SBT projects to Nexus sbt-sonatype on Github: have a look at the readme ","description":"Publish to Maven Central via Sonatype with SBT","tags":["sbt","maven","central","sonatype"],"title":"Publish to Maven Central via Sonatype with SBT","uri":"/posts/2017-01-31-publish-to-maven-central-via-sonatype-with-sbt/"},{"content":"","description":"","tags":null,"title":"sbt","uri":"/tags/sbt/"},{"content":"","description":"","tags":null,"title":"sonatype","uri":"/tags/sonatype/"},{"content":"","description":"","tags":null,"title":"apache","uri":"/tags/apache/"},{"content":"It is entirely possible that what I am going to describe here is an edge case not many people hit with their Kafka deployments. However, in my experience, when Kafka is used to ingest large volumes of data, it makes perfect sense. Considering that every now and then people ask for a cold storage feature on the Kafka mailing list, I am not the only one who would find this useful.\nApache Kafka According to the Kafka website: Apache Kafka is publish-subscribe messaging rethought as a distributed commit log. It is fast: it can handle hundreds of megabytes of data from thousands of clients. It is durable: Kafka persists the data on disk and provides data replication to ensure no data loss. It is scalable: Kafka systems can be scaled without a hassle.\nTo understand where the need for cold storage comes from, it is important to understand how Kafka system works and what trade-offs a Kafka based system needs to deal with.\nKafka stores all data on disk. The servers running Kafka are limited by the amount of storage they can handle. It is always an arbitrary number defined by the amount of storage available to the single Kafka server. Single Kafka node within the cluster may be responsible for one or more topics. Each topic is stored in files called segments. All segments of a topic have an index file describing which parts of the queue reside in which segment. The segment size is configurable. The total amount of data kept for a topic is either the cumulative size of the topic or the oldest message to be kept. For example, a value of two weeks or 500GB of data; depending on which is reached first, that value will be used to determine how much data to keep. If a segment size for such topic is 1GB, the maximum of 500 of segments will be kept.\nThe process presented above applies to a regular topic. This kind of topic stores all the data in sequence. Kafka also offers compacted topics. These are more like key / value store.\nThe process of cleaning up old segments is called log compaction. In case of regular topics, log compaction will remove all segments falling out of range of data to be kept. In case of compacted topics, only the most recent value for a given key is kept.\nThe cold storage case applies only to regular topics.\nThe problem The problem with regular topics, as they are implemented today in Kafka, is that, it is not possible to move excess segments out from under Kafka management without restarting Kafka broker.\nConsider a data processing system using Kafka ingesting hundreds of megabytes of data every second. Such system will have a certain number of servers, call them terminators, used by millions of devices / applications as a connection point. These servers accept the data, they don’t do any processing, just put the data in Kafka. The volume of data is so large that the system needs a certain trade-off: how much data should be kept in Kafka and what to do with historical data? Historical data is important. It needs to be stored somewhere in a format ready for post-processing in case of, for example, having to introduce a new feature during the life-time of a product.\nToday, most of the implementations solve this problem in the following way: the raw data is ingested with Kafka. Some processes work with that data by reading Kafka topics and crunching for real-time dashboards and whatnot. There is also a process somewhere which exists only for the purpose of reading all ingested data out of Kafka and putting it in the raw format in external storage. S3 or Swift Object Store come to mind.\nThere are two drawbacks of such solution. First: the storage of the raw data is basically another format. It needs naming rules to know how to access the data in the future, compression, transport mechanism, verification, replication. Second: the data is already in Kafka, so why the need for consuming it out for Kafka, putting the load on the system and using additional processing power for applying compression and moving the data out to external storage?\nCold storage Kafka comes with a feature which allows it to build the index file from an arbitrary segment file. What this means is: instead of consuming the topic and uploading raw data to external storage, it is possible to move a segment file as it is persisted on disk. The data in the topic is still stored in its raw format, albeit inside of a segment file. The advantage of storing segments files is such that there is no additional cost of consuming for cold storage purpose and no cost in feeding back to Kafka for reprocessing. One can simply download the segment file and use Kafka libraries—no need to run Kafka cluster at all—to read data out. Such segment files can be processed in parallel with Storm, Spark, Hadoop, or any other sufficient tool.\nImplementation Nothing stops anyone from doing this today. The simplest way is to have a program running on the Kafka machine which would check when the old segments are closed, copy them to external storage and let Kafka simply deal with old segments as it does now. This is, however, another one of those “roll it out yourself” approaches. Kafka could help. There are two possible options I can think of.\nFirst approach: if Kafka provided a notification mechanism and could trigger a program when a segment file is to be discarded, it would become feasible to provide a standard method of moving data to cold storage in reaction to those events. Once the program finishes backing the segments up, it could tell Kafka “it is now safe to delete these segments”.\nThe second option is to provide an additional value for the log.cleanup.policy setting, call it cold-storage. In case of this value, Kafka would move the segment files—which otherwise would be deleted—to another destination on the server. They can be picked up from there and moved to the cold storage.\nBoth of these options ensure that Kafka data can be archived without having to interfere with Kafka process itself. Considering data replication features in Kafka, the former method seems more plausible. It would free the cluster operator from having to track file system changes. Furthermore, it could be implemented in such a way that, if there are any listeners waiting for those events for a given topic, Kafka switches to this mode automatically. It does put a responsibility on the operator to ensure flawless execution of the archive process but it is an opt-in mechanism—the operator is aware of the behavior.\nConclusion A well defined, standardized method for moving Kafka segments to the cold storage would significantly improve the availability and processing of historical data.\nNotes This article has been published on Medium and LinkedIn. 1 2\nMedium ↩︎\nLinkedIn ↩︎\n","description":"Investigation of available options for running and operating a Certificate Authority for modern applications","tags":["apache","kafka"],"title":"The case for Kafka cold storage","uri":"/posts/2016-05-08-the-case-for-kafka-cold-storage/"},{"content":"About two weeks ago, Virdata released a set of patches for Apache Spark enabling Spark to work on Mesos with Docker bridge networking. We are using these in production for our multi tenant Spark environment.\nSPARK-11638: Spark patches All patches for all components described below are available in Spark JIRA. We’ve released patches for all versions of Spark available at the time of creating them - from 1.4.0 to 1.5.2. We have also released patches for Akka 2.3.4 and Akka 2.3.11; these are required to make this solution usable.\nWhat has not been released with the patches is the complete minimum example of how these can be verified. This post serves as a missing part.\nAll instructions on how to build Spark and Akka with patches applied can be found at the end of this post.\nScenario and sources The scenario: setting up Spark Notebook running in Docker container, bridge networking on Mesos. Spark notebook, once a document is opened, starts Apache Spark Master process. Executing Spark code, schedules tasks on the same Mesos the notebook server is running on.\nThe observed outcome: executors communicate back to the master, inside of the Docker container.\nAll programs discussed below are available on github. Let’s go through a short summary of what is happening…\nSetting up the machine For the simplicity, I assume clean Ubuntu 14.04 installation. These instructions should work without any issues on EC2, SoftLayer or any other cloud provider.\nMy VM is a 4 CPUs, 8GB RAM VMWare Fusion machine.\nFirst, the machine has to be set up:\ncheckout the repository and put them in ~ on your test machine (cd ~ \u0026\u0026 git clone https://github.com/virdata/mesos-spark-docker-bridge.git .) build relevant Spark version with the patch from Spark JIRA (all examples assume 1.5.1) build relevant Akka version for your Spark version (in case of this example it always 2.3.11, spark-notebook excludes any other Akka and assumes 2.3.11) put the akka-remote_2.10-2.3.11.jar, result of the build, in ~ put spark-core_2.10-1.5.1.jar and spark-repl_2.10-1.5.1.jar, result of the build, in ~ execute setup.sh The most important things to know:\nOpenJDK 7 will be used Mesosphere apt repo will be added Mesos 0.24.1 with Marathon 0.10.1 is used Mesos agent runs on the same machine as Mesos master latest docker is installed local installation of ZooKeeper is used /etc/mesos.agent.sh file is created, explained below Spark stock release is downloaded and stored on the machine to be used for executors patched Akka and spark JARs required only for the master, executors can happily use the stock release /etc/mesos.agent.sh The container has to know what is the IP address of the Mesos agent the container’s task is running on. Mesos, nor Marathon, currently does not provide this information. When a task is scheduled, there is no way to know which agent the task is going to end up on until the task is running. This make it impossible to give the agent’s IP at task submission time. /etc/mesos.agent.sh is given to the container as a Docker volume. This file needs to exist on every agent node. In this example, it is not really necessary. The master and agent are the same node. The file is provided for clarity and makes it possible to use multi node setup.\nThis step takes takes about 15 minutes on my VMWare Fusion VM. When finished, Mesos UI and Marathon UI should be reachable:\nMesos: http://mesos:5050 Marathon: http://mesos:8080 mesos host may have to be added /etc/hosts file.\nSpark notebook Docker image In theory, a snapshot version of Data Fellas notebook should work but, if there are any doubts, build-docker-image.sh program should be used to get an upstream version of the Docker image.\nThe most important things to know:\ngit master version of the code is used base image changed to ubuntu:trusty and entrypoint changed so it is possible to run the container with docker -ti ... /bin/bash This will take about half an hour to build. The andypetrella/spark-notebook:0.6.2-SNAPSHOT-scala-2.10.4-spark-x.x.x-hadoop-2.4.0 image should be on the list after executing docker images command.\nRun ~/run-task.sh This will request a Mesos task with Marathon. There are 6 ports being requested, the order:\n9000: Spark notebook server user interface 9050: LIBPROCESS_PORT 6666: spark.driver.port 6677: spark.fileserver.port 6688: spark.broadcast.port 6699: spark.replClassServer.port The command used in the container is $MESOS_SANBOX/run-notebook.sh. What happens inside is the following:\nsource /etc/mesos.agent.sh to learn the IP address of the host agent set LIBPROCESS_* properties, the advetise IP is set to the host IP and advertise port is $PORT_9050 (the port assigned by Mesos); Mesos will now correctly return offers to the container SPARK_LOCAL_IP is set to the container’s IP address SPARK_PUBLIC_DNS and SPARK_LOCAL_HOSTNAME are set to the hostname of the agent next, agent’s hostname is added to the container’s /etc/hosts file; this makes it possible for Spark to bind all services to the agent hostname so it can be correctly advetised to the executors; for the spark.driver.port / spark.driver.advertisedPort, the akka patch is required CLASSPATH_OVERRIDES contains 3 JARs: patched akka-remote, patched spark-core and spark-repl; this env variable was added to spark-notebook just to make these patches work; the most important thing to know: these 3 JARs have to be on the class path before any other Spark / Akka jars; if loaded first, the JVM will use them over any other classes from any other JARs side note: Zeppelin has recently added support for ZEPPELIN_CLASSPATH_OVERRIDES environment variable The final step is starting the notebook server. Worth noticing are all settings ending with advertisedPort. This is the main part of the Spark patch.\nTo verify Create a new notebook and execute the following:\nval rdd = sparkContext.parallelize(Seq(1,2)) rdd.count() Rather simple but… The executors will be scheduled on the same Mesos cluster the notebook server is running on. The executors will successfully publish the results bask to the master running in the Docker container.\nWith the fine-grained mode, it sometimes happes that the tasks are stuck in STAGING for a long time and the whole job fails. If this is the case during the test, repeat the test with:\nspark.mesos.coarse = true using notebook metadata. It is important to remember that notebooks saved on one server carry over the Spark configuration in them. As such, it is not safe to import a notebook from another notebook server to execute the test. Please use a new, clean notebook.\nPerfomance-wise, no real numbers to share but we are getting within ~95% of the usual setup with coarse mode.\nComponents Patched Akka AKKA_VERSION=2.3.11 git clone https://github.com/akka/akka.git cd akka git fetch origin git checkout v${AKKA_VERSION} git apply path/to/${AKKA_VERSION}.patch sbt package -Dakka.scaladoc.diagrams=false Patched Spark SPARK_VERSION=1.5.1 git clone github.com/apache/spark.git cd spark git fetch origin git checkout v{$SPARK_VERSION} # from the tag git apply path/to/${SPARK_VERSION}.patch ./make-distribution ... Akka versions 2.3.4: Spark 1.4.0 Spark 1.4.1 2.3.11 Spark 1.5.0 Spark 1.5.1 Spark 1.5.2 Comments? Thougths? Thoughts and comments are appreciated! Please use github issues for communication.\nHappy coding!\n","description":"Apache Spark on Mesos with Docker bridge networking","tags":["spark","mesos","marathon","docker"],"title":"Apache Spark on Mesos with Docker bridge networking","uri":"/posts/2015-11-23-apache-spark-on-mesos-with-docker-bridge-networking/"},{"content":"","description":"","tags":null,"title":"marathon","uri":"/tags/marathon/"},{"content":"","description":"","tags":null,"title":"spark","uri":"/tags/spark/"},{"content":"","description":"","tags":null,"title":"euc2015","uri":"/tags/euc2015/"},{"content":"","description":"","tags":null,"title":"gossiperl","uri":"/tags/gossiperl/"},{"content":"Wow. It’s difficult to believe it’s been almost a week since I gave a talk about gossip protocols at Erlang User Conference in Stockholm. It was a fantastic event, great agenda, great topics, fantastic networking. EUC is one of those events you should attend, you will not regret it. No matter if you are interested in Erlang/Elixir or not. Big “Thank You” to all who made it happen.\nEUC2015 was also a place of first public appearance of Gossiperl. This was the first time I was giving a public talk to such an audience, I’m not the best at this. Judging by the number of people attending my talk, the subject was good. Judging by the number of people who were there when I was wrapping up, I wasn’t terrible. I do hope that people got the value out of my 45 minutes. And finally, I was able to show Gossiperl.\nAt the end of my talk I’ve presented a couple of very simple demos:\nevent subscribe / dissemination: events arriving only at interested nodes Gossiperl running over multicast on Raspberry Pi cluster These are the very basic Gossiperl services covered in the documentation and the core of what Gossiperl is about. The core is ready. So, what are the next steps?\nTop priority: Raft, in order to make Gossiperl fully data center aware. The idea behind this one is to make the data centers (racks in Gossiperl notion) exchange the data between the physical locations only via seeds. This will ensure that the physical locations communicate via established, dedicated, streamlined channel. Currently, all members from different locations communicate with each other. This isn’t very effective because of the latency. As explained in my talk, it is not desired to have overlay members in, say, Oregon talk to members, say, in Singapore. The latency is expected but it should be controllable. This will be achieved with Raft. Only the seeds of those locations will talk to each other. The members of respective locations will talk only to local seeds.\nPriority number two: subscriptions. Gossiperl subscription message redelivery can be configured in two different ways. Either the delivery will be attempted selected number of times or indefinitely. In case of a fixed number, if a message can’t be delivered, the sender will never become aware of this fact. This problem will be addressed by solving the following issue.\nNumber three: subscriptions, again. Well, kind of. Ability to attach a luggage to the digest. Such information will be used to build an initial overview of the overlay with application specific data. Similar to membership but domain specific. Subscriptions are available only to members who already participate in an overlay. Any new joiners miss on this data and will never receive it. It’s possible to build a mechanism to resend such information but it is not straightforward. For data which should be available to all new joiners luggage is the answer.\nNext one is exometer. Exometer is de facto the standard for metrics in Erlang world. Like JMX for JVM. Currently only InfluxDB connector is available out of the box and exometer needs to be supported.\nAfter that comes an easy one. The only encryption algorithm available (and hard coded) is AES-256. This will become a configurable option with pluggable behaviours. Gossiperl will ship with configuration for AES-128, AES-192 and AES-256. Other algorithms can still be added and selected on per overlay basis.\nThe final one is: user interface improvements. Gossiperl ships with a REST API. Next to the REST API a web interface will be available. All management operations must be accessible through the browser and statistics will be streamed via web sockets.\nHere we are. I’m sure there will be more.\nThis post would not be complete without words of appreciation to my managers at Virdata who supported me in this endeavour. You know who you are, thank you.\n","description":"","tags":["gossiperl","euc2015"],"title":"Gossiperl at EUC 2015 and next steps","uri":"/posts/2015-06-17-gossiperl-at-euc-2015-and-next-steps/"},{"content":"A little moan to start with… I owned a mid 2009 MacBook Pro, I never used it for presenting stuff to others but I actually bought a remote control for it. The computer cost me a lot of money, it was top end stuff when it was bought. The remote control cost me the money as well. I also have a copy of Keynote. It also cost me money. A while ago I’ve switched to a new MacBook, the Retina one, top end as well. It cost even more money than the first one.\nAs it turns out, next month I will be speaking at a conference and I thought it would be great if I could use the remote control to switch between the slides. Right? Well, except that the new, very expensive computer, doesn’t have a built in IR receiver. There goes an idea of using the remote. But hold on, there must an application to control Keynote presentation using a phone, surely. Emmm… there was. It was called Keynote remote. But Apple pulled it from Appstore and merged the functionality with Keynote for iOS. Keynote for iOS costs money. Now, I could’ve just bite the bullet and buy the damn thing. But something kicked off inside of me though…\nWhy would I spend another c.a. €8 for a bit of software to flip the slides from my phone?\nDear Apple, I’ve spent so much money on the stuff already:\n2 MBPs roughly €6k together ~€700 on a phone €20 for Keynote I mean, come on, surely you’d at least give me a remote for free. Hell no, not paying for it. It’s not like I can’t afford it, but I’m not paying for it…\nSinatra presenter Here it is. The application is called sinatra-presenter and is now available on github. I’ve built it purely to vent off my frustration…\nAll the instructions on how to use it are available in the readme file.\nQuick rundown through the features:\nlists presentation placed in a given directory (RTFM) selecting a presentation from the list opens it and puts it in slideshow mode starting with the first slide go to the next and previous slide using the buttons on the screen if the presnetation is put in the background (say to switch to the terminal to demo something), clicking next/previous buttons will restore the presentation into slideshow and handle slide change correctly if it happens that the browser interface is reloaded, the state will be restored by introspecting Keynote state Is it free? Yep, it’s free. Available under LGPLv3 license.\nHave fun with it.\n","description":"","tags":["keynote","presenting"],"title":"Control Keynote presentation with your mobile browser","uri":"/posts/2015-05-11-control-keynote-presnetation-with-your-mobile-browser/"},{"content":"","description":"","tags":null,"title":"keynote","uri":"/tags/keynote/"},{"content":"","description":"","tags":null,"title":"presenting","uri":"/tags/presenting/"},{"content":"Unit testing has evolved a long way over the years, continuous integration services even more so. Few years back, when mentioning CI, Jenkins was the only reasonable choice someone would take. It was kind of easy to set up, depending on what one was planning to do with it. Looking at this space today there are plenty of services offering painless CI for the masses. The most known one - Travis CI. Compared to Jenkins it’s night and day. Simply sign in with Github, go to settings, enable repository to be tested and you are off. The build can be customised by modifying .travis.yml file placed in the root of the repository to be tested. Every push to Github, every pull request and every merge is automatically tested. Results are available on Github together with full history of the project. Easy.\nI love Travis CI, like I would most likely love a lot of similar products, Snap CI comes to mind, there are dozens of those out there in the wild. Travis has changed how people think of unit testing / CI. If you are working on an open source project, there’s no reason not to use Travis or similar solution. For open source projects Travis CI is available for free. But there is one problem with such products. Obviously, people who run them are not charities. This is not to criticise, we all write software and would like to make for living somehow. Travis, like any other platform of such kind, has a paid version. The free version is a driver to bring customers to the paid version. The free version has limits. In case of Travis, there are two most obvious. Free version allows public repositories only and there’s a global resource limit available for all users. As the product grows in popularity, testing becomes slower and slower. To the point where it becomes, by far, the slowest process of all, 10 minute wait is not uncommon these days. Please keep in mind, this is not a complaint, it’s great that these products exist and can be used for free. The problem still exists though. There is a lot of great free tools that don’t get any direct financial support and a paid version is not an option. Any kind of payment may be a stretch.\nLet’s have a look at what the modern open source project cycle is. There is a (distributed) version control system. The members (committers) are most likely distributed around the world, they contribute at different times and different pace. Their work is available to the public via central repository. Most often Github, but also Bitbucket and probably more I never heard of. A central testing / CI mechanism is employed, such as Travis, to enable full visibility to everybody involved in development and everyone who wants to use the product. It’s a central source of truth. Any delay hinders the progress.\nIs there a (better) way to do this? A few weeks ago, while talking to one of my colleagues at work, I mentioned an idea of a system which would enable testing / CI of the open source products on a big scale using resources already available to the developers. The purpose is to fulfil the process described above. How could such system work?\nThere would be a central website displaying build / testing progress, like Travis does. This website would be communicating with systems such as Github or Bitbucket, it would receive notifications about pushes, pull requests and merges. It would display every build status and progress. It would report results back to Github, Bitbucket and such. The problem to solve is the resources’ availability to run tests / CI. The website could have a number of virtual machines available for download, say VMWare, VirtualBox, QEMU, KVM and so on. Each of those would be configured to receive a build / test / CI request from the central system, execute it according to the given specification and send the results in real-time back to the central system. People could contribute to the overall processing capacity of such system by simply running these VMs. The more of them out there, the more processing capacity available for everybody. Such VMs could either run Docker images, for fast tests that don’t require root permissions (or sudo as one prefers), or could be used directly to run tests with all the options enabled.\nThere are obvious problems to solve. Security — how to prevent this system being turned into a bot net and ensure that the results are not manipulated with. How to ensure that tests are run in timely fashion, the results are available. How to enable the system to be fault tolerant. Once the build is accepted by a VM worker, it has to be completed. A VM failure could prevent the results from arriving at the central system. The job may have to be replayed somewhere else. The tools are available. It would be in an interest of open source developers to run such VMs, maybe some organisation could donate computing power to run more VMs? There is cost involved, running a “central” system wouldn’t come for free. This would have to be load balanced and replicated across different geo-locations to make it fault tolerant in itself. But it would be significantly cheaper to run than having a fleet of machines on stand-by ready to execute tests for the world.\nJust a thought…\n","description":"","tags":[],"title":"Crowd sourced unit testing / CI","uri":"/posts/2015-04-21-crowd-sourced-unit-testing-ci/"},{"content":"","description":"","tags":null,"title":"gossip","uri":"/tags/gossip/"},{"content":"The gossiperl project is growing. I had a great, productive, working Christmas break, implementing a number of client libraries for the message bus. In the last couple of weeks a number of client libraries for gossiperl have been released. The full list now includes:\nRuby JVM with examples in Clojure and Scala .NET as a mono project Erlang And the most exciting so far — gossiperl client Chrome extension. This one is not complete yet and it will take a little bit longer until it is fully implemented. However, the perspective of running a gossiperl client in the browser — communicating with applications running locally — opens the door for some fantastic opportunities. While working on the Chrome client I also implemented the Thrift BinaryProtocol for JavaScript library. Currently the code is not included in the main Thrift project due to some issues with Thrift unit tests but there’s an ongoing effort to make this code part of Thrift as soon as possible. Right now the code available as a separate github repository.\nThe next steps for gossiperl are:\nprovide unit tests for gossiperl itself and integrate with Travis CI enable unit testing of the client libraries via Travis CI provide end to end tests for gossiperl Afterwards, there will be more clients for other languages:\nPython Node.JS Haskell C++ examples in F# There will also examples of useful end to end things appearing on github!\n","description":"","tags":["gossip","thrift"],"title":"State of gossiperl and some JavaScript Thrift goodies","uri":"/posts/2015-01-05-state-of-gossiperl-and-some-javascript-thrift-goodies/"},{"content":"","description":"","tags":null,"title":"thrift","uri":"/tags/thrift/"},{"content":"","description":"","tags":null,"title":"erlang","uri":"/tags/erlang/"},{"content":"Today I’ve released a project called gossiperl. Gossiperl is a language agnostic gossip middleware written in Erlang. The purpose of the project was purely research on gossip based systems, as well as, learning Erlang.\nMain intent was to create a common communication middleware enabled over gossip using a standard binary transport mechanism. Gossiperl is a result of over 6 months of research, reading, learning, planning and implementation. Gossiperl uses Apache Thrift binary serialisation over UDP. Gossiperl is intended for projects where order of delivery is not important and latency is acceptable. Intended projects require a messaging layer but not necessarily want to implement their own communication stack.\nGossiperl provides distributed subscriptions over gossip via UDP with Thrift digests secured with AES-256.\nGossiperl overlay contains a number of members — could be servers in a cluster. These servers do some common operations and the data has to be shared across the cluster. Every server within the cluster runs a gossiperl daemon. All servers connect over a gossip overlay. Application requiring exchanging the data connect to the local member and subscribe for a certain digest type. Other instances of same application connected to members on other nodes of a cluster (overlay) publish digests of expected type to gossip. The data is being forwarded across the overlay, only to the members that actually require receiving the data. Gossiperl is a presence mechanism and a message bus.\nThe main features of gossiperl:\nmultiple overlays — adding / removing overlays at runtime reconfiguration at runtime presence mechanism distributed subscriptions with “at least once delivery” semantics every overlay secured using separate AES-256 symmetric key / iv combination local subscribers secured using subscriber specific secret language agnostic — uses Apache Thrift for all communication REST API with HTTPS and authentication provided statistics (using InfluxDB) Being a research code and a subject of learning Erlang, gossiperl might not be the highest quality Erlang code in the wild. It is also not intended to be used in production environments. The work on gossiperl is ongoing.\nCurrently only the main codebase is available. Unit tests, end to end tests, client libraries for different languages and deployment / packaging tools will be released soon (it won’t be months). Most of these parts are written but need to be structured for shipping.\nAdditionally, there is development on providing communication across physical locations. This enables gossiperl to operate in a multi datacenter deployment, using RAFT consensus. Every seed of every overlay will elect an overlay leader, the leaders of different racks will exchange the data across locations.\nI hope some find this stuff useful. If you do, please let me know! The code is available on github under MIT license. Some basic documentation / brain dump can be found in the github wiki. The main website of the project is at gossiperl.com. The list of work items, issues and bugs is at github issues.\nIt’s been fun building the basics. There’s more fun ahead.\n","description":"","tags":["erlang","gossip","thrift","influxdb"],"title":"Gossiperl gossip middleware in Erlang","uri":"/posts/2014-12-09-gossiperl-gossip-middleware-in-erlang/"},{"content":"","description":"","tags":null,"title":"influxdb","uri":"/tags/influxdb/"},{"content":"A few months ago I have started learning Erlang, mostly for fun but it was right about time to jump on the functional bandwagon. The best way to learn a new language is to find an engaging project, in my case its been something what has been on my mind for quite a while: a cloud communication protocol / framework for distributed computing. Some basic principles of what it is about can be found here: CloudDDS.\nWhile learning Erlang, I decided to also learn a number of other technologies, or using other words - apply them in practice. One of those is Apache Thrift. Apache Thrift is a binary cross-language framework for distributed service development. It is a serialization protocol with its own RPC stack.\nThrift RPC stack comes with a number of supported protocols, referred to as transports. Erlang implementation provides HTTP, JSON, file, disk log and framed transports. The project I was working on uses UDP for all communication. Below, I am going to describe how I implemented Thrift over UDP in Erlang. I will walk the reader through building a simple active-active setup of two clients communicating via UDP using Thrift messages.\nPrerequisites Apache Thrift uses .thrift file to describe services and types to used by the software. To generate the language code, Apache Thrift must be installed first. There is a lot of examples available how this can be achieved. I have created a bash script for Ubuntu 12.04 for my own reference, available here: Install Thrift 0.9.1 with all language support.\nDesign I will follow the standard OTP principles while developing the example.\napplication -\u003e supervisor -\u003e UDP messaging worker | -\u003e serialization worker While discussing the implementation, I will not focus on the details of the messaging code, neither I will dive into the details of the standard OTP components. I will focus only on the Thrift part.\nBasic OTP modules First step is to create a basic Erlang application. All the code can be found below. Additionally, at the bottom of the article I am publishing the link to the sources available on github.\nrebar.config A bare minimum required for installing dependencies.\n1 2 3 4 {deps, [ {thrift, \".*\", {git, \"https://github.com/radekg/thrift-erlang.git\", \"master\"}} ]}. I am using my own fork of Thrift, it contains updated JSX dependencies.\nsrc/udp_thrift.app.src 1 2 3 4 5 6 7 8 9 10 11 12 {application, udp_thrift, [{description, \"Example of using Thrift with UDP\"}, {vsn, \"1\"}, {modules, [udp_thrift_types, udp_thrift_app, udp_thrift_common, udp_thrift_sup, udp_thrift_messaging, udp_thrift_serialization]}, {registered, udp_thrift_sup}, {applications, [kernel, stdlib, crypto, thrift]}, {mod, {udp_thrift_app, []}}]}. src/udp_thrift_app.erl 1 2 3 4 5 6 7 8 -module(udp_thrift_app). -behaviour(application). -export([start/2, stop/1]). start(_Type, _Args) -\u003e udp_thrift_sup:start_link(). stop(_State) -\u003e ok. src/udp_thrift_common.erl 1 2 3 4 5 6 7 8 9 10 11 -module(udp_thrift_common). -export([ get_timestamp/0, get_message_id_as_string/0 ]). get_timestamp() -\u003e {Mega,Sec,Micro} = os:timestamp(), trunc( ((Mega*1000000+Sec)*1000000+Micro) / 1000000 ). get_message_id_as_string() -\u003e lists:flatten( io_lib:format( \"~p\", [ make_ref() ] ) ). src/udp_thrift_sup.erl 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 -module(udp_thrift_sup). -behaviour(supervisor). -export([start_link/0, init/1]). start_link() -\u003e supervisor:start_link({local, ?MODULE}, ?MODULE, []). init([]) -\u003e Port = case application:get_env(udp_thrift, port) of { ok, Value } -\u003e Value; undefined -\u003e { error, no_port } end, PeerPort = case application:get_env(udp_thrift, peer_port) of { ok, Value2 } -\u003e Value2; undefined -\u003e { error, no_peer_port } end, case application:get_env(udp_thrift, name) of { ok, Name } -\u003e {ok, { { one_for_all, 10, 10}, [ { udp_thrift_messaging, {udp_thrift_messaging, start_link, [ {127,0,0,1}, Port, Name, PeerPort ]}, permanent, brutal_kill, worker, [] }, { udp_thrift_serialization, {udp_thrift_serialization, start_link, []}, permanent, brutal_kill, worker, [] } ] } }; undefined -\u003e { error, no_name_given } end. The supervisor expects a number of environment properties to be set. This will be provided later on, in Running and testing section. If one or more of these is not provided, the application will not start or fail at a worker start step.\nsrc/udp_thrift_messaging.erl 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 -module(udp_thrift_messaging). -behaviour(gen_server). -export([start_link/4, stop/0]). -export([init/1, handle_call/3, handle_cast/2, handle_info/2, code_change/3, terminate/2]). -define(SERVER, ?MODULE). -include(\"udp_thrift_types.hrl\"). start_link(BindAddress, BindPort, Name, PeerPort) -\u003e gen_server:start_link({local, ?MODULE}, ?MODULE, [BindAddress, BindPort, Name, PeerPort], []). stop() -\u003e gen_server:cast(?MODULE, stop). init([BindAddress, BindPort, Name, PeerPort]) -\u003e case gen_udp:open(BindPort, [binary, {ip, BindAddress}]) of {ok, Socket} -\u003e erlang:send_after(2000, self(), { contact_peer, BindPort }), {ok, {messaging, Socket, Name, PeerPort}}; {error, Reason} -\u003e {error, Reason} end. terminate(_Reason, {messaging, Socket, _}) -\u003e gen_udp:close(Socket). handle_cast(stop, LoopData) -\u003e {noreply, LoopData}. %% SENDING handle_info({ contact_peer, Port }, { messaging, Socket, Name, PeerPort }) -\u003e error_logger:info_msg( \"Sending digest to ~p.\", [ PeerPort ] ), Digest = #digest{ name = Name, port = Port, heartbeat = udp_thrift_common:get_timestamp(), id = udp_thrift_common:get_message_id_as_string() }, udp_thrift_serialization ! { serialize, digest, Digest, self() }, erlang:send_after(2000, self(), { contact_peer, Port }), {noreply, { messaging, Socket, Name, PeerPort }}; handle_info({ message_serialized, { ok, SerializedMessage } }, { messaging, Socket, Name, PeerPort }) -\u003e gen_udp:send( Socket, { 127,0,0,1 }, PeerPort, SerializedMessage ), {noreply, { messaging, Socket, Name, PeerPort } }; %% RECEIVING handle_info({udp, _ClientSocket, _ClientIp, _ClientPort, Msg}, { messaging, Socket, Name, PeerPort }) -\u003e udp_thrift_serialization ! { deserialize, Msg, self() }, {noreply, { messaging, Socket, Name, PeerPort }}; handle_info({ message_deserialized, { ok, DecodedPayloadType, DecodedPayload } }, { messaging, Socket, Name, PeerPort }) -\u003e self() ! { message, DecodedPayloadType, DecodedPayload }, {noreply, { messaging, Socket, Name, PeerPort }}; handle_info({ message, digest, DecodedPayload }, { messaging, Socket, Name, PeerPort }) -\u003e error_logger:info_msg(\"Received digest from ~p. Replying to ~p\", [ DecodedPayload#digest.name, DecodedPayload#digest.port ]), DigestAck = #digestAck{ name = Name, heartbeat = udp_thrift_common:get_timestamp(), reply_id = DecodedPayload#digest.id }, udp_thrift_serialization ! { serialize, digestAck, DigestAck, self() }, {noreply, { messaging, Socket, Name, PeerPort }}; handle_info({ message, digestAck, DecodedPayload }, { messaging, Socket, Name, PeerPort }) -\u003e error_logger:info_msg(\"Received digestAck to digest ~p.\", [ DecodedPayload#digestAck.reply_id ]), {noreply, { messaging, Socket, Name, PeerPort }}; %% ERROR HANDLING handle_info({ message_deserialized, {error, Reason} }, { messaging, Socket, Name, PeerPort }) -\u003e error_logger:error_msg(\"Message decode failed. Reason ~p.\", [Reason]), {noreply, { messaging, Socket, Name, PeerPort }}; handle_info(Msg, LoopData) -\u003e error_logger:info_msg(\"Unhandled handle_info ~p\", [Msg]), {noreply, LoopData}. %% REMAINDER OF GEN_SERVER BEHAVIOUR handle_call({message, _Msg}, _From, LoopData) -\u003e {reply, ok, LoopData}. code_change(_OldVsn, State, _Extra) -\u003e {ok, State}. private/udp_thrift.thrift The task of this example is for every client, at two seconds interval, send a Digest to the other client. On the other side of the wire, upon receiving the Digest, send a DigestAck message back. Every message is wrapped in a DigestEnvelope container so the program can easily establish what is the digest type sent. The thrift file looks like this:\nnamespace erl udp_thrift struct DigestEnvelope { 1: required string payload_type; 2: required string bin_payload; } struct Digest { 1: required string name; 2: required i32 port; 3: required i64 heartbeat; 4: required string id; } struct DigestAck { 1: required string name; 2: required i64 heartbeat; 3: required string reply_id; } Generate code from Thrift and make it available Once thrift is installed on the system, the code can be generated from the thrift file. To do so:\n1 2 3 4 5 6 7 8 cd \u003cproject root\u003e/private/ thrift --gen erl udp_thrift.thrift mkdir -p ../include # make the generated code available for the program: mv gen-erl/udp_thrift_types.erl ../src/udp_thrift_types.erl mv gen-erl/udp_thrift_constants.hrl ../include/udp_thrift_constants.hrl mv gen-erl/udp_thrift_types.hrl ../include/udp_thrift_types.hrl rm -Rf gen-erl The source code on github contains a Vagrant box which can be used to generate the code:\n1 vagrant up thrift Thrift serialisation and deserialisation The module responsible for these tasks has been already started in the supervisor. Let’s go through the code and discuss the parts in detail.\n1 2 3 4 5 6 7 8 9 10 11 12 -module(udp_thrift_serialization). -behaviour(gen_server). -export([start_link/0, stop/0]). -export([init/1, handle_call/3, handle_cast/2, handle_info/2, code_change/3, terminate/2]). -include(\"udp_thrift_types.hrl\"). First, these records are required. As the serialisation happens within this single module, I include them directly by copying from Erlang Thrift sources.\n1 2 3 4 5 -record(binary_protocol, {transport, strict_read=true, strict_write=true }). -record(memory_buffer, {buffer}). Following is the code for the module setup. For serialisation, a single protocol instance may be used for all serialized messages.\n1 2 3 4 5 6 7 8 9 start_link() -\u003e gen_server:start_link({local, ?MODULE}, ?MODULE, [], []). init([]) -\u003e {ok, OutThriftTransport} = thrift_memory_buffer:new(), {ok, OutThriftProtocol} = thrift_binary_protocol:new(OutThriftTransport), {ok, { serialization, OutThriftProtocol }}. stop() -\u003e gen_server:cast(?MODULE, stop). Serialization 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 handle_info({ serialize, DigestType, Digest, CallerPid }, LoopData) -\u003e self() ! { serialize, DigestType, Digest, udp_thrift_types:struct_info(DigestType), CallerPid }, {noreply, LoopData}; handle_info({ serialize, DigestType, Digest, StructInfo, CallerPid }, { serialization, OutThriftProtocol }) -\u003e CallerPid ! { message_serialized, { ok, digest_to_binary( #digestEnvelope{ payload_type = atom_to_list(DigestType), bin_payload = digest_to_binary(Digest, StructInfo, OutThriftProtocol) }, udp_thrift_types:struct_info(digestEnvelope), OutThriftProtocol ) } }, {noreply, { serialization, OutThriftProtocol } }; First handle_info receives the data to serialize, it uses udp_thrift_types:struct_info provided by the thrift generated modules to get the thrift type definition. This is forwarded to the the second handle_info. digest_to_binary is where all the action happens (presented shortly). The inner digest_to_binary call serializes the digest received from messaging component. This is then wrapped inside of the digestEnvelope digest which has to be converted to binary data for UDP delivery.\nDeserialization 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 handle_info({ deserialize, BinaryDigest, CallerPid }, LoopData) -\u003e try case digest_from_binary(digestEnvelope, BinaryDigest) of {ok, DecodedResult} -\u003e case payload_type_as_known_atom(DecodedResult#digestEnvelope.payload_type) of { ok, PayloadTypeAtom } -\u003e case digest_from_binary( PayloadTypeAtom, DecodedResult#digestEnvelope.bin_payload) of { ok, DecodedResult2 } -\u003e CallerPid ! { message_deserialized, { ok, PayloadTypeAtom, DecodedResult2 } }; _ -\u003e error_logger:error_msg(\"Message could not be decoded.\"), CallerPid ! { message_deserialized, { error, decode_binary_content_failed } } end; { error, UnsupportedPayloadType } -\u003e error_logger:error_msg(\"Unsupprted message ~p.\", [UnsupportedPayloadType]), CallerPid ! { message_deserialized, {error, unsuppoted_payload_type} } end; _ -\u003e error_logger:error_msg(\"Could not open digestEnvelope.\"), CallerPid ! { message_deserialized, {error, digest_envelope_open_failed} } end catch _Error:Reason -\u003e gossiper_log:err(\"Error while reading digest: ~p.\", [Reason]), CallerPid ! { message_deserialized, { error, digest_read } } end, {noreply, LoopData}; handle_info(_Info, LoopData) -\u003e {noreply, LoopData}. Upon received a deserialization request, the first thing that happens is deserialize_from_binary. This function assumes that the data received is Thrift and the data is of type digestEnvelope. The details will be presented shortly. Once the payload carried by the envelope has been read, the program detects if the payload is of a supported type. If so, it attempts deserializing the content. If all of the above succeeds, the result is passed back to messaging (caller of the serialization). Majority of that function is simple error handling.\ndigest_to_binary, digest_from_binary and payload_type_as_known_atom 1 2 3 4 5 6 digest_to_binary(Digest, StructInfo, OutThriftProtocol) -\u003e {PacketThrift, ok} = thrift_protocol:write(OutThriftProtocol, { {struct, element(2, StructInfo)}, Digest}), {protocol, _, OutProtocol} = PacketThrift, {transport, _, OutTransport} = OutProtocol#binary_protocol.transport, iolist_to_binary(OutTransport#memory_buffer.buffer). digest_to_binary serializes the payload. It uses the copy of the protocol defined in init to write the thrift binary representation of data to be sent. All what has to be done afterwards is converting iolist contained in the resulting OutTransport to binary data. This is returned to CallerPid and sent via UDP to the destination.\n1 2 3 4 5 6 7 8 9 10 11 12 digest_from_binary(DigestType, BinaryDigest) -\u003e {ok, InTransport} = thrift_memory_buffer:new(BinaryDigest), {ok, InProtocol} = thrift_binary_protocol:new(InTransport), case thrift_protocol:read( InProtocol, {struct, element(2, udp_thrift_types:struct_info(DigestType))}, DigestType) of {_, {ok, DecodedResult}} -\u003e {ok, DecodedResult}; _ -\u003e {error, not_thrift} end. digest_from_binary receives an atom of an expected digest type and a binary digest as an input. First, a memory_buffer transport is constructed from the binary data. It is then converted into a binary protocol and an attempt to read the data from thrift is made using a structure provided by udp_thrift_types:struct_info.\n1 2 3 4 5 6 7 8 payload_type_as_known_atom(DigestTypeBin) -\u003e KnownDigestTypes = [ { \u003c\u003c\"digest\"\u003e\u003e, digest }, { \u003c\u003c\"digestAck\"\u003e\u003e, digestAck } ], case lists:keyfind( DigestTypeBin, 1, KnownDigestTypes ) of false -\u003e { error, DigestTypeBin }; { _Bin, Atom } -\u003e { ok, Atom } end. The above function is rather self explanatory. The deserialization will be attempted if an atom can be found in the KnownDigestTypes for given digest type. Otherwise an error is returned.\nThe module ends with the rest of required gen_server behaviour.\n1 2 3 4 5 6 7 8 9 10 11 handle_call(_Msg, _From, LoopData) -\u003e {reply, ok, LoopData}. handle_cast(stop, LoopData) -\u003e {noreply, LoopData}. terminate(_Reason, _LoopData) -\u003e {ok}. code_change(_OldVsn, State, _Extra) -\u003e {ok, State}. Running and testing Preparation:\n1 2 3 cd \u003cproject directory\u003e ./rebar get-deps ./rebar compile To run, two terminal windows are required. In the first window, start Erlang with this command:\n1 erl -pa ebin/ -pa deps/*/ebin Run the application:\n1 2 3 4 5 6 application:set_env(udp_thrift, name, \u003c\u003c\"memberA\"\u003e\u003e). application:set_env(udp_thrift, port, 6666). application:set_env(udp_thrift, peer_port, 6667). application:start(crypto). application:start(thrift). application:start(udp_thrift). In the second terminal window, start Erlang:\n1 erl -pa ebin/ -pa deps/*/ebin Run the application:\n1 2 3 4 5 6 application:set_env(udp_thrift, name, \u003c\u003c\"memberB\"\u003e\u003e). application:set_env(udp_thrift, port, 6667). application:set_env(udp_thrift, peer_port, 6666). application:start(crypto). application:start(thrift). application:start(udp_thrift). Both windows should be showing output similar to this:\n… Sending digest to 6667. =INFO REPORT==== 12-Oct-2014::21:43:43 === Sending digest to 6667. =INFO REPORT==== 12-Oct-2014::21:43:43 === Received digestAck to digest \u003c\u003c\"#Ref\u003c0.0.0.95\u003e\"\u003e\u003e. =INFO REPORT==== 12-Oct-2014::21:43:45 === Received digest from \u003c\u003c\"memberB\"\u003e\u003e. Replying to 6667 =INFO REPORT==== 12-Oct-2014::21:43:45 === Sending digest to 6667. =INFO REPORT==== 12-Oct-2014::21:43:45 === Received digestAck to digest \u003c\u003c\"#Ref\u003c0.0.0.100\u003e\"\u003e\u003e. =INFO REPORT==== 12-Oct-2014::21:43:47 === Received digest from \u003c\u003c\"memberB\"\u003e\u003e. Replying to 6667 =INFO REPORT==== 12-Oct-2014::21:43:47 === Sending digest to 6667. =INFO REPORT==== 12-Oct-2014::21:43:47 === Received digestAck to digest \u003c\u003c\"#Ref\u003c0.0.0.105\u003e\"\u003e\u003e. … If that is the case, both clients are communicating via UDP with Thrift protocol.\nCode and license All code discussed above can be found on github: Using Apache Thrift with UDP in Erlang. The code is available under MIT license.\n","description":"","tags":["thrift","erlang"],"title":"Apache Thrift via UDP in Erlang","uri":"/posts/2014-10-12-apache-thrift-via-udp-in-erlang/"},{"content":"","description":"","tags":null,"title":"devstack","uri":"/tags/devstack/"},{"content":"","description":"","tags":null,"title":"openstack","uri":"/tags/openstack/"},{"content":"I’d like to develop for OpenStack The eaiest way to start, is to use a project called devstack. Devstack is:\nA documented shell script to build complete OpenStack development environments.\nIt looks promising. Unfortunately, the script is not as simple as what is claimed on the website:\ngit clone https://github.com/openstack-dev/devstack.git # configure, while optional... cd devstack; ./stack.sh There be Dragons.\nSo? After looking at a number of different search results on the Internets, digging through some Chef cookbooks, Puppet stuff, here’s a Vagrant file to set up Devstack using Vagrant (Ubuntu 12.04):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 Vagrant.configure(\"2\") do |config| config.vm.provider \"virtualbox\" do |vbox, override| override.vm.box = \"precise64\" override.vm.box_url = \"http://files.vagrantup.com/precise64_vmware.box\" vbox.customize [\"modifyvm\", :id, \"--memory\", \"2048\"] end config.vm.define \"devstack\" do |devstack| # redirect horizon, we can't run on port 80 devstack.vm.network :forwarded_port, guest: 80, host: 9000 # redirect the API devstack.vm.network :forwarded_port, guest: 8774, host: 8774 # redirect the auth API devstack.vm.network :forwarded_port, guest: 35357, host: 35357 # install git devstack.vm.provision :shell, :inline =\u003e \"apt-get install -y git-core\" # clone Devstack devstack.vm.provision :shell, :inline =\u003e \"cd /opt \u0026\u0026 git clone https://github.com/openstack-dev/devstack.git\" # Create the stack user devstack.vm.provision :shell, :inline =\u003e \"/opt/devstack/tools/create-stack-user.sh\" # add user to sudoers devstack.vm.provision :shell, :inline =\u003e \"echo \\\"stack ALL=(ALL) NOPASSWD: ALL\\\" \u003e\u003e /etc/sudoers\" # localrc file, explained below # we are going to copy this file from the host to the guest where we run Devstack devstack.vm.provision :shell, :inline =\u003e \"cat \u003e /opt/devstack/localrc \u003c\u003c'VAGRANTEOP'\\n#{File.read( File.dirname(__FILE__) + \"/setup/devstack-localrc\" )}\\nVAGRANTEOP\" # just make it work, it's dev devstack.vm.provision :shell, :inline =\u003e \"chmod -R 0777 /opt/devstack\" # and stack it all up devstack.vm.provision :shell, :inline =\u003e \"su - stack -c '/opt/devstack/stack.sh'\" end end About 12.5 minutes Vagrant should display something like this:\n2014-05-20 15:03:49.711 | stack.sh completed in 758 seconds. Horizon will be accessible via http://localhost:9000, the API is available on http://localhost:8774/v2/…\nWait, localrc! The structure of the project should looks like this:\n-- Vagrantfile -- setup |-- devstack-localrc The devstack-localrc file contains some critical information, its place is most likely in .gitignore. Here’s the basic content, just put your own data in it:\nDEST=/opt/devstack ENABLED_SERVICES+=,heat ADMIN_PASSWORD=meh MYSQL_PASSWORD=meh RABBIT_PASSWORD=meh SERVICE_PASSWORD=meh SERVICE_TOKEN=meh HOST_IP=0.0.0.0 LOGFILE=stack.sh.log LOGDAYS=1 LOG_COLOR=False SCREEN_LOGDIR=/opt/stack/logs/screen API_RATE_LIMIT=False APT_FAST=True RECLONE=yes Voilà!\n","description":"","tags":["openstack","devstack","vagrant","ubuntu"],"title":"OpenStack Devstack up and running with Vagrant, in 12.5 minutes","uri":"/posts/2014-05-20-openstack-devstack-up-and-running-with-vagrant-in-125-minutes/"},{"content":"","description":"","tags":null,"title":"ubuntu","uri":"/tags/ubuntu/"},{"content":"","description":"","tags":null,"title":"vagrant","uri":"/tags/vagrant/"},{"content":"It’s been already a month since I released erflux on github. Erflux is an Erlang client for InfluxDB HTTP protocol.\nInstallation 1 2 3 4 {deps, [ {erflux, \".*\", {git, \"git://github.com/radekg/erflux.git\", {tag, \"version-1\"}}} }]} and run\n./rebar get-deps Configuration Erflux allows configuring a number of parameters:\nInfluxDB host, default 127.0.0.1 InfluxDB port, default 8086 username, default: root password, default: root SSL usage, default: false timeout, default: infinity The simplest way of applying configuration is to use application:set_env, like the example below:\napplication:start(crypto), application:start(asn1), application:start(public_key), application:start(ssl), application:start(idna), application:start(hackney), application:start(jsx), application:load(erflux), application:set_env(erflux, host, \u003c\u003c\"192.168.50.115\"\u003e\u003e), application:set_env(erflux, port, 8086), application:set_env(erflux, username, \u003c\u003c\"root\"\u003e\u003e), application:set_env(erflux, password, \u003c\u003c\"root\"\u003e\u003e), application:set_env(erflux, ssl, false), application:set_env(erflux, timeout, infinity), application:start(erflux), erflux_sup:add_erflux(erflux_http), erflux_http:get_databases(). Writing data To write data with erlfux:\n1 2 3 4 5 6 7 erflux_http:write_series(erfluxtest, [ [ { points, [ [ 1, 2, 3 ] ] }, { name, testseries }, { columns, [ a, b, c ] } ] ]). or\n1 2 3 4 5 erflux_http:write_point(erfluxtest, testseries, [ { a, 1 }, { b, 2 }, { c, 3 } ]). Reading data Reading many columns:\nerflux_http:read_point(erfluxtest, [a, b], testseries). or a single column:\nerflux_http:read_point(erfluxtest, a, testseries). More complex queries like this can be executed using the q function:\nerflux_http:q(erfluxtest, \u003c\u003c\"select A from testseries limit 1\"\u003e\u003e). Advanced features In case if it is necessary to open multiple connection to different InfluxDB servers, erflux allows for it by instantiating multiple clients. The application has a choice of using provider supervisor:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 application:start(crypto), application:start(asn1), application:start(public_key), application:start(ssl), application:start(idna), application:start(hackney), application:start(jsx), application:start(erflux), %% This will connect to localhost, if no other settings provided: erflux_sup:add_erflux(erflux_http), erflux_http:get_databases(). %% Start the additional client, connect to a different host: {ok, RemoteHost} = erflux_sup:add_erflux(erflux_custom_host, \u003c\u003c\"root\"\u003e\u003e, \u003c\u003c\"root\"\u003e\u003e, \u003c\u003c\"somehost.influxdb\"\u003e\u003e). %% To list databases of the remote host, do the following: erflux_http:get_databases(RemoteHost). %% To remove the instance: erflux_sup:remove_erflux(erflux_custom_host). or bypassing the supervisor:\n1 2 { ok, Pid } = erflux_http:start_link( erflux_custom, #erflux_config{} ), erflux_http:get_databases( Pid ). Atoms and binaries Every function comes in 2 variants:\naccepting atoms and accepting binaries It’s not possible to mix argument types. For example, this will fail:\n1 erflux_http:create_database_user(database, \u003c\u003c\"username\"\u003e\u003e, password). Query parameter of erflux_http:q is always binary.\nThe exceptions to the all rule are columns in write_series, write_point and read_series. All these are valid:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 erflux_http:write_series(erfluxtest, [ [ { points, [ [ 1, 2, 3 ] ] }, { name, testseries }, { columns, [ a, \u003c\u003c\"b\"\u003e\u003e, c ] } ] ]). erflux_http:write_point(erfluxtest, testseries, [ { a, 1 }, { \u003c\u003c\"b\"\u003e\u003e, 2 }, { c, 3 } ]). erflux_http:read_point(erfluxtest, [\u003c\u003c\"a\"\u003e\u003e, b], testseries). The code The code is available on github. It is licensed under the MIT license.\n","description":"","tags":["erlang","influxdb"],"title":"Erflux, InfluxDB client for Erlang","uri":"/posts/2014-10-21-erflux-influxdb-client-for-erlang/"},{"content":"The problem One of my Zookeeper clusters has to be available to everyone. But I want to make sure only specific known hosts are allowed to connect. The problem was, those known hosts are dynamic and I didn’t want any configuration for this component. The servers are running in the cloud, they come and go.\nAt Technicolor, we use Chef to manage all our boxes. Every box is registered on the Chef server, we know every private and public IP address of every server within our setup. Be it EC2, IBM SCE, Rackaspace or any other cloud provider. What I came up with required writing Zookeeper authentication provider connected to the Chef server; only allow the connection if the IP is known at the time of connection.\nIt all sounds pretty straightforward, however, Zookeeper authentication is not very well documented. It took quite a while to connect all the dots and make it work.\nThe code Full code available on Github.\nThe solution First of all, the term authentication. It is a bit confusing at start, Zookeeper uses the term ACL. That’s because permissions are applied per znode and not the server. When talking about authentication one is really thinking access to that particular znode.\nMy authentication implementation can be found here. All the Ruby bits are in the same project.\nAs can be seen, all what has to be done is creating a class which will implement org.apache.zookeeper.server.auth.AuthenticationProvider interface. In case of Chef based authentication, in my environment, I wanted every node to have full post authentication access to Zookeeper. Hence this:\n1 2 3 4 5 6 7 8 9 10 11 public boolean matches(String id, String aclExpr) { return true; } public boolean isAuthenticated() { return true; } public boolean isValid(String id) { return true; } If the purpose of the authenticartion provider was to lock down separate znodes, the implementation of matches(String id, String aclExpr) would differ. There are quite good examples in Zookeeper sources, these are available here.\nWriting the provider was the easy part. Once the jar is compiled, placed in the lib folder and Zookeeper is restarted, it is available. The poorly documented bit is the following:\nEvery node that should not be available to the world should have the ACL applied. ACL is not recursive.\nThat basically means:\nEven though the autentication is enabled and working, it can’t be observed because everything is public anyway.\nExample Let’s consider fresh Zookeeper installation. It comes with 3 znodes. These are /, /zookeeper and /zookeeper/quota. Even if the authentication provider is loaded and working, everyone in the world will be able to see and modify these if ACL is not applied. Following code may be used to lock down the / znode:\n1 2 3 4 5 6 7 8 require \"rubygems\" require \"zookeeper\" acl = [ Zookeeper::ACLs::ACL.new( :perms =\u003e 31, :id =\u003e Zookeeper::ACLs::Id.new( :scheme =\u003e \"chef\", :id =\u003e \"\" ) )] z = Zookeeper.new(\"127.0.0.1:2181\") z.set_acl(:path =\u003e \"/\", :acl =\u003e acl ) From now on the world would not be allowed to browse nor modify anything under /. However, /zookeeper and /zookeeper/quota are still wide open. Full lock down of a fresh Zookeeper installation is simple:\n1 2 3 4 5 6 7 require \"rubygems\" require \"zookeeper\" acl = [ Zookeeper::ACLs::ACL.new(:perms =\u003e 31, :id =\u003e Zookeeper::ACLs::Id.new( :scheme =\u003e \"chef\", :id =\u003e \"\" ) )] z = Zookeeper.new(\"127.0.0.1:2181\") z.set_acl(:path =\u003e \"/\", :acl =\u003e acl ) z.set_acl(:path =\u003e \"/zookeeper\", :acl =\u003e acl ) z.set_acl(:path =\u003e \"/zookeeper/quota\", :acl =\u003e acl ) From now on the world will still be able to connect to my Zookeeper. But no operations are permitted. Obviously, whatever new znode is created, it should have the ACL applied. Like this:\n... zk_acl = [ Zookeeper::ACLs::ACL.new( :perms =\u003e 31, :id =\u003e Zookeeper::ACLs::Id.new( :scheme =\u003e \"chef\", :id =\u003e \"\" ) )] z.create(:path =\u003e \"/nodes/#{@@ipv4}\", :acl =\u003e zk_acl, :ephemeral =\u003e true) ... Unless the znode is meant to be wide open, obviously.\n","description":"","tags":["zookeeper","apache","chef","opscode"],"title":"Apache Zookeeper authentication","uri":"/posts/2013-06-24-apache-zookeeper-authentication/"},{"content":"","description":"","tags":null,"title":"chef","uri":"/tags/chef/"},{"content":"","description":"","tags":null,"title":"opscode","uri":"/tags/opscode/"},{"content":" I had to cut out cookbooks/nodejs from the develop into a separate repository and move develop to master.\nThe problem is, there is a git repository called XYZ with the following structure:\ncookbooks nodejs ... mysql ... other_stuff ... and following branches:\nrefs/heads/master refs/heads/develop I had to cut out cookbooks/nodejs from the develop into a separate repository and move develop to master. Here’s how I’ve done it:\ngit clone git@github.com:.../XYZ.git . git filter-branch --tag-name-filter cat --prune-empty --subdirectory-filter cookbooks/nodejs develop rm -Rf cookbooks rm -Rf other_stuff # move develop to master git checkout develop git branch -D master git branch master git checkout master git branch -D develop # now we're left with master only, make sure no unwanted tags are carried over: git tag # for each unwanted tag do: git tag -d tagname git remote remove origin git remote add git@.../NEWREPO.git git clone --bare . NEWREPO.git cd NEWREPO.git # create new repo on github and push: git push --mirror git@github.com:.../NEWREPO.git Job done.\nUseful links Detach subdirectory into separate Git repository ","description":"","tags":["git"],"title":"Git: chopping out part of the repo into a separate repo","uri":"/posts/2013-03-22-git-chopping-out-part-of-the-repo-into-a-separate-repo/"},{"content":"","description":"","tags":null,"title":"devops","uri":"/tags/devops/"},{"content":"","description":"","tags":null,"title":"ec2","uri":"/tags/ec2/"},{"content":"","description":"","tags":null,"title":"knife","uri":"/tags/knife/"},{"content":" I keep getting Fog::Compute::AWS::NotFound: The key pair ... does not exist error!\nHow to solve this problem isn’t explained well enough anywhere.\nInstall knife-ec2 first:\nsudo gem install knife-ec2 Then I added this to my knife.rb:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # key name, as defined in EC2 Key Pairs, # this value has to be exactly the same as the one in the management console: knife[:aws_ssh_key_id] = \"my-ec2-key\" # AWS ID/SECRET: knife[:aws_access_key_id] = \"...\" knife[:aws_secret_access_key] = \"...\" # Some defaults knife[:region] = \"eu-west-1\" knife[:availability_zone] = \"eu-west-1a\" knife[:ssh_user] = \"ubuntu\" knife[:flavor] = \"m1.medium\" # Ubuntu 12.04 LTS 64bit: knife[:image] = \"ami-f2191786\" knife[:use_sudo] = \"true\" As I was trying to create an instance with this command:\nknife ec2 server create -G \"default\" -r 'recipe[monit]' I kept getting following error:\nFog::Compute::AWS::NotFound: The key pair 'my-ec2-key' does not exist It turned out, 2 hours later, in order to fix this I had to run:\nssh-add ~/.ssh/my-ec2-key.pem It worked.\nUseful links ServerFault: the only place where I found a mention of ssh-add with this particular issue Knife EC2 Extension – Install and Use ","description":"I keep getting Fog::Compute::AWS::NotFound: The key pair … does not exist error","tags":["knife","ec2","chef","devops"],"title":"Setting up knife ec2","uri":"/posts/2013-03-07-setting-up-knife-ec2/"},{"content":"{% include collecttags.html %}\nI was asked to build a box with all the monitoring tools required. Chef, Logstash, Ganglia and Monit were selected. Here’s a core dump.\nIn this post I’m going to describe how to setup a box containing following elements:\nOpsworks Chef Server Logstash indexer with Redis as an incoming queue Kibana Ganglia collector Monit with the log shipped to logstash M/Monit I have this nasty habit - this whole thing is running as root. Well, Chef Server isn’t, it creates it’s own user ane group. But everything else is…\nAll of those components will sit behind nginx SSL. Components that don’t provide authentication will be secured with HTTP basic auth.\nWe are going to start with Chef Server. The whole process is nicely explained in the Chef documentation. But it is scattered across a number of pages, some sort of condensed knowledge is always nice to have.\nChef Server Start with creating an A record for chef.[your domain name] pointing to your box. Then SSH to the box.\nSet some initial values, the password in the first line doesn’t really matter, it has to be changed upon first login:\nCHEF_WEBUI_ADMIN_PASSWORD=somerandompassword CHEF_RABBITMQ_CONSUMER_PASSWORD=[use-some-strong-password-here] CHEF_WEBUI_HOST=http://chef.[your domain name]:4000 set -e -x export HOME=\"/root\" export DEBIAN_FRONTEND=noninteractive We have to install debconf-util to be able to provide the settings dutring installation. And git, we will need it later…\napt-get install -q -y debconf-utils git-core Follow the steps from Chef docs:\necho \"deb http://apt.opscode.com/ `lsb_release -cs`-0.10 main\" | sudo tee /etc/apt/sources.list.d/opscode.list mkdir -p /etc/apt/trusted.gpg.d gpg --keyserver keys.gnupg.net --recv-keys 83EF826A gpg --export packages@opscode.com | sudo tee /etc/apt/trusted.gpg.d/opscode-keyring.gpg \u003e /dev/null apt-get update apt-get install -q -y opscode-keyring apt-get -y upgrade This is where we add the additional steps. Set those settings to be able to do noninteractive setup:\necho chef-server-webui chef-server-webui/admin_password password $CHEF_WEBUI_ADMIN_PASSWORD | debconf-set-selections echo chef-solr chef-solr/amqp_password password $CHEF_RABBITMQ_CONSUMER_PASSWORD | debconf-set-selections echo chef chef/chef_server_url string $CHEF_WEBUI_HOST | debconf-set-selections apt-get -q -y install chef chef-server When this is done, and it can take a bit, you can go http://[your server]:4040. Log in as admin using $CHEF_WEBUI_ADMIN_PASSWORD password. You will have to chnge it upon first login.\nnginx I am sure you would like to see Chef running. We may as well set up nginx just now. Why not. If you have ports 4000 and 4040 opened for public access, close them. You won’t need them.\napt-get install -q -y nginx Now we need a self-signed certificate so we can serve everything via SSL:\nmkdir -p /tmp/certs cd /tmp/certs You will have to type the password a number of times in this step. Just make sure it is always the same.\nopenssl genrsa -des3 -out myssl.key 1024 openssl req -new -key myssl.key -out myssl.csr cp myssl.key myssl.key.org openssl rsa -in myssl.key.org -out myssl.key openssl x509 -req -days 365 -in myssl.csr -signkey myssl.key -out myssl.crt cp myssl.crt /etc/ssl/certs/ cp myssl.key /etc/ssl/private/ Time to confiure nginx:\ntouch /etc/nginx/sites-available/devops ln -s /etc/nginx/sites-available/devops /etc/nginx/sites-enabled/devops [vi|vim|nano|joe] /etc/nginx/sites-enabled/devops Paste the following content:\nupstream chef_api_local { server localhost:4000; } upstream chef_webui_local { server localhost:4040; } server { server_name chef.[your domain name]; ssl on; ssl_certificate /etc/ssl/certs/myssl.crt; ssl_certificate_key /etc/ssl/private/myssl.key; listen 443; ssl_session_timeout 5m; ssl_protocols SSLv2 SSLv3 TLSv1; ssl_ciphers ALL:!ADH:!EXPORT56:RC4+RSA:+HIGH:+MEDIUM:+LOW:+SSLv2:+EXP; ssl_prefer_server_ciphers on; root /var/www; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto https; location / { # API request incoming if ( $http_x_ops_timestamp != \"\" ){ proxy_pass http://chef_api_local; break; } #webui request incoming proxy_pass http://chef_webui_local; } } Save the file and execute:\n/etc/init.d/nginx stop /etc/init.d/nginx start The reload command gives some incosistent results. It doesn’t like to work every single time.\nNow you can simply go to https://chef.[your domain name].\nKibana and logstash Logstash will be used for logs aggregation. It can be set up in standalone or distributed mode. But standalone isn’t really “aggregation”. No?\nIn the distributed mode we are looking at two components. An indexer and shipper. Indexer is the end point. Somwehere where we see everything. Or where we ship to another indexer. Can be tricky, depending on the setup.\nFor now let’s just assume that we have “some shippers” and “an indexer”. Shipper is really easy, we will focus on the indexer for now. Bear with me.\nCreate an A record for kibana.[your server].\nNext, back on the server:\nmkdir /opt/kibana cd /opt/kibana git clone --branch=kibana-ruby https://github.com/rashidkpc/Kibana.git . gem install bundler bundle install Please note - Ruby with all additional dependencies is already installed for us by Chef. Now the maual stuff:\n[vi|vim|nano|joe] /opt/kibana/KibanaConfig.rb and change KibanaHost to 0.0.0.0. Save the file.\nNow logstash dependencies - ElasticSearch comes first:\nmkdir /opt/elasticsearch cd /opt/elasticsearch cd /tmp wget https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-0.20.5.tar.gz tar xvf elasticsearch-0.20.5.tar.gz -C /opt/elasticsearch --strip 1 rm elasticsearch-0.20.5.tar.gz and Redis:\ncd /tmp wget http://redis.googlecode.com/files/redis-2.6.10.tar.gz tar xvf redis-2.6.10.tar.gz cd redis-2.6.10/ mkdir -p /opt/redis make PREFIX=/opt/redis install cp redis.conf /opt/redis/redis.conf Edit the /opt/redis/redis.conf file, change port to 10000, or whatever isn’t already in use… (hint: it is at the top of the file). For whatever reason 6379 default port is already used by something.\nIn a moment, when we start Redis, it will become available to everyone. If this is a what the fuck moment for you, read the red header section right below.\nRedis password: depending on where you run this setup If you run on EC2 and you have your security rules based on security groups, I assume you know how to enable the port and you understand the restrictions around that solution. In this case you don’t have to set any passwords for Redis.\nIf you have nothing like this handy, you can simply enable Redis authentication. Or use firewall. If you decide to go for the password: edit redis.conf again. Find the line starting with # requirepass, uncomment, set strong password. Be careful when choosing the password with Redis \u003c= 2.6.10.\nMake sure you also read this - Redis AUTH command.\nUpstart We need 3 upstart services for:\nKibana ElasticSearch Redis Ubuntu comes with upstart already installed, we simply need the files in /etc/init.\nelasticsearch.conf description \"ElasticSearch\" start on filesystem and net-device-up IFACE=eth0 stop on shutdown respawn script sudo -u root /opt/elasticsearch/bin/elasticsearch end script kibana.conf description \"kibana\" start on filesystem and net-device-up IFACE=eth0 stop on shutdown respawn chdir /opt/kibana script exec ruby kibana.rb end script redis.conf description \"redis\" start on filesystem and net-device-up IFACE=eth0 stop on shutdown respawn script /opt/redis/bin/redis-server /opt/redis/redis.conf end script You must chmod +x those files and start services 1 :\nchmod +x /etc/init/elasticsearch.conf chmod +x /etc/init/kibana.conf chmod +x /etc/init/redis.conf service elasticsearch start service kibana start service redis start I don’t know why ElasticSearch must be started manually the first time. Just run:\n/opt/elasticsearch/bin/elasticsearch Which will run it as a service.\nBy default Kibana runs on port 6501. But we want it to run over SSL. Edit /etc/nginx/sites-enabled/devops. Add the following upstream, at the top of the file:\nupstream kibana_local\t{ server localhost:5601; } And this at the bottom:\nserver { server_name kibana.[your domain name]; ssl on; ssl_certificate /etc/ssl/certs/myssl.crt; ssl_certificate_key /etc/ssl/private/myssl.key; listen 443; ssl_session_timeout 5m; ssl_protocols SSLv2 SSLv3 TLSv1; ssl_ciphers ALL:!ADH:!EXPORT56:RC4+RSA:+HIGH:+MEDIUM:+LOW:+SSLv2:+EXP; ssl_prefer_server_ciphers on; root /var/www; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto https; location / { auth_basic \"Resricted - Kibana\"; auth_basic_user_file /etc/nginx/htpasswd; proxy_pass http://kibana_local; } } One thing to note over here is the use of basic authentication. We must create a user:\nhtpasswd -c -d /etc/nginx/htpasswd your-user-name You will have to provide the password, 8 characters max. We will reuse the same htpasswd for ganglia a bit later.\nTime to restart nginx again.\n/etc/init.d/nginx reload You should now be able to go to https://kibana.[your domain name].\nLogstash indexer This is really straightforward. Our logstash will read from local Redis. We will store all input/output configuration in a designated folder. By default our indexer will process incoming monit logs.\nmkdir -p /opt/logstash/conf.d cd /opt/logstash wget https://logstash.objects.dreamhost.com/release/logstash-1.1.9-monolithic.jar Save this file in /opt/logstash/conf.d/monit.conf:\ninput { redis { port =\u003e 10000 type =\u003e \"monit-input\" key =\u003e \"monit_logs\" data_type =\u003e \"list\" format =\u003e \"json_event\" } } output { elasticsearch { host =\u003e \"127.0.0.1\" } } And create an upstart service for it, save the file in /etc/init/logstash-indexer.conf:\ndescription \"logstash indexer\" start on filesystem and net-device-up IFACE=eth0 stop on shutdown respawn chdir /opt/logstash script java -jar /opt/logstash/logstash-1.1.9-monolithic.jar agent -v -f /opt/logstash/conf.d/ end script Start the service:\nchmod +x /etc/init/logstash-indexer.conf service logstash-indexer start Ganglia collector (master) Create an A record for ganglia.[your domain name]. Point it to the server.\nWe need just a few packages more on the box:\napt-get install -q -y ganglia-monitor gmetad ganglia-webfrontend php5-cgi Create a www-data user and group:\nmkdir -p /home/www-data groupadd -g 3320 www-data useradd -m -d /home/www-data -s /bin/bash -u 3320 -g 3320 www-data chown www-data:www-data /home/www-data chown -R www-data:www-data /usr/share/ganglia-webfrontend Ganglia is written in PHP. Nginx needs FastCGI to process PHP. We must create a FastCGI service. Save this file in /etc/init.d/nginx-fastcgi:\n#!/bin/bash BIND=127.0.0.1:9000 USER=www-data PHP_FCGI_CHILDREN=15 PHP_FCGI_MAX_REQUESTS=1000 PHP_CGI=/usr/bin/php-cgi PHP_CGI_NAME=`basename $PHP_CGI` PHP_CGI_ARGS=\"- USER=$USER PATH=/usr/bin PHP_FCGI_CHILDREN=$PHP_FCGI_CHILDREN PHP_FCGI_MAX_REQUESTS=$PHP_FCGI_MAX_REQUESTS $PHP_CGI -b $BIND\" RETVAL=0 start() { echo -n \"Starting PHP FastCGI: \" start-stop-daemon --quiet --start --background --chuid \"$USER\" --exec /usr/bin/env -- $PHP_CGI_ARGS RETVAL=$? echo \"$PHP_CGI_NAME.\" } stop() { echo -n \"Stopping PHP FastCGI: \" killall -q -w -u $USER $PHP_CGI RETVAL=$? echo \"$PHP_CGI_NAME.\" } case \"$1\" in start) start ;; stop) stop ;; restart) stop start ;; *) echo \"Usage: php-fastcgi {start|stop|restart}\" exit 1 ;; esac exit $RETVAL Execute:\nchmod +x /etc/init.d/nginx-fastcgi update-rc.d /etc/init.d/nginx-fastcgi /etc/init.d/nginx-fastcgi start Final change in the /etc/nginx/sites-enabled/devops file. Add this at the bottom:\nserver { server_name ganglia.[your domain name]; ssl on; ssl_certificate /etc/ssl/certs/myssl.crt; ssl_certificate_key /etc/ssl/private/myssl.key; listen 443; ssl_session_timeout 5m; ssl_protocols SSLv2 SSLv3 TLSv1; ssl_ciphers ALL:!ADH:!EXPORT56:RC4+RSA:+HIGH:+MEDIUM:+LOW:+SSLv2:+EXP; ssl_prefer_server_ciphers on; location / { auth_basic \"Resricted - Ganglia\"; auth_basic_user_file /etc/nginx/htpasswd; } location ~ \\.php$ { include /etc/nginx/fastcgi_params; fastcgi_split_path_info ^(.+\\.php)(/.+)$; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME /usr/share/ganglia-webfrontend$fastcgi_script_name; } } And restart nginx for the final time.\n/etc/init.d/nginx reload M/Monit Installing M/Monit is really easy.\nmkidr /opt wget http://mmonit.com/dist/mmonit-2.4-linux-x64.tar.gz tar xvf mmonit-2.4-linux-x64.tar.gz ln -s /opt/mmonit-2.4 /opt/mmonit rm mmonit-2.4-linux-x64.tar.gz Upstart job, save it as /etc/init/mmonit.conf:\ndescription \"mmonit\" start on filesystem and net-device-up IFACE=eth0 stop on shutdown respawn script /opt/mmonit/bin/mmonit start end script pre-stop script /opt/mmonit/bin/mmonit stop end script Make it executable and run:\nchmod +x /etc/init/mmonit.conf service mmonit start Create A record for mmonit.[your domain name] and update nginx config, at the top of the file add:\nupstream mmonit_local { server localhost:8080; } And then, at the bottom:\nserver { server_name mmonit.[your domain name]; ssl on; ssl_certificate /etc/ssl/certs/myssl.crt; ssl_certificate_key /etc/ssl/private/myssl.key; listen 443; ssl_session_timeout 5m; ssl_protocols SSLv2 SSLv3 TLSv1; ssl_ciphers ALL:!ADH:!EXPORT56:RC4+RSA:+HIGH:+MEDIUM:+LOW:+SSLv2:+EXP; ssl_prefer_server_ciphers on; root /var/www; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto https; location / { proxy_pass http://mmonit_local; } } Reload nginx config:\n/etc/init.d/nginx reload Wrap up In this post we have covered setting up a collector box. We now have:\nhttps://chef.[your domain name], runs thr API and the WebUI, supports it’s own authentication and OpenID https://kibana.[your domain name], web frontend for logstash, protected with basic authentication https://ganglia.[your domain name], web frontend for ganglia, protected with basic authentication, uses the same credentials as kibana https://mmonit.[your domain name], web frontend for mmonit, protected with it’s own authentication We have also limited a number of ports opened on the firewall / security group. These have to be opened:\n22 (SSH) 443 (HTTPS) 8080 (HTTP) - M/Monit still redirects to 8080 10000 (Redis) In the next post I’m going to cover setting up knife so we can start deploying the infrastructure with it. Our infrastructure is going to contain:\nmonit recipe ganglia client recipe logstash shipper recipe We will use those to bootstrap a core setup box.\nUseful links Chef:\nInstalling Chef Server on Ubuntu using Packages Chef Resources A Brief Chef Tutorial (From Concentrate) Logstash:\nLogstash website Nginx:\nFcgi Example ","description":"I was asked to build a box with all the monitoring tools required. Chef, Logstash and Ganglia were suggested. Here’s a brain dump.","tags":["devops","redis","logstash","ganglia","monit"],"title":"DevOps box","uri":"/posts/2013-03-05-devops-box/"},{"content":"","description":"","tags":null,"title":"ganglia","uri":"/tags/ganglia/"},{"content":"","description":"","tags":null,"title":"logstash","uri":"/tags/logstash/"},{"content":"","description":"","tags":null,"title":"monit","uri":"/tags/monit/"},{"content":"","description":"","tags":null,"title":"redis","uri":"/tags/redis/"}]
